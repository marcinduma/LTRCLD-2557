{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to CiscoLive 2023 - Instructor Lab about ACI Multicloud Speakers: Karol Okraska , CX Delivery Architect, Cisco Systems, Inc. Marcin Duma , CX Delivery Architect, Cisco Systems, Inc. Cisco ACI support of Public Cloud infrastructure Cisco Cloud Network Controller (formerly Cloud APIC) provides enterprises with networking tools necessary to accelerate their hybrid-cloud and/or multicloud journey. Utilizing cloud-native constructs, the solution enables automation that accelerates infrastructure deployment and governance and simplifies management to easily connect workloads across multicloud environments. The Cisco Cloud Network Controller vision is to support enhanced observability, operations, and troubleshooting across the entire environment. Cisco Cloud Network Controller enables: \u25cf Seamless connectivity for any workload at scale across any location \u25cf Operational simplicity and visibility across a vast multisite, multicloud \u25cf data-center network \u25cf Easy L4-7 services integration \u25cf Consistent security and segmentation \u25cf Business continuity and disaster recovery Cloud Network Controller components: Cisco Cloud Network Controller is the main architectural component of this multicloud solution. It is the unified point of automation and management for the solution fabric including network and security policy, health monitoring, and optimizes performance and agility. The complete solution includes: \u25cf Cisco Cloud Network Controller(deployed in each Public Cloud which is to be managed) \u25cf Cisco Nexus Dashboard (in our lab deployed in AWS Cloud) - Multicloud networking orchestration and policy management, disaster recovery, and high availability, as well as provisioning and health monitoring. \u25cf Cisco Catalyst\u00ae 8000V - deployed in Public Clouds, allowing for communication with other Clouds or on-premises datacenter. Responsible for traffic secuirty and end to end policy enforcement. Topology In this lab we will be using two(2) Public Cloud Provders: \u25cf Microsoft Azure \u25cf Amazon AWS However Cloud Network Controller (CNC) is also supported on Google Cloud. In both Public Enviroments CNC will be deployed. Nexus Dashboard orchestrator will be run on Nexus Dashboard appliance deployed in AWS cloud. Due to time concern of this lab, Cloud Network Controller and Nexus Dasboard along with Nexus Dashboard Orchestrator Application will be pre-installed for you and ready for configuration. Intial lab diagram: Lab agenda 1. Infrastructure veryfication - login and access 2. Site onboarding in Nexus Dashboard 3. Multisite infrastructure configuration 4. Multisite configuration check 5. Tenant creation and Public Cloud Trust configuration 6. Three(3) common use-cases configuration and veryfcation - Stretched VRF across Public Clouds with Cloud-local EPGs - Internet Gateway configuration in AWS - Inter-Tenant routing Step by step configuration will guide you towards final topology as indicated in the picture below: Enjoy!","title":"Home"},{"location":"#welcome-to-ciscolive-2023-instructor-lab-about-aci-multicloud","text":"Speakers: Karol Okraska , CX Delivery Architect, Cisco Systems, Inc. Marcin Duma , CX Delivery Architect, Cisco Systems, Inc.","title":"Welcome to CiscoLive 2023 - Instructor Lab about ACI Multicloud"},{"location":"#cisco-aci-support-of-public-cloud-infrastructure","text":"Cisco Cloud Network Controller (formerly Cloud APIC) provides enterprises with networking tools necessary to accelerate their hybrid-cloud and/or multicloud journey. Utilizing cloud-native constructs, the solution enables automation that accelerates infrastructure deployment and governance and simplifies management to easily connect workloads across multicloud environments. The Cisco Cloud Network Controller vision is to support enhanced observability, operations, and troubleshooting across the entire environment. Cisco Cloud Network Controller enables: \u25cf Seamless connectivity for any workload at scale across any location \u25cf Operational simplicity and visibility across a vast multisite, multicloud \u25cf data-center network \u25cf Easy L4-7 services integration \u25cf Consistent security and segmentation \u25cf Business continuity and disaster recovery","title":"Cisco ACI support of Public Cloud infrastructure"},{"location":"#cloud-network-controller-components","text":"Cisco Cloud Network Controller is the main architectural component of this multicloud solution. It is the unified point of automation and management for the solution fabric including network and security policy, health monitoring, and optimizes performance and agility. The complete solution includes: \u25cf Cisco Cloud Network Controller(deployed in each Public Cloud which is to be managed) \u25cf Cisco Nexus Dashboard (in our lab deployed in AWS Cloud) - Multicloud networking orchestration and policy management, disaster recovery, and high availability, as well as provisioning and health monitoring. \u25cf Cisco Catalyst\u00ae 8000V - deployed in Public Clouds, allowing for communication with other Clouds or on-premises datacenter. Responsible for traffic secuirty and end to end policy enforcement.","title":"Cloud Network Controller components:"},{"location":"#topology","text":"In this lab we will be using two(2) Public Cloud Provders: \u25cf Microsoft Azure \u25cf Amazon AWS However Cloud Network Controller (CNC) is also supported on Google Cloud. In both Public Enviroments CNC will be deployed. Nexus Dashboard orchestrator will be run on Nexus Dashboard appliance deployed in AWS cloud. Due to time concern of this lab, Cloud Network Controller and Nexus Dasboard along with Nexus Dashboard Orchestrator Application will be pre-installed for you and ready for configuration. Intial lab diagram:","title":"Topology"},{"location":"#lab-agenda","text":"","title":"Lab agenda"},{"location":"#1-infrastructure-veryfication-login-and-access","text":"","title":"1. Infrastructure veryfication - login and access"},{"location":"#2-site-onboarding-in-nexus-dashboard","text":"","title":"2. Site onboarding in Nexus Dashboard"},{"location":"#3-multisite-infrastructure-configuration","text":"","title":"3. Multisite infrastructure configuration"},{"location":"#4-multisite-configuration-check","text":"","title":"4. Multisite configuration check"},{"location":"#5-tenant-creation-and-public-cloud-trust-configuration","text":"","title":"5. Tenant creation and Public Cloud Trust configuration"},{"location":"#6-three3-common-use-cases-configuration-and-veryfcation","text":"","title":"6. Three(3) common use-cases configuration and veryfcation"},{"location":"#-stretched-vrf-across-public-clouds-with-cloud-local-epgs","text":"","title":"- Stretched VRF across Public Clouds with Cloud-local EPGs"},{"location":"#-internet-gateway-configuration-in-aws","text":"","title":"- Internet Gateway configuration in AWS"},{"location":"#-inter-tenant-routing","text":"Step by step configuration will guide you towards final topology as indicated in the picture below: Enjoy!","title":"- Inter-Tenant routing"},{"location":"LAB_access-RDP/","text":"Connectivity Check 1. Lab access general description The lab has been built leveraging multiple environments as following: Amazon Web Services Private intrastructure on-prem You will have access to Cisco Intersight GUI, where you will see Kubernetes Clusters deployed On-Prem. In the end you will manage your application that will be deployed in 2 different environments. The infrastructure between On-prem and AWS is ready and functioning. In this lab you will see how to connect microservices together to make whole application work. Most of the tasks you will do from Linux Jumphost that is running on-premise. From there you will deploy components of your application in Kubernetes Cluster in AWS and on-prem. 2. VPN connection to dCloud infrastructure The entire lab for the session is built using Cisco dCloud environment. Details of the session are provided in POD assigned to you. You will find there \"Connect VPN\" link which allow you connection to dCloud session. When you decide to use BYOD, please connect to VPN using provided credentials. Access session with RDP Once logged to the dCloud session, you will be able to RDP Windows workstation ready to use for you Open RDP client and use credentials provided below: IP Address: 198.18.133.10 Username: administrator User password: C1sco12345 When you login to Remote Desktop, you will find installed Chrome as web browser, from where you get access to Cisco Intersight page. To access CSR router and Linux jumphost, use Putty installed - shortcut is on Desktop. 3. Accessing Linux Jumphost Open PuTTY client on RDP-workstation taskbar. PuTTY has pre-defined session to CSR router as well as to ubuntu-terminal. Use predefined ubuntu-terminal session by selecting it and click Open button. Username: dcloud User password: C1sco12345 4. Accessing Cisco Intersight Platform Cisco Intersight Platform manages Kuberenetes clusters in the private infrasturcture. You will have access to dedicated instance of Cisco Intersight, from which you will manage your own Kuberenetes Clusters used later on to deploy application. Please find login credentials and URL to your IKS instance below: Please connect to Cisco Intersight from RDP session. URL: https://intersight.com/ User name: holcld2611+pod'X'@gmail.com Tip Intersight username you find in POD details at WILAssistant page. User password: CiscoLive!2022 Warning You can explore Cisco Intersight through the GUI, but please do not delete content already created. 5. Accessing Cisco Intersight Assist Cisco Intersight requires installation of local (on-prem) satelite system. Cisco Intersight Assist collects data from local vCenter and send it to Cisco Intersight PaaS in secure manner. You can login to local Cisco Intersight Assist using credentials provided below: URL: https://iva.dcloud.cisco.com User name: admin User password: C1sco12345 Warning You can explore Cisco Intersight Assist through the GUI, but please do not delete content already created. 5. Accessing CSR1kv Lab router Your session contains Cisco CSR1kv router. It is used to terminate Site-2-Site tunnels with AWS tenant. All configurations are ready and doesn't require any modifications. Please find login credentials and IP address to your CSR1kv router below - use PuTTY predefined session: User name: admin User password: C1sco12345 Warning Do not delete configuration already existing on the router. 6. Accessing vCenter for Lab Whole setup is done on ESXi in Cisco dCloud environment. During the lab you don't need to perform actions on vCenter itself. However for case of troubleshooting or exploration, credentials to your vCenter below. URL: https://vc1.dcloud.cisco.com/ui User name: administrator@vsphere.local User password: C1sco12345! Warning Do not delete configuration nor VM machines already existing on the vCenter.","title":"Connectivity Check"},{"location":"LAB_access-RDP/#connectivity-check","text":"","title":"Connectivity Check"},{"location":"LAB_access-RDP/#1-lab-access-general-description","text":"The lab has been built leveraging multiple environments as following: Amazon Web Services Private intrastructure on-prem You will have access to Cisco Intersight GUI, where you will see Kubernetes Clusters deployed On-Prem. In the end you will manage your application that will be deployed in 2 different environments. The infrastructure between On-prem and AWS is ready and functioning. In this lab you will see how to connect microservices together to make whole application work. Most of the tasks you will do from Linux Jumphost that is running on-premise. From there you will deploy components of your application in Kubernetes Cluster in AWS and on-prem.","title":"1. Lab access general description"},{"location":"LAB_access-RDP/#2-vpn-connection-to-dcloud-infrastructure","text":"The entire lab for the session is built using Cisco dCloud environment. Details of the session are provided in POD assigned to you. You will find there \"Connect VPN\" link which allow you connection to dCloud session. When you decide to use BYOD, please connect to VPN using provided credentials.","title":"2. VPN connection to dCloud infrastructure"},{"location":"LAB_access-RDP/#access-session-with-rdp","text":"Once logged to the dCloud session, you will be able to RDP Windows workstation ready to use for you Open RDP client and use credentials provided below: IP Address: 198.18.133.10 Username: administrator User password: C1sco12345 When you login to Remote Desktop, you will find installed Chrome as web browser, from where you get access to Cisco Intersight page. To access CSR router and Linux jumphost, use Putty installed - shortcut is on Desktop.","title":"Access session with RDP"},{"location":"LAB_access-RDP/#3-accessing-linux-jumphost","text":"Open PuTTY client on RDP-workstation taskbar. PuTTY has pre-defined session to CSR router as well as to ubuntu-terminal. Use predefined ubuntu-terminal session by selecting it and click Open button. Username: dcloud User password: C1sco12345","title":"3. Accessing Linux Jumphost"},{"location":"LAB_access-RDP/#4-accessing-cisco-intersight-platform","text":"Cisco Intersight Platform manages Kuberenetes clusters in the private infrasturcture. You will have access to dedicated instance of Cisco Intersight, from which you will manage your own Kuberenetes Clusters used later on to deploy application. Please find login credentials and URL to your IKS instance below: Please connect to Cisco Intersight from RDP session. URL: https://intersight.com/ User name: holcld2611+pod'X'@gmail.com Tip Intersight username you find in POD details at WILAssistant page. User password: CiscoLive!2022 Warning You can explore Cisco Intersight through the GUI, but please do not delete content already created.","title":"4. Accessing Cisco Intersight Platform"},{"location":"LAB_access-RDP/#5-accessing-cisco-intersight-assist","text":"Cisco Intersight requires installation of local (on-prem) satelite system. Cisco Intersight Assist collects data from local vCenter and send it to Cisco Intersight PaaS in secure manner. You can login to local Cisco Intersight Assist using credentials provided below: URL: https://iva.dcloud.cisco.com User name: admin User password: C1sco12345 Warning You can explore Cisco Intersight Assist through the GUI, but please do not delete content already created.","title":"5. Accessing Cisco Intersight Assist"},{"location":"LAB_access-RDP/#5-accessing-csr1kv-lab-router","text":"Your session contains Cisco CSR1kv router. It is used to terminate Site-2-Site tunnels with AWS tenant. All configurations are ready and doesn't require any modifications. Please find login credentials and IP address to your CSR1kv router below - use PuTTY predefined session: User name: admin User password: C1sco12345 Warning Do not delete configuration already existing on the router.","title":"5. Accessing CSR1kv Lab router"},{"location":"LAB_access-RDP/#6-accessing-vcenter-for-lab","text":"Whole setup is done on ESXi in Cisco dCloud environment. During the lab you don't need to perform actions on vCenter itself. However for case of troubleshooting or exploration, credentials to your vCenter below. URL: https://vc1.dcloud.cisco.com/ui User name: administrator@vsphere.local User password: C1sco12345! Warning Do not delete configuration nor VM machines already existing on the vCenter.","title":"6. Accessing vCenter for Lab"},{"location":"LAB_access/","text":"Connectivity Check 1. Lab access general description This lab is fully hosted in Public Cloud infrastrucure, both AWS and Azure, no local resources are used. You will have access to Public Cloud Console Interface, as well as you will be able to login to necessary resources and appliances. 2. AWS and Azure console access details: Login link for AWS console: https://console.aws.amazon.com Login link for Azure portal: https://portal.azure.com 3. Accessing Cloud Network Controller and Nexus Dashboard. All appliances are addressed with Public IP address, hence it can be accessed from local browser without any additional settings or VPN connections.","title":"Accessing the Lab Environment"},{"location":"LAB_access/#connectivity-check","text":"","title":"Connectivity Check"},{"location":"LAB_access/#1-lab-access-general-description","text":"This lab is fully hosted in Public Cloud infrastrucure, both AWS and Azure, no local resources are used. You will have access to Public Cloud Console Interface, as well as you will be able to login to necessary resources and appliances.","title":"1. Lab access general description"},{"location":"LAB_access/#2-aws-and-azure-console-access-details","text":"Login link for AWS console: https://console.aws.amazon.com Login link for Azure portal: https://portal.azure.com","title":"2. AWS and Azure console access details:"},{"location":"LAB_access/#3-accessing-cloud-network-controller-and-nexus-dashboard","text":"All appliances are addressed with Public IP address, hence it can be accessed from local browser without any additional settings or VPN connections.","title":"3. Accessing Cloud Network Controller and Nexus Dashboard."},{"location":"URL-restAPI/","text":"Description Parameters are enclosed in double curly brackets Prepend the query URLs with the following if perform the query via https https://{{APIC_IP_ADDR}} URLs for REST API Collect the configuration of fabric, infra and tenants /api/mo/uni/fabric.xml?rsp-subtree=full&rsp-prop-include=config-only /api/mo/uni/infra.xml?rsp-subtree=full&rsp-prop-include=config-only /api/class/fvTenant.xml?rsp-subtree=full&rsp-prop-include=config-only /api/mo/uni/fabric.json?rsp-subtree=full&rsp-prop-include=config-only /api/mo/uni/infra.json?rsp-subtree=full&rsp-prop-include=config-only /api/class/fvTenant.json?rsp-subtree=full&rsp-prop-include=config-only Collect the configuration of a particular tenant /api/class/fvTenant.xml?rsp-subtree=full&rsp-prop-include=config-only&query-target=subtree&query-target-filter=eq(fvTenant.name, \"{{Tenant_name}}\") /api/class/fvTenant.json?rsp-subtree=full&rsp-prop-include=config-only&query-target=subtree&query-target-filter=eq(fvTenant.name, \"{{Tenant_name}}\") Collect the list of Access (non port-channel or vPC) Leaf interface policy groups /api/node/class/infraAccPortGrp.xml /api/node/class/infraAccPortGrp.json Collect the list of Bundle (port-channel or vPC) Leaf interface policy groups /api/node/class/infraAccBndlGrp.xml /api/node/class/infraAccBndlGrp.json Collect the list of Leaf interface profiles /api/node/mo/uni/infra.xml?query-target=subtree&target-subtree-class=infraAccPortP&query-target=subtree /api/node/mo/uni/infra.json?query-target=subtree&target-subtree-class=infraAccPortP&query-target=subtree Collect the list of Leaf interface profiles associated with a particular port interface policy group /api/node/class/infraAccPortGrp.xml?query-target=subtree&query-target-filter=eq(infraAccPortGrp.name, \"{{IPG_name}}\")&rsp-subtree=children&rsp-subtree-class=infraRtAccBaseGrp /api/node/class/infraAccPortGrp.json?query-target=subtree&query-target-filter=eq(infraAccPortGrp.name, \"{{IPG_name}}\")&rsp-subtree=children&rsp-subtree-class=infraRtAccBaseGrp Collect the list of Leaf interface profiles associated with a particular bundle interface policy group /api/node/class/infraAccBndlGrp.xml?query-target=subtree&query-target-filter=eq(infraAccBndlGrp.name, \"{{IPG_name}}\")&rsp-subtree=children&rsp-subtree-class=infraRtAccBaseGrp /api/node/class/infraAccBndlGrp.json?query-target=subtree&query-target-filter=eq(infraAccBndlGrp.name, \"{{IPG_name}}\")&rsp-subtree=children&rsp-subtree-class=infraRtAccBaseGrp Collect the list of switch node IDs where a particular interface profile is applied /api/node/mo/uni/infra/accportprof-{{Intf_profile_name}}.xml?query-target=subtree&target-subtree-class=infraRtAccPortP /api/node/mo/uni/infra/accportprof-{{Intf_profile_name}}.json?query-target=subtree&target-subtree-class=infraRtAccPortP Collect the range of switch node IDs associated with a particular switch profile /api/node/mo/uni/infra/nprof-{{Switch_profile_name}}.xml?query-target=subtree&target-subtree-class=infraNodeBlk /api/node/mo/uni/infra/nprof-{{Switch_profile_name}}.json?query-target=subtree&target-subtree-class=infraNodeBlk Collect the port channel information per Pod and Node /api/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys.xml?query-target=subtree&target-subtree-class=pcAggrIf&rsp-subtree=children&rsp-subtree-class=pcRsMbrIfs /api/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys.json?query-target=subtree&target-subtree-class=pcAggrIf&rsp-subtree=children&rsp-subtree-class=pcRsMbrIfs Collect the list of interfaces dynamically added to EPG due to VMM integration /api/node/mo/uni/epp/fv-[uni/tn-{{Tenant_name}}/ap-{{APP_name}}/epg-{{EPG_name}}].xml?query-target=subtree&target-subtree-class=fvIfConn /api/node/mo/uni/epp/fv-[uni/tn-{{Tenant_name}}/ap-{{APP_name}}/epg-{{EPG_name}}].json?query-target=subtree&target-subtree-class=fvIfConn Collect the interface RX traffic statistics /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/phys-[{{Intf_id}}]/HDeqptIngrTotal15min-0.xml /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/phys-[{{Intf_id}}]/HDeqptIngrTotal15min-0.json Collect the interface TX traffic statistics /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/phys-[{{Intf_id}}]/HDeqptEgrTotal15min-0.xml /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/phys-[{{Intf_id}}]/HDeqptEgrTotal15min-0.json Collect the CDP neighbor for an interface /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/cdp/inst/if-[{{Intf_id}}].xml?query-target=children&target-subtree-class=cdpAdjEp&query-target=subtree /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/cdp/inst/if-[{{Intf_id}}].json?query-target=children&target-subtree-class=cdpAdjEp&query-target=subtree Collect the LLDP neighbor for an interface /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/lldp/inst/if-[{{Intf_id}}].xml?query-target=children&target-subtree-class=lldpAdjEp&query-target=subtree /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/lldp/inst/if-[{{Intf_id}}].json?query-target=children&target-subtree-class=lldpAdjEp&query-target=subtree Collect the interface RX counters /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/phys-[{{Intf_id}}]/dbgIfIn.xml /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/phys-[{{Intf_id}}]/dbgIfIn.json Collect the interface TX counters /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/phys-[{{Intf_id}}]/dbgIfOut.xml /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/phys-[{{Intf_id}}]/dbgIfOut.json Collect the interface statistics /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/phys-[{{Intf_id}}]/dbgEtherStats.xml /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/phys-[{{Intf_id}}]/dbgEtherStats.json Collect the interface SFP information /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/phys-[{{Intf_id}}]/phys.xml?query-target=children&target-subtree-class=ethpmFcot /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/phys-[{{Intf_id}}]/phys.json?query-target=children&target-subtree-class=ethpmFcot Collect the information on interfaces dedicated for FEX uplinks on a node /api/node/class/topology/pod-{{Pod_id}}/node-{{Node_id}}/l2ExtIf.xml /api/node/class/topology/pod-{{Pod_id}}/node-{{Node_id}}/l2ExtIf.json Collect the information on interfaces dedicated for FEX uplinks on a node per FEX /api/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}.xml?query-target=subtree&target-subtree-class=satmFabP&query-target-filter=eq(satmFabP.extChId,\"{{Fex_id}}\") /api/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}.json?query-target=subtree&target-subtree-class=satmFabP&query-target-filter=eq(satmFabP.extChId,\"{{Fex_id}}\") Collect the FEX information /api/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}.xml?query-target=subtree&target-subtree-class=satmDExtCh&query-target-filter=eq(satmDExtCh.id, \"{{Fex_id}}\") /api/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}.json?query-target=subtree&target-subtree-class=satmDExtCh&query-target-filter=eq(satmDExtCh.id, \"{{Fex_id}}\") Collect the FEX inventory per switch node /api/node/class/topology/pod-{{Pod_id}}/node-{{Node_id}}/eqptExtCh.xml /api/node/class/topology/pod-{{Pod_id}}/node-{{Node_id}}/eqptExtCh.json Collect the supervisor inventory of a node /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/ch.xml?query-target=subtree&target-subtree-class=eqptSupC /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/ch.json?query-target=subtree&target-subtree-class=eqptSupC Collect the linecard inventory of a node /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/ch.xml?query-target=subtree&target-subtree-class=eqptLC /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/ch.json?query-target=subtree&target-subtree-class=eqptLC Collect the power supply inventory of a node /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/ch.xml?query-target=subtree&target-subtree-class=eqptPsu /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/ch.json?query-target=subtree&target-subtree-class=eqptPsu Collect the fan tray inventory of a node /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/ch.xml?query-target=subtree&target-subtree-class=eqptFt /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/ch.json?query-target=subtree&target-subtree-class=eqptFt Collect the fabric card inventory of a node /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/ch.xml?query-target=subtree&target-subtree-class=eqptFC /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/ch.json?query-target=subtree&target-subtree-class=eqptFC Collect the system controller inventory of a node /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/ch.xml?query-target=subtree&target-subtree-class=eqptSysC /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/ch.json?query-target=subtree&target-subtree-class=eqptSysC Collect the APIC time zone /api/node/mo/uni/fabric/format-default.xml /api/node/mo/uni/fabric/format-default.json Collect the NTP provider and status of a switch node /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/time.json?rsp-subtree=children /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/time.json?rsp-subtree=children Collect the NTP provider and status of an APIC node /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys.json?query-target=subtree&target-subtree-class=datetimeNtpq /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys.json?query-target=subtree&target-subtree-class=datetimeNtpq Collect the list of allowed VLANs on interface /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/phys-[{{Intf_id}}].xml?query-target=children&target-subtree-class=ethpmPhysIf /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/phys-[{{Intf_id}}].json?query-target=children&target-subtree-class=ethpmPhysIf Collect Leaf Switch profile info /api/node/mo/uni/infra/nprof-{{Profile_name}}.xml?query-target=subtree&target-subtree-class=infraNodeBlk /api/node/mo/uni/infra/nprof-{{Profile_name}}.json?query-target=subtree&target-subtree-class=infraNodeBlk Collect Spine Switch profile info /api/node/mo/uni/infra/spprof-{{Profile_name}}.xml?query-target=subtree&target-subtree-class=infraNodeBlk /api/node/mo/uni/infra/spprof-{{Profile_name}}.json?query-target=subtree&target-subtree-class=infraNodeBlk Collect vPC domain info /api/node/mo/uni/fabric/protpol/expgep-{{vPC_domain}}.xml?query-target=subtree&target-subtree-class=fabricNodePEp /api/node/mo/uni/fabric/protpol/expgep-{{vPC_domain}}.json?query-target=subtree&target-subtree-class=fabricNodePEp Collect static VLAN pool info /api/node/mo/uni/infra/vlanns-[{{VLANpool_name}}]-static.xml?query-target=subtree /api/node/mo/uni/infra/vlanns-[{{VLANpool_name}}]-static.json?query-target=subtree Collect dynamic VLAN pool info /api/node/mo/uni/infra/vlanns-[{{VLANpool_name}}]-dynamic.xml?query-target=subtree /api/node/mo/uni/infra/vlanns-[{{VLANpool_name}}]-dynamic.json?query-target=subtree Collect physical domain info /api/node/mo/uni/phys-{{Domain_name}}.xml?query-target=subtree /api/node/mo/uni/phys-{{Domain_name}}.json?query-target=subtree Collect physical domain VLAN pool name /api/node/mo/uni/phys-{{Domain_name}}/rsvlanNs.xml /api/node/mo/uni/phys-{{Domain_name}}/rsvlanNs.json Collect L3 domain info /api/node/mo/uni/l3dom-{{Domain_name}}.xml?query-target=subtree /api/node/mo/uni/l3dom-{{Domain_name}}.json?query-target=subtree Collect L3 domain VLAN pool name /api/node/mo/uni/l3dom-{{Domain_name}}/rsvlanNs.xml /api/node/mo/uni/l3dom-{{Domain_name}}/rsvlanNs.json Collect L2 domain info /api/node/mo/uni/l2dom-{{Domain_name}}.xml?query-target=subtree /api/node/mo/uni/l2dom-{{Domain_name}}.json?query-target=subtree Collect L2 domain VLAN pool name /api/node/mo/uni/l2dom-{{Domain_name}}/rsvlanNs.xml /api/node/mo/uni/l2dom-{{Domain_name}}/rsvlanNs.json Collect VMWare VMM domain info /api/node/mo/uni/vmmp-VMware/dom-{{Domain_name}}.xml?query-target=subtree /api/node/mo/uni/vmmp-VMware/dom-{{Domain_name}}.json?query-target=subtree Collect VMWare VMM domain VLAN pool name /api/node/mo/uni/vmmp-VMware/dom-{{Domain_name}}/rsvlanNs.xml /api/node/mo/uni/vmmp-VMware/dom-{{Domain_name}}/rsvlanNs.json Collect AAEP info /api/node/mo/uni/infra/attentp-{{AAEP_name}}.xml?query-target=subtree /api/node/mo/uni/infra/attentp-{{AAEP_name}}.json?query-target=subtree Collect Spine Interface Policy Group info /api/node/mo/uni/infra/funcprof/spaccportgrp-{{IPG_name}}.xml?query-target=subtree /api/node/mo/uni/infra/funcprof/spaccportgrp-{{IPG_name}}.json?query-target=subtree Collect Spine Interface Policy Group AAEP name /api/node/mo/uni/infra/funcprof/spaccportgrp-{{IPG_name}}/rsattEntP.xml /api/node/mo/uni/infra/funcprof/spaccportgrp-{{IPG_name}}/rsattEntP.json Collect Leaf Access Interface Policy Group info /api/node/mo/uni/infra/funcprof/accportgrp-{{IPG_name}}.xml?query-target=subtree /api/node/mo/uni/infra/funcprof/accportgrp-{{IPG_name}}.json?query-target=subtree Collect Leaf Access Interface Policy Group AAEP name /api/node/mo/uni/infra/funcprof/accportgrp-{{IPG_name}}/rsattEntP.xml /api/node/mo/uni/infra/funcprof/accportgrp-{{IPG_name}}/rsattEntP.json Collect Leaf PC or vPC Interface Policy Group info /api/node/mo/uni/infra/funcprof/accbundle-{{IPG_name}}.xml?query-target=subtree /api/node/mo/uni/infra/funcprof/accbundle-{{IPG_name}}.json?query-target=subtree Collect Leaf PC or vPC Interface Policy Group AAEP name /api/node/mo/uni/infra/funcprof/accbundle-{{IPG_name}}/rsattEntP.xml /api/node/mo/uni/infra/funcprof/accbundle-{{IPG_name}}/rsattEntP.json Collect Spine Interface Profile info /api/node/mo/uni/infra/spaccportprof-{{Profile_name}}.xml?query-target=subtree /api/node/mo/uni/infra/spaccportprof-{{Profile_name}}.json?query-target=subtree Collect Spine Interface Profile IPG name /api/node/mo/uni/infra/spaccportprof-{{Profile_name}}.xml?query-target=subtree&target-subtree-class=infraRsSpAccGrp /api/node/mo/uni/infra/spaccportprof-{{Profile_name}}.json?query-target=subtree&target-subtree-class=infraRsSpAccGrp Collect Leaf Interface Profile info /api/node/mo/uni/infra/accportprof-{{Profile_name}}.xml?query-target=subtree /api/node/mo/uni/infra/accportprof-{{Profile_name}}.json?query-target=subtree Collect Leaf Interface Profile IPG name /api/node/mo/uni/infra/accportprof-{{Profile_name}}.xml?query-target=subtree&target-subtree-class=infraRsAccBaseGrp /api/node/mo/uni/infra/accportprof-{{Profile_name}}.json?query-target=subtree&target-subtree-class=infraRsAccBaseGrp Collect Spine Interface profile names associated with Spine Switch profile /api/node/mo/uni/infra/spprof-{{Switchprofile_name}}.xml?query-target=subtree&target-subtree-class=infraRsSpAccPortP /api/node/mo/uni/infra/spprof-{{Switchprofile_name}}.json?query-target=subtree&target-subtree-class=infraRsSpAccPortP Collect Leaf Interface profile names associated with Leaf Switch profile /api/node/mo/uni/infra/nprof-{{Switchprofile_name}}.xml?query-target=subtree&target-subtree-class=infraRsAccPortP /api/node/mo/uni/infra/nprof-{{Switchprofile_name}}.json?query-target=subtree&target-subtree-class=infraRsAccPortP Collect Tenant info /api/node/mo/uni/tn-{{Tenant_name}}.xml?query-target=subtree /api/node/mo/uni/tn-{{Tenant_name}}.json?query-target=subtree Collect VRF info /api/node/mo/uni/tn-{{Tenant_name}}/ctx-{{VRF_name}}.xml?query-target=subtree /api/node/mo/uni/tn-{{Tenant_name}}/ctx-{{VRF_name}}.json?query-target=subtree Collect Bridge Domain info /api/node/mo/uni/tn-{{Tenant_name}}/BD-{{BD_name}}.xml?query-target=subtree /api/node/mo/uni/tn-{{Tenant_name}}/BD-{{BD_name}}.json?query-target=subtree Collect Bridge Domain VRF name /api/node/mo/uni/tn-{{Tenant_name}}/BD-{{BD_name}}.xml?query-target=children&target-subtree-class=fvRsCtx /api/node/mo/uni/tn-{{Tenant_name}}/BD-{{BD_name}}.json?query-target=children&target-subtree-class=fvRsCtx Collect Bridge Domain IP subnet /api/node/mo/uni/tn-{{Tenant_name}}/BD-{{BD_name}}.xml?query-target=children&target-subtree-class=fvSubnet /api/node/mo/uni/tn-{{Tenant_name}}/BD-{{BD_name}}.json?query-target=children&target-subtree-class=fvSubnet Collect Bridge Domain L3Out name /api/node/mo/uni/tn-{{Tenant_name}}/BD-{{BD_name}}.xml?query-target=children&target-subtree-class=fvRsBDToOut /api/node/mo/uni/tn-{{Tenant_name}}/BD-{{BD_name}}.json?query-target=children&target-subtree-class=fvRsBDToOut Collect Bridge Domain L3Out profile /api/node/mo/uni/tn-{{Tenant_name}}/BD-{{BD_name}}.xml?query-target=children&target-subtree-class=fvRsBDToProfile /api/node/mo/uni/tn-{{Tenant_name}}/BD-{{BD_name}}.json?query-target=children&target-subtree-class=fvRsBDToProfile Collect Application profile info /api/node/mo/uni/tn-{{Tenant_name}}/ap-{{APP_name}}.xml?query-target=children /api/node/mo/uni/tn-{{Tenant_name}}/ap-{{APP_name}}.json?query-target=children Collect EPG info /api/node/mo/uni/tn-{{Tenant_name}}/ap-{{APP_name}}/epg-{{EPG_name}}.xml?query-target=children /api/node/mo/uni/tn-{{Tenant_name}}/ap-{{APP_name}}/epg-{{EPG_name}}.json?query-target=children Collect EPG BD name /api/node/mo/uni/tn-{{Tenant_name}}/ap-{{APP_name}}/epg-{{EPG_name}}/rsbd.xml /api/node/mo/uni/tn-{{Tenant_name}}/ap-{{APP_name}}/epg-{{EPG_name}}/rsbd.json Collect L3Out info /api/node/mo/uni/tn-{{Tenant_name}}/out-{{L3Out_name}}.xml?query-target=children /api/node/mo/uni/tn-{{Tenant_name}}/out-{{L3Out_name}}.json?query-target=children Collect L3Out VRF name /api/node/mo/uni/tn-{{Tenant_name}}/out-{{L3Out_name}}/rsectx.xml /api/node/mo/uni/tn-{{Tenant_name}}/out-{{L3Out_name}}/rsectx.json Collect L3Out Node Profile info /api/node/mo/uni/tn-{{Tenant_name}}/out-{{L3Out_name}}/lnodep-{{NP_name}}.xml?query-target=children /api/node/mo/uni/tn-{{Tenant_name}}/out-{{L3Out_name}}/lnodep-{{NP_name}}.json?query-target=children Collect L3Out Interface Profile info /api/node/mo/uni/tn-{{Tenant_name}}/out-{{L3Out_name}}/lnodep-{{NP_name}}/lifp-{{Intfprof_name}}.xml?query-target=children /api/node/mo/uni/tn-{{Tenant_name}}/out-{{L3Out_name}}/lnodep-{{NP_name}}/lifp-{{Intfprof_name}}.json?query-target=children Collect L3Out Network (External EPG) info /api/node/mo/uni/tn-{{Tenant_name}}/out-{{L3Out_name}}/instP-{{ExtEPG_name}}.xml?query-target=children /api/node/mo/uni/tn-{{Tenant_name}}/out-{{L3Out_name}}/instP-{{ExtEPG_name}}.json?query-target=children Collect Contract info /api/node/mo/uni/tn-{{Tenant_name}}/brc-{{Contract_name}}.xml?query-target=children /api/node/mo/uni/tn-{{Tenant_name}}/brc-{{Contract_name}}.json?query-target=children","title":"Description"},{"location":"URL-restAPI/#description","text":"Parameters are enclosed in double curly brackets Prepend the query URLs with the following if perform the query via https https://{{APIC_IP_ADDR}}","title":"Description"},{"location":"URL-restAPI/#urls-for-rest-api","text":"Collect the configuration of fabric, infra and tenants /api/mo/uni/fabric.xml?rsp-subtree=full&rsp-prop-include=config-only /api/mo/uni/infra.xml?rsp-subtree=full&rsp-prop-include=config-only /api/class/fvTenant.xml?rsp-subtree=full&rsp-prop-include=config-only /api/mo/uni/fabric.json?rsp-subtree=full&rsp-prop-include=config-only /api/mo/uni/infra.json?rsp-subtree=full&rsp-prop-include=config-only /api/class/fvTenant.json?rsp-subtree=full&rsp-prop-include=config-only Collect the configuration of a particular tenant /api/class/fvTenant.xml?rsp-subtree=full&rsp-prop-include=config-only&query-target=subtree&query-target-filter=eq(fvTenant.name, \"{{Tenant_name}}\") /api/class/fvTenant.json?rsp-subtree=full&rsp-prop-include=config-only&query-target=subtree&query-target-filter=eq(fvTenant.name, \"{{Tenant_name}}\") Collect the list of Access (non port-channel or vPC) Leaf interface policy groups /api/node/class/infraAccPortGrp.xml /api/node/class/infraAccPortGrp.json Collect the list of Bundle (port-channel or vPC) Leaf interface policy groups /api/node/class/infraAccBndlGrp.xml /api/node/class/infraAccBndlGrp.json Collect the list of Leaf interface profiles /api/node/mo/uni/infra.xml?query-target=subtree&target-subtree-class=infraAccPortP&query-target=subtree /api/node/mo/uni/infra.json?query-target=subtree&target-subtree-class=infraAccPortP&query-target=subtree Collect the list of Leaf interface profiles associated with a particular port interface policy group /api/node/class/infraAccPortGrp.xml?query-target=subtree&query-target-filter=eq(infraAccPortGrp.name, \"{{IPG_name}}\")&rsp-subtree=children&rsp-subtree-class=infraRtAccBaseGrp /api/node/class/infraAccPortGrp.json?query-target=subtree&query-target-filter=eq(infraAccPortGrp.name, \"{{IPG_name}}\")&rsp-subtree=children&rsp-subtree-class=infraRtAccBaseGrp Collect the list of Leaf interface profiles associated with a particular bundle interface policy group /api/node/class/infraAccBndlGrp.xml?query-target=subtree&query-target-filter=eq(infraAccBndlGrp.name, \"{{IPG_name}}\")&rsp-subtree=children&rsp-subtree-class=infraRtAccBaseGrp /api/node/class/infraAccBndlGrp.json?query-target=subtree&query-target-filter=eq(infraAccBndlGrp.name, \"{{IPG_name}}\")&rsp-subtree=children&rsp-subtree-class=infraRtAccBaseGrp Collect the list of switch node IDs where a particular interface profile is applied /api/node/mo/uni/infra/accportprof-{{Intf_profile_name}}.xml?query-target=subtree&target-subtree-class=infraRtAccPortP /api/node/mo/uni/infra/accportprof-{{Intf_profile_name}}.json?query-target=subtree&target-subtree-class=infraRtAccPortP Collect the range of switch node IDs associated with a particular switch profile /api/node/mo/uni/infra/nprof-{{Switch_profile_name}}.xml?query-target=subtree&target-subtree-class=infraNodeBlk /api/node/mo/uni/infra/nprof-{{Switch_profile_name}}.json?query-target=subtree&target-subtree-class=infraNodeBlk Collect the port channel information per Pod and Node /api/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys.xml?query-target=subtree&target-subtree-class=pcAggrIf&rsp-subtree=children&rsp-subtree-class=pcRsMbrIfs /api/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys.json?query-target=subtree&target-subtree-class=pcAggrIf&rsp-subtree=children&rsp-subtree-class=pcRsMbrIfs Collect the list of interfaces dynamically added to EPG due to VMM integration /api/node/mo/uni/epp/fv-[uni/tn-{{Tenant_name}}/ap-{{APP_name}}/epg-{{EPG_name}}].xml?query-target=subtree&target-subtree-class=fvIfConn /api/node/mo/uni/epp/fv-[uni/tn-{{Tenant_name}}/ap-{{APP_name}}/epg-{{EPG_name}}].json?query-target=subtree&target-subtree-class=fvIfConn Collect the interface RX traffic statistics /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/phys-[{{Intf_id}}]/HDeqptIngrTotal15min-0.xml /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/phys-[{{Intf_id}}]/HDeqptIngrTotal15min-0.json Collect the interface TX traffic statistics /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/phys-[{{Intf_id}}]/HDeqptEgrTotal15min-0.xml /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/phys-[{{Intf_id}}]/HDeqptEgrTotal15min-0.json Collect the CDP neighbor for an interface /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/cdp/inst/if-[{{Intf_id}}].xml?query-target=children&target-subtree-class=cdpAdjEp&query-target=subtree /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/cdp/inst/if-[{{Intf_id}}].json?query-target=children&target-subtree-class=cdpAdjEp&query-target=subtree Collect the LLDP neighbor for an interface /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/lldp/inst/if-[{{Intf_id}}].xml?query-target=children&target-subtree-class=lldpAdjEp&query-target=subtree /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/lldp/inst/if-[{{Intf_id}}].json?query-target=children&target-subtree-class=lldpAdjEp&query-target=subtree Collect the interface RX counters /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/phys-[{{Intf_id}}]/dbgIfIn.xml /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/phys-[{{Intf_id}}]/dbgIfIn.json Collect the interface TX counters /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/phys-[{{Intf_id}}]/dbgIfOut.xml /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/phys-[{{Intf_id}}]/dbgIfOut.json Collect the interface statistics /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/phys-[{{Intf_id}}]/dbgEtherStats.xml /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/phys-[{{Intf_id}}]/dbgEtherStats.json Collect the interface SFP information /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/phys-[{{Intf_id}}]/phys.xml?query-target=children&target-subtree-class=ethpmFcot /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/phys-[{{Intf_id}}]/phys.json?query-target=children&target-subtree-class=ethpmFcot Collect the information on interfaces dedicated for FEX uplinks on a node /api/node/class/topology/pod-{{Pod_id}}/node-{{Node_id}}/l2ExtIf.xml /api/node/class/topology/pod-{{Pod_id}}/node-{{Node_id}}/l2ExtIf.json Collect the information on interfaces dedicated for FEX uplinks on a node per FEX /api/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}.xml?query-target=subtree&target-subtree-class=satmFabP&query-target-filter=eq(satmFabP.extChId,\"{{Fex_id}}\") /api/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}.json?query-target=subtree&target-subtree-class=satmFabP&query-target-filter=eq(satmFabP.extChId,\"{{Fex_id}}\") Collect the FEX information /api/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}.xml?query-target=subtree&target-subtree-class=satmDExtCh&query-target-filter=eq(satmDExtCh.id, \"{{Fex_id}}\") /api/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}.json?query-target=subtree&target-subtree-class=satmDExtCh&query-target-filter=eq(satmDExtCh.id, \"{{Fex_id}}\") Collect the FEX inventory per switch node /api/node/class/topology/pod-{{Pod_id}}/node-{{Node_id}}/eqptExtCh.xml /api/node/class/topology/pod-{{Pod_id}}/node-{{Node_id}}/eqptExtCh.json Collect the supervisor inventory of a node /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/ch.xml?query-target=subtree&target-subtree-class=eqptSupC /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/ch.json?query-target=subtree&target-subtree-class=eqptSupC Collect the linecard inventory of a node /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/ch.xml?query-target=subtree&target-subtree-class=eqptLC /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/ch.json?query-target=subtree&target-subtree-class=eqptLC Collect the power supply inventory of a node /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/ch.xml?query-target=subtree&target-subtree-class=eqptPsu /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/ch.json?query-target=subtree&target-subtree-class=eqptPsu Collect the fan tray inventory of a node /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/ch.xml?query-target=subtree&target-subtree-class=eqptFt /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/ch.json?query-target=subtree&target-subtree-class=eqptFt Collect the fabric card inventory of a node /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/ch.xml?query-target=subtree&target-subtree-class=eqptFC /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/ch.json?query-target=subtree&target-subtree-class=eqptFC Collect the system controller inventory of a node /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/ch.xml?query-target=subtree&target-subtree-class=eqptSysC /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/ch.json?query-target=subtree&target-subtree-class=eqptSysC Collect the APIC time zone /api/node/mo/uni/fabric/format-default.xml /api/node/mo/uni/fabric/format-default.json Collect the NTP provider and status of a switch node /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/time.json?rsp-subtree=children /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/time.json?rsp-subtree=children Collect the NTP provider and status of an APIC node /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys.json?query-target=subtree&target-subtree-class=datetimeNtpq /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys.json?query-target=subtree&target-subtree-class=datetimeNtpq Collect the list of allowed VLANs on interface /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/phys-[{{Intf_id}}].xml?query-target=children&target-subtree-class=ethpmPhysIf /api/node/mo/topology/pod-{{Pod_id}}/node-{{Node_id}}/sys/phys-[{{Intf_id}}].json?query-target=children&target-subtree-class=ethpmPhysIf Collect Leaf Switch profile info /api/node/mo/uni/infra/nprof-{{Profile_name}}.xml?query-target=subtree&target-subtree-class=infraNodeBlk /api/node/mo/uni/infra/nprof-{{Profile_name}}.json?query-target=subtree&target-subtree-class=infraNodeBlk Collect Spine Switch profile info /api/node/mo/uni/infra/spprof-{{Profile_name}}.xml?query-target=subtree&target-subtree-class=infraNodeBlk /api/node/mo/uni/infra/spprof-{{Profile_name}}.json?query-target=subtree&target-subtree-class=infraNodeBlk Collect vPC domain info /api/node/mo/uni/fabric/protpol/expgep-{{vPC_domain}}.xml?query-target=subtree&target-subtree-class=fabricNodePEp /api/node/mo/uni/fabric/protpol/expgep-{{vPC_domain}}.json?query-target=subtree&target-subtree-class=fabricNodePEp Collect static VLAN pool info /api/node/mo/uni/infra/vlanns-[{{VLANpool_name}}]-static.xml?query-target=subtree /api/node/mo/uni/infra/vlanns-[{{VLANpool_name}}]-static.json?query-target=subtree Collect dynamic VLAN pool info /api/node/mo/uni/infra/vlanns-[{{VLANpool_name}}]-dynamic.xml?query-target=subtree /api/node/mo/uni/infra/vlanns-[{{VLANpool_name}}]-dynamic.json?query-target=subtree Collect physical domain info /api/node/mo/uni/phys-{{Domain_name}}.xml?query-target=subtree /api/node/mo/uni/phys-{{Domain_name}}.json?query-target=subtree Collect physical domain VLAN pool name /api/node/mo/uni/phys-{{Domain_name}}/rsvlanNs.xml /api/node/mo/uni/phys-{{Domain_name}}/rsvlanNs.json Collect L3 domain info /api/node/mo/uni/l3dom-{{Domain_name}}.xml?query-target=subtree /api/node/mo/uni/l3dom-{{Domain_name}}.json?query-target=subtree Collect L3 domain VLAN pool name /api/node/mo/uni/l3dom-{{Domain_name}}/rsvlanNs.xml /api/node/mo/uni/l3dom-{{Domain_name}}/rsvlanNs.json Collect L2 domain info /api/node/mo/uni/l2dom-{{Domain_name}}.xml?query-target=subtree /api/node/mo/uni/l2dom-{{Domain_name}}.json?query-target=subtree Collect L2 domain VLAN pool name /api/node/mo/uni/l2dom-{{Domain_name}}/rsvlanNs.xml /api/node/mo/uni/l2dom-{{Domain_name}}/rsvlanNs.json Collect VMWare VMM domain info /api/node/mo/uni/vmmp-VMware/dom-{{Domain_name}}.xml?query-target=subtree /api/node/mo/uni/vmmp-VMware/dom-{{Domain_name}}.json?query-target=subtree Collect VMWare VMM domain VLAN pool name /api/node/mo/uni/vmmp-VMware/dom-{{Domain_name}}/rsvlanNs.xml /api/node/mo/uni/vmmp-VMware/dom-{{Domain_name}}/rsvlanNs.json Collect AAEP info /api/node/mo/uni/infra/attentp-{{AAEP_name}}.xml?query-target=subtree /api/node/mo/uni/infra/attentp-{{AAEP_name}}.json?query-target=subtree Collect Spine Interface Policy Group info /api/node/mo/uni/infra/funcprof/spaccportgrp-{{IPG_name}}.xml?query-target=subtree /api/node/mo/uni/infra/funcprof/spaccportgrp-{{IPG_name}}.json?query-target=subtree Collect Spine Interface Policy Group AAEP name /api/node/mo/uni/infra/funcprof/spaccportgrp-{{IPG_name}}/rsattEntP.xml /api/node/mo/uni/infra/funcprof/spaccportgrp-{{IPG_name}}/rsattEntP.json Collect Leaf Access Interface Policy Group info /api/node/mo/uni/infra/funcprof/accportgrp-{{IPG_name}}.xml?query-target=subtree /api/node/mo/uni/infra/funcprof/accportgrp-{{IPG_name}}.json?query-target=subtree Collect Leaf Access Interface Policy Group AAEP name /api/node/mo/uni/infra/funcprof/accportgrp-{{IPG_name}}/rsattEntP.xml /api/node/mo/uni/infra/funcprof/accportgrp-{{IPG_name}}/rsattEntP.json Collect Leaf PC or vPC Interface Policy Group info /api/node/mo/uni/infra/funcprof/accbundle-{{IPG_name}}.xml?query-target=subtree /api/node/mo/uni/infra/funcprof/accbundle-{{IPG_name}}.json?query-target=subtree Collect Leaf PC or vPC Interface Policy Group AAEP name /api/node/mo/uni/infra/funcprof/accbundle-{{IPG_name}}/rsattEntP.xml /api/node/mo/uni/infra/funcprof/accbundle-{{IPG_name}}/rsattEntP.json Collect Spine Interface Profile info /api/node/mo/uni/infra/spaccportprof-{{Profile_name}}.xml?query-target=subtree /api/node/mo/uni/infra/spaccportprof-{{Profile_name}}.json?query-target=subtree Collect Spine Interface Profile IPG name /api/node/mo/uni/infra/spaccportprof-{{Profile_name}}.xml?query-target=subtree&target-subtree-class=infraRsSpAccGrp /api/node/mo/uni/infra/spaccportprof-{{Profile_name}}.json?query-target=subtree&target-subtree-class=infraRsSpAccGrp Collect Leaf Interface Profile info /api/node/mo/uni/infra/accportprof-{{Profile_name}}.xml?query-target=subtree /api/node/mo/uni/infra/accportprof-{{Profile_name}}.json?query-target=subtree Collect Leaf Interface Profile IPG name /api/node/mo/uni/infra/accportprof-{{Profile_name}}.xml?query-target=subtree&target-subtree-class=infraRsAccBaseGrp /api/node/mo/uni/infra/accportprof-{{Profile_name}}.json?query-target=subtree&target-subtree-class=infraRsAccBaseGrp Collect Spine Interface profile names associated with Spine Switch profile /api/node/mo/uni/infra/spprof-{{Switchprofile_name}}.xml?query-target=subtree&target-subtree-class=infraRsSpAccPortP /api/node/mo/uni/infra/spprof-{{Switchprofile_name}}.json?query-target=subtree&target-subtree-class=infraRsSpAccPortP Collect Leaf Interface profile names associated with Leaf Switch profile /api/node/mo/uni/infra/nprof-{{Switchprofile_name}}.xml?query-target=subtree&target-subtree-class=infraRsAccPortP /api/node/mo/uni/infra/nprof-{{Switchprofile_name}}.json?query-target=subtree&target-subtree-class=infraRsAccPortP Collect Tenant info /api/node/mo/uni/tn-{{Tenant_name}}.xml?query-target=subtree /api/node/mo/uni/tn-{{Tenant_name}}.json?query-target=subtree Collect VRF info /api/node/mo/uni/tn-{{Tenant_name}}/ctx-{{VRF_name}}.xml?query-target=subtree /api/node/mo/uni/tn-{{Tenant_name}}/ctx-{{VRF_name}}.json?query-target=subtree Collect Bridge Domain info /api/node/mo/uni/tn-{{Tenant_name}}/BD-{{BD_name}}.xml?query-target=subtree /api/node/mo/uni/tn-{{Tenant_name}}/BD-{{BD_name}}.json?query-target=subtree Collect Bridge Domain VRF name /api/node/mo/uni/tn-{{Tenant_name}}/BD-{{BD_name}}.xml?query-target=children&target-subtree-class=fvRsCtx /api/node/mo/uni/tn-{{Tenant_name}}/BD-{{BD_name}}.json?query-target=children&target-subtree-class=fvRsCtx Collect Bridge Domain IP subnet /api/node/mo/uni/tn-{{Tenant_name}}/BD-{{BD_name}}.xml?query-target=children&target-subtree-class=fvSubnet /api/node/mo/uni/tn-{{Tenant_name}}/BD-{{BD_name}}.json?query-target=children&target-subtree-class=fvSubnet Collect Bridge Domain L3Out name /api/node/mo/uni/tn-{{Tenant_name}}/BD-{{BD_name}}.xml?query-target=children&target-subtree-class=fvRsBDToOut /api/node/mo/uni/tn-{{Tenant_name}}/BD-{{BD_name}}.json?query-target=children&target-subtree-class=fvRsBDToOut Collect Bridge Domain L3Out profile /api/node/mo/uni/tn-{{Tenant_name}}/BD-{{BD_name}}.xml?query-target=children&target-subtree-class=fvRsBDToProfile /api/node/mo/uni/tn-{{Tenant_name}}/BD-{{BD_name}}.json?query-target=children&target-subtree-class=fvRsBDToProfile Collect Application profile info /api/node/mo/uni/tn-{{Tenant_name}}/ap-{{APP_name}}.xml?query-target=children /api/node/mo/uni/tn-{{Tenant_name}}/ap-{{APP_name}}.json?query-target=children Collect EPG info /api/node/mo/uni/tn-{{Tenant_name}}/ap-{{APP_name}}/epg-{{EPG_name}}.xml?query-target=children /api/node/mo/uni/tn-{{Tenant_name}}/ap-{{APP_name}}/epg-{{EPG_name}}.json?query-target=children Collect EPG BD name /api/node/mo/uni/tn-{{Tenant_name}}/ap-{{APP_name}}/epg-{{EPG_name}}/rsbd.xml /api/node/mo/uni/tn-{{Tenant_name}}/ap-{{APP_name}}/epg-{{EPG_name}}/rsbd.json Collect L3Out info /api/node/mo/uni/tn-{{Tenant_name}}/out-{{L3Out_name}}.xml?query-target=children /api/node/mo/uni/tn-{{Tenant_name}}/out-{{L3Out_name}}.json?query-target=children Collect L3Out VRF name /api/node/mo/uni/tn-{{Tenant_name}}/out-{{L3Out_name}}/rsectx.xml /api/node/mo/uni/tn-{{Tenant_name}}/out-{{L3Out_name}}/rsectx.json Collect L3Out Node Profile info /api/node/mo/uni/tn-{{Tenant_name}}/out-{{L3Out_name}}/lnodep-{{NP_name}}.xml?query-target=children /api/node/mo/uni/tn-{{Tenant_name}}/out-{{L3Out_name}}/lnodep-{{NP_name}}.json?query-target=children Collect L3Out Interface Profile info /api/node/mo/uni/tn-{{Tenant_name}}/out-{{L3Out_name}}/lnodep-{{NP_name}}/lifp-{{Intfprof_name}}.xml?query-target=children /api/node/mo/uni/tn-{{Tenant_name}}/out-{{L3Out_name}}/lnodep-{{NP_name}}/lifp-{{Intfprof_name}}.json?query-target=children Collect L3Out Network (External EPG) info /api/node/mo/uni/tn-{{Tenant_name}}/out-{{L3Out_name}}/instP-{{ExtEPG_name}}.xml?query-target=children /api/node/mo/uni/tn-{{Tenant_name}}/out-{{L3Out_name}}/instP-{{ExtEPG_name}}.json?query-target=children Collect Contract info /api/node/mo/uni/tn-{{Tenant_name}}/brc-{{Contract_name}}.xml?query-target=children /api/node/mo/uni/tn-{{Tenant_name}}/brc-{{Contract_name}}.json?query-target=children","title":"URLs for REST API"},{"location":"aws-trust/","text":"Nexus Dasboard Orchestrator Tenant AWS Trust Configuration In previus step we select Tenant configuration as Trusted, trust need to be made to allow for configuration. AWS Cloud Network Controller Login Using the IP found during site onboarding (can be also found in POD guide), connect via browser to CNC GUI for AWS instance. Provide Credentials and hit Login Username: admin Password: CiscoLive2023! Hit \"Get started\" to view Cloud Network Controller Dashboard. Look on the Dashboard -> System -> Fault Summary We can see Major Fault higher that \"0\" Click on Major button to view details. List of all Major faults will pop-up. One particular fault, with Code F3526 is related to AccessDenied for User Account configuration. Double-Click on any column for that fault to open details. Inside Fault Description there is information about CFT Script which has to be executed on User Tenant in AWS for Trust configuration. Extract the CFT URL link for that message - it will be different that one in the screenshot as ID is generated dynamically. https://capic-common-XXXXXXXXXX-data.s3.amazonaws.com/tenant-cft.json Note it down, will be used in a moment. AWS User Account Login Each User POD has two(2) AWS Accounts. 1st for Infrastrucre Configuration 2nd for Tenant Configuration 1. Open AWS console via browser Note As you are already logged into AWS for Infrastructure Account, you can logout or user Incognito Mode, or different browser. 2. Select IAM user, provide Account ID and hit \"Next\" Note For this login please use AWS User Account ID 3. Provide Username and password and hit \"Sign In\" 3. Trust script execution In the same browser you logged into User Account, open New Tab and paste CFT link collected above.","title":"AWS Trust"},{"location":"aws-trust/#nexus-dasboard-orchestrator-tenant-aws-trust-configuration","text":"In previus step we select Tenant configuration as Trusted, trust need to be made to allow for configuration.","title":"Nexus Dasboard Orchestrator Tenant AWS Trust Configuration"},{"location":"aws-trust/#aws-cloud-network-controller-login","text":"Using the IP found during site onboarding (can be also found in POD guide), connect via browser to CNC GUI for AWS instance. Provide Credentials and hit Login Username: admin Password: CiscoLive2023! Hit \"Get started\" to view Cloud Network Controller Dashboard. Look on the Dashboard -> System -> Fault Summary We can see Major Fault higher that \"0\" Click on Major button to view details. List of all Major faults will pop-up. One particular fault, with Code F3526 is related to AccessDenied for User Account configuration. Double-Click on any column for that fault to open details. Inside Fault Description there is information about CFT Script which has to be executed on User Tenant in AWS for Trust configuration. Extract the CFT URL link for that message - it will be different that one in the screenshot as ID is generated dynamically. https://capic-common-XXXXXXXXXX-data.s3.amazonaws.com/tenant-cft.json Note it down, will be used in a moment.","title":"AWS Cloud Network Controller Login"},{"location":"aws-trust/#aws-user-account-login","text":"Each User POD has two(2) AWS Accounts. 1st for Infrastrucre Configuration 2nd for Tenant Configuration","title":"AWS User Account Login"},{"location":"aws-trust/#1-open-aws-console-via-browser","text":"Note As you are already logged into AWS for Infrastructure Account, you can logout or user Incognito Mode, or different browser.","title":"1. Open AWS console via browser"},{"location":"aws-trust/#2-select-iam-user-provide-account-id-and-hit-next","text":"Note For this login please use AWS User Account ID","title":"2. Select IAM user, provide Account ID and hit \"Next\""},{"location":"aws-trust/#3-provide-username-and-password-and-hit-sign-in","text":"","title":"3. Provide Username and password and hit \"Sign In\""},{"location":"aws-trust/#3-trust-script-execution","text":"In the same browser you logged into User Account, open New Tab and paste CFT link collected above.","title":"3. Trust script execution"},{"location":"backend_exercise/","text":"Explore Backend App and Kubernetes Dashboard: Your Hybrid Cloud App's backend compoents (MariaDB, MQTT DB Agent, and REST API agent) should be up and running. Now let's explore some low level details to understand the application containers and kubernetes better. Task 0: Tasks in this section will be executed in Kubernetes on-premise. Change kubectl context to the correct one. Execute command below on linux jumphost: kubectl config use-context admin@CLUS-IKS-1 Task 1: Find the size of the 'Persistent Volume Claim' used for MariaDB database? To check 'CAPACITY' value of PVC execute this command on linux jumphost: kubectl get pvc mariadb-pv-claim Task 2: Login to the MariaDB database and explore the data tables. Find the pod name for 'MariaDB' using the kubectl command kubectl get pods and locate the pod name starting with 'iot-backend-mariadb-'. Login to the MariaDB container using the kubectl command kubectl exec -it <pod name> -- /bin/bash Tip (replace the <pod name> in command above with correct value). Inside the container execute ss -tulpn and check the port MariaDB is listing to for incoming connections. You can also check the version of MariaDB using the shell command: mysql --version Now connect to the MariaDB from the container shell and check Databases and Tables. Use mysql -u root -pcisco123 command to login to MaraDB. On mariaDB shell, use show databases; command to list all the databases. Look for ' sensor_db ' in the output (this is the database where we are storing the sensor data). Switch to sensor_db using the command use sensor_db; and list all the tables in this database using the command show tables; You should see only one table with the name 'sensor_data' ; Try to list the data from this table using the SQL statement select * from sensor_data; Now check the record count in this table using the sql statement: select count(*) from sensor_data; Repeat the SQL from step 8 several times and check if the record count is increasing (each sensor would send the data after every 10 seconds). Use exit command at MariaDB prompt to exit the DB shell. Use exit command again to exit 'iot-backend-mariadb' container shell. Task 3: Connect to the REST API Agent container and find the port it is listing on for incoming REST calls. Find the pod name for 'REST API Agent' using the kubectl command kubectl get pods and locate the pod name starting with 'iot-backend-rest-api-agent-' (you may see more than one pod for this service, pick any one). login to the REST API Agent container using the kubectl command kubectl exec -it <pod name> -- /bin/ash Tip (replace the <pod name> in command above with correct value). Execute netstat -an command on the container shell and check the output. This container listens on port '5050' for incoming REST connections and connects with MariaDB using port '3306' (Connection to DB will timeout in case there are no requests coming in). Use exit command to come out of the container shell. Task 4: Check the logs messages from 'REST API Agent'. Find the pod name for 'REST API Agent' using the kubectl command kubectl get pods and locate the pod name starting with 'iot-backend-rest-api-agent-' (you may see more than one pod for this service, pick any one). Check the logs using the kubectl command kubectl logs <pod name> Tip (replace the <pod name> in command above with correct value). Task 5: How the traffic would be distributed, if you have multiple Kubernetes Pods behind a Kubernetes NodePort Service? Right click on top of your putty window and click on \"Duplicate Session\" (You should have two putty windows side by side logged into same linux jumphost). In the first putty window (logged into linux jumphost) execute the command kubectl get nodes -o wide and note down the Kubernetes master node's external IP address. Open a web browser on your machine and access the following url http://<kubernetes node's external ip>:<port>/temperature In the first putty window, run kubectl get pods on the linux jumphost. In the output you should see 2 pods for 'iot-backend-rest-api-agent'. In the first putty window, run the command watch kubectl logs <first pod name> (Use the first 'iot-backend-rest-api-agent' pod out of 2 pods that you saw in the output of the command executed in step#5). In the Second putty window, display the logs for second 'iot-backend-rest-api-agent' pod using the command watch kubectl logs <second pod name> Refresh or reload the web page with the url you used in step 4. Repeat step 8 and check the requests hitting the kubernetes pods in the log messages.","title":"Explore Backend App and Kubernetes Dashboard:"},{"location":"backend_exercise/#explore-backend-app-and-kubernetes-dashboard","text":"Your Hybrid Cloud App's backend compoents (MariaDB, MQTT DB Agent, and REST API agent) should be up and running. Now let's explore some low level details to understand the application containers and kubernetes better. Task 0: Tasks in this section will be executed in Kubernetes on-premise. Change kubectl context to the correct one. Execute command below on linux jumphost: kubectl config use-context admin@CLUS-IKS-1 Task 1: Find the size of the 'Persistent Volume Claim' used for MariaDB database? To check 'CAPACITY' value of PVC execute this command on linux jumphost: kubectl get pvc mariadb-pv-claim Task 2: Login to the MariaDB database and explore the data tables. Find the pod name for 'MariaDB' using the kubectl command kubectl get pods and locate the pod name starting with 'iot-backend-mariadb-'. Login to the MariaDB container using the kubectl command kubectl exec -it <pod name> -- /bin/bash Tip (replace the <pod name> in command above with correct value). Inside the container execute ss -tulpn and check the port MariaDB is listing to for incoming connections. You can also check the version of MariaDB using the shell command: mysql --version Now connect to the MariaDB from the container shell and check Databases and Tables. Use mysql -u root -pcisco123 command to login to MaraDB. On mariaDB shell, use show databases; command to list all the databases. Look for ' sensor_db ' in the output (this is the database where we are storing the sensor data). Switch to sensor_db using the command use sensor_db; and list all the tables in this database using the command show tables; You should see only one table with the name 'sensor_data' ; Try to list the data from this table using the SQL statement select * from sensor_data; Now check the record count in this table using the sql statement: select count(*) from sensor_data; Repeat the SQL from step 8 several times and check if the record count is increasing (each sensor would send the data after every 10 seconds). Use exit command at MariaDB prompt to exit the DB shell. Use exit command again to exit 'iot-backend-mariadb' container shell. Task 3: Connect to the REST API Agent container and find the port it is listing on for incoming REST calls. Find the pod name for 'REST API Agent' using the kubectl command kubectl get pods and locate the pod name starting with 'iot-backend-rest-api-agent-' (you may see more than one pod for this service, pick any one). login to the REST API Agent container using the kubectl command kubectl exec -it <pod name> -- /bin/ash Tip (replace the <pod name> in command above with correct value). Execute netstat -an command on the container shell and check the output. This container listens on port '5050' for incoming REST connections and connects with MariaDB using port '3306' (Connection to DB will timeout in case there are no requests coming in). Use exit command to come out of the container shell. Task 4: Check the logs messages from 'REST API Agent'. Find the pod name for 'REST API Agent' using the kubectl command kubectl get pods and locate the pod name starting with 'iot-backend-rest-api-agent-' (you may see more than one pod for this service, pick any one). Check the logs using the kubectl command kubectl logs <pod name> Tip (replace the <pod name> in command above with correct value). Task 5: How the traffic would be distributed, if you have multiple Kubernetes Pods behind a Kubernetes NodePort Service? Right click on top of your putty window and click on \"Duplicate Session\" (You should have two putty windows side by side logged into same linux jumphost). In the first putty window (logged into linux jumphost) execute the command kubectl get nodes -o wide and note down the Kubernetes master node's external IP address. Open a web browser on your machine and access the following url http://<kubernetes node's external ip>:<port>/temperature In the first putty window, run kubectl get pods on the linux jumphost. In the output you should see 2 pods for 'iot-backend-rest-api-agent'. In the first putty window, run the command watch kubectl logs <first pod name> (Use the first 'iot-backend-rest-api-agent' pod out of 2 pods that you saw in the output of the command executed in step#5). In the Second putty window, display the logs for second 'iot-backend-rest-api-agent' pod using the command watch kubectl logs <second pod name> Refresh or reload the web page with the url you used in step 4. Repeat step 8 and check the requests hitting the kubernetes pods in the log messages.","title":"Explore Backend App and Kubernetes Dashboard:"},{"location":"basic_kubectl_cmds/","text":"kubectl version kubectl get nodes kubectl run --image= --port= Kubernetes Deployments: A Deployment controller provides declarative updates for Pods and ReplicaSets. You describe a desired state in a Deployment object, and the Deployment controller changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments. Create Deployment Use the following command to create a deployment using a yaml file - kubectl create -f <yaml file path> List Deployments: Use the following command to list all kubernetes deployments - kubectl get deployments Deployment Details: Use the following command to display the details of deployment - kubectl describe deployment Scale Deployment: Use the following command to scale (up/down) a kubernetes deployment - kubectl scale deployments/<deployment_name> --replicas=<number of replicas> Delete Deployment: Use the following command to delete a deployment - kubectl delete deployment <deployment_name> Kubernetes Pods: A Pod represents a unit of deployment: a single instance of an application in Kubernetes, which might consist of either a single container or a small number of containers that are tightly coupled and that share resources. List Pods: Use the following command to list all the pods: kubectl get pods or use wide option to see more details - kubectl get pods -o wide List Pods Filter: You can filter the pods using the labels used in deployment - kubectl get pods -l <label_name>=<label_value> Pod Details: Use the following command to see the containers and used images for pods - kubectl describe pods Pod Logs: Use the following command to check the pod logs - kubectl logs <pod_name> Kubernetes Services: A Kubernetes Service is an abstraction which defines a logical set of Pods and a policy by which to access them - sometimes called a micro-service. List Service: Use the following command to list the current Services - kubectl get services You can filter the services using the labels used in deployment - kubectl get services -l <label_name>=<label_value> Create Service: Use the following command to create a new service - kubectl expose deployment/<deployment_name> --type=\"NodePort\" --port <port> Service Details: Use the following command to find out what port was opened externally (by the NodePort option) - kubectl describe services/<service_name> Delete Service: Use the following command to delete a service - kubectl delete service/<service_name> or kubectl delete service -l <label_name>=<label_value> Parse Service Node Port: Use the following script to filterout the node-port of a service (change the service name) - export NODE_PORT=$(kubectl get services/<service-name> -o go-template='{{(index .spec.ports 0).nodePort}}') echo NODE_PORT=$NODE_PORT Kubernetes Secrets: A Secret is an object that stores a piece of sensitive data like a password or key. List Secrets: Use the following command to list all secrets kubectl get secrets Secret Details: Use the following command to list the secret details - kubectl describe secrets/<secret_name> Create Secret: Use the following command to create secret - kubectl create secret generic <secret_name> --from-literal=<key_name>=<key_value> Delete Secret: use the following command to delete a secret - kubectl delete secret <secret_name> Interacting with Pod Containers List Env Variables: Use the following command to list the environment variables - kubectl exec <pod_name> env Access Container Shell: Use the following command to access bash shell in a container - kubectl exec -ti <pod_name> bash Note: To close your container connection type ' exit '. ========================================= kubectl proxy curl http://localhost:8001/version export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{\"\\n\"}}{{end}}') echo Name of the Pod: $POD_NAME curl http://localhost:8001/api/v1/namespaces/default/pods/$POD_NAME/proxy =========================================","title":"Basic kubectl cmds"},{"location":"basic_kubectl_cmds/#kubernetes-deployments","text":"A Deployment controller provides declarative updates for Pods and ReplicaSets. You describe a desired state in a Deployment object, and the Deployment controller changes the actual state to the desired state at a controlled rate. You can define Deployments to create new ReplicaSets, or to remove existing Deployments and adopt all their resources with new Deployments. Create Deployment Use the following command to create a deployment using a yaml file - kubectl create -f <yaml file path> List Deployments: Use the following command to list all kubernetes deployments - kubectl get deployments Deployment Details: Use the following command to display the details of deployment - kubectl describe deployment Scale Deployment: Use the following command to scale (up/down) a kubernetes deployment - kubectl scale deployments/<deployment_name> --replicas=<number of replicas> Delete Deployment: Use the following command to delete a deployment - kubectl delete deployment <deployment_name>","title":"Kubernetes Deployments:"},{"location":"basic_kubectl_cmds/#kubernetes-pods","text":"A Pod represents a unit of deployment: a single instance of an application in Kubernetes, which might consist of either a single container or a small number of containers that are tightly coupled and that share resources. List Pods: Use the following command to list all the pods: kubectl get pods or use wide option to see more details - kubectl get pods -o wide List Pods Filter: You can filter the pods using the labels used in deployment - kubectl get pods -l <label_name>=<label_value> Pod Details: Use the following command to see the containers and used images for pods - kubectl describe pods Pod Logs: Use the following command to check the pod logs - kubectl logs <pod_name>","title":"Kubernetes Pods:"},{"location":"basic_kubectl_cmds/#kubernetes-services","text":"A Kubernetes Service is an abstraction which defines a logical set of Pods and a policy by which to access them - sometimes called a micro-service. List Service: Use the following command to list the current Services - kubectl get services You can filter the services using the labels used in deployment - kubectl get services -l <label_name>=<label_value> Create Service: Use the following command to create a new service - kubectl expose deployment/<deployment_name> --type=\"NodePort\" --port <port> Service Details: Use the following command to find out what port was opened externally (by the NodePort option) - kubectl describe services/<service_name> Delete Service: Use the following command to delete a service - kubectl delete service/<service_name> or kubectl delete service -l <label_name>=<label_value> Parse Service Node Port: Use the following script to filterout the node-port of a service (change the service name) - export NODE_PORT=$(kubectl get services/<service-name> -o go-template='{{(index .spec.ports 0).nodePort}}') echo NODE_PORT=$NODE_PORT","title":"Kubernetes Services:"},{"location":"basic_kubectl_cmds/#kubernetes-secrets","text":"A Secret is an object that stores a piece of sensitive data like a password or key. List Secrets: Use the following command to list all secrets kubectl get secrets Secret Details: Use the following command to list the secret details - kubectl describe secrets/<secret_name> Create Secret: Use the following command to create secret - kubectl create secret generic <secret_name> --from-literal=<key_name>=<key_value> Delete Secret: use the following command to delete a secret - kubectl delete secret <secret_name>","title":"Kubernetes Secrets:"},{"location":"basic_kubectl_cmds/#interacting-with-pod-containers","text":"List Env Variables: Use the following command to list the environment variables - kubectl exec <pod_name> env Access Container Shell: Use the following command to access bash shell in a container - kubectl exec -ti <pod_name> bash Note: To close your container connection type ' exit '.","title":"Interacting with Pod Containers"},{"location":"basic_kubectl_cmds/#_1","text":"kubectl proxy curl http://localhost:8001/version export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{\"\\n\"}}{{end}}') echo Name of the Pod: $POD_NAME curl http://localhost:8001/api/v1/namespaces/default/pods/$POD_NAME/proxy =========================================","title":"========================================="},{"location":"deploy_backend/","text":"Deploy the Backend Application Components on IKS Kubernetes Cluster (IKS Tenant Cluster) In this section you would deploy the backend components of the IoT Application on the Kubernetes cluster deployed on-prem using Intesight. Following diagram shows the high-level architecture of these backend application containers Login to Kubernetes Master CLI Shell: SSH into Linux Jumphost using Putty, use predefined session named \"ubuntu-terminal\" (198.18.133.11) - use password C1sco12345. From here you will deploy two microservices in on-premise Kubernetes Cluster and one microservice in AWS EKS Kubernetes cluster. You will see how microservices talks to each other and how to establish necessary communication. 1. Deploy MariaDB Databse: MariaDB will be used in the backend to save the sensor data received from AWS IoT platform over MQTT protocol. For this we would create following objects - Secret Persistent Volume Claim (PVC) MariaDB Deployment ClusterIP Service (Headless Service) Following diagram shows the relationship between these objects - 1.1 Create Kubernetes Secret for MariaDB: A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might otherwise be put in a Pod specification or in an image; putting it in a Secret object allows for more control over how it is used, and reduces the risk of accidental exposure. The MariaDB container image uses an environment variable named as 'MYSQL_ROOT_PASSWORD', it hold the root password required to access the database. So you would create a new secret with 'password' key (value as 'cisco123') which would later be used in mariaDB deployment yaml file. 1.1.1: Switch context to on-premise Kubernetes Cluster - Change context of kubectl command to access on-premise Kubernetes Cluster. kubectl config use-context admin@CLUS-IKS-1 kubectl config get-contexts 1.1.2: Create DB Password Secret - Use the following command to create a new secret on your kubernetes cluster - kubectl create secret generic mariadb-root-pass --from-literal=password=cisco123 1.1.3: Verify DB Password Secret - Check if the secret was created successfully or not - kubectl get secret mariadb-root-pass You should have the output similar to the following screenshot - 1.2 Create Kubernetes Persistent Volume Claim for MariaDB: A Persistent Volume Claim (PVC) is a request for storage by a user. It is similar to a pod. Pods consume node resources and PVCs consume Persistent Volume (PV) resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes (e.g., can be mounted once read/write or many times read-only). To keep the sensor data safe during Pod restarts, you would create a new Persistent Volume Claim. The following yaml definition would be used to create the 'PersistentVolumeClaim' - --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : mariadb-pv-claim labels : app : iot-backend spec : accessModes : - ReadWriteOnce resources : requests : storage : 2Gi * 1.2.1: Create Persistent Volume Claim - Use the following command to create a new Persistent Volume Claim for MariaDB Pod - kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/Mariadb/mariadb_persistent_volume.yaml 1.2.2: Verify Persistent Volume Claim - Check if the PVC was created successfully or not - kubectl get pvc mariadb-pv-claim You should have the output similar to the following screenshot - Caution It can take up to a few minutes for the PVs to be provisioned. DO NOT procced futher till the PVC deployment gets completed. 1.3 Deploy MariaDB on Kubernetes: MariaDB is a community-developed fork of the MySQL relational database management system intended to remain free under the GNU GPL. Development is led by some of the original developers of MySQL, who forked it due to concerns over its acquisition by Oracle Corporation. MariaDB intends to maintain high compatibility with MySQL, ensuring a drop-in replacement capability with library binary parity and exact matching with MySQL APIs and commands. The following yaml definition will be used to deploy MariaDB pod - --- apiVersion : apps/v1 kind : Deployment metadata : name : iot-backend-mariadb labels : app : iot-backend spec : selector : matchLabels : app : iot-backend tier : mariadb strategy : type : Recreate template : metadata : labels : app : iot-backend tier : mariadb spec : containers : - image : mariadb:10.3 name : mariadb env : - name : MYSQL_ROOT_PASSWORD valueFrom : secretKeyRef : name : mariadb-root-pass key : password ports : - containerPort : 3306 name : mariadb volumeMounts : - name : mariadb-persistent-storage mountPath : /var/lib/mysql volumes : - name : mariadb-persistent-storage persistentVolumeClaim : claimName : mariadb-pv-claim * 1.3.1: Deploy MariaDB - Use the following command to create a MariaDB kubernetes deployment kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/Mariadb/mariadb_deployment.yaml 1.3.2: Check Deployment Status - Use the following command to check if the kubernetes deployment was successfully created or not kubectl get deployment iot-backend-mariadb You should have the output similar to the following screenshot 1.3.3: Check Pod Status - Use the following command to check if the 'iot-backend-mariadb' pod is in ' Running ' state kubectl get pods Caution Kubernetes may take some time to deploy the MariaDB. DO NOT proceed further till the time DB Pod is up. 1.4 Create Kubernetes LoadBalancer Service for MariaDB: Since the MariaDB will be accessed by other services like 'MQTT to DB Agent' and 'REST API Agent'; you need to expose it externally, since 'MQTT to DB Agent' will be running on another Kubernetes Cluster A LoadBalancer Service provides external access to your application from systems outside of Kubernetes. LoadBalancer service is exposed under dedicated VIP address, routable in external network. Traffic directed to this IP address is load balanced by Kubernetes across Kubernetes nodes. Following yaml definition would be used to create the LoadBalancer Service for MariaDB --- apiVersion : v1 kind : Service metadata : name : mariadb-service labels : app : iot-backend spec : ports : - protocol : TCP port : 3306 selector : app : iot-backend tier : mariadb type : \"LoadBalancer\" 1.4.1: Expose MariaDB to other Pods - Create a new kubernetes service using the following command kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/Mariadb/mariadb_service.yaml 1.4.2: Verify Service Status - Use the following command to check if the kubernetes service was deployed successfully or not kubectl get service mariadb-service You should have the output similar to the following screenshot 2. Deploy REST API Agent on Kubernetes: The 'REST API Agent' would act as the gateway to the backend application. It will listen to the incoming HTTP requests from the frontend application that you will deploy on AWS. 2.1 Deploy REST API Agent: The following yaml definition will be used to create REST API Agent pods --- apiVersion : apps/v1 kind : Deployment metadata : name : iot-backend-rest-api-agent labels : app : iot-backend-rest-api-agent spec : replicas : 1 selector : matchLabels : app : iot-backend-rest-api-agent tier : rest-api-agent strategy : type : Recreate template : metadata : labels : app : iot-backend-rest-api-agent tier : rest-api-agent spec : containers : - image : pradeesi/rest_api_agent:v1 name : rest-api-agent env : - name : DB_PASSWORD valueFrom : secretKeyRef : name : mariadb-root-pass key : password 2.1.1: Deploy REST API Agent - Use the following command to create the rest-api-agent kubernetes deployment kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/REST_API_Agent/rest_api_agent.yaml 2.1.2: Check Deployment Status - Use the following command to check if the kubernetes deployment was created successfully or not kubectl get deployment iot-backend-rest-api-agent You should have the output similar to the following screenshot 2.1.3: Check Pod Status - Use the following command to check if the 'iot-backend-rest-api-agent' pod is in ' Running ' state kubectl get pods Tip You may check the Pod Logs using the command ' kubectl logs <pod_name> ' 2.2 Create Kubernetes NodePort Service for REST API Agent: Since the frontend app from AWS would access the REST APIs exposed by the 'REST API Agent', you need to create a new kubernetes service for it. The following yaml definition would be used for to create a NodePort Service for the REST API Agent - --- apiVersion : v1 kind : Service metadata : name : rest-api-agent-service labels : app : iot-backend spec : ports : - protocol : TCP port : 5050 selector : app : iot-backend-rest-api-agent tier : rest-api-agent type : \"NodePort\" 2.2.1: Create REST API Agent NodePort Service - You can create a new kubernetes service using the following command kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/REST_API_Agent/rest_api_agent_service_node_port.yaml 2.2.2: Check REST API Agent Service Status - You can use the following command to check if the kubernetes service was created successfully or not kubectl get service rest-api-agent-service You should have the output similar to the following screenshot 2.3 Locate the IP and Port to Access Node-Port Service for REST API Agent: You need to find the NodePort and Kubernetes Node external IP to access the 'rest-api-agent. Use the following command to display the port exposed by 'rest-api-agent-service' kubectl get service rest-api-agent-service Use the following command to display the 'External-IP' of you kubernetes nodes kubectl get nodes -o wide Following screenshot highlights the Port and Node IPs in the command outputs Important Note down the Node External IP Address and NodePort Service Port Number. These values will be used in next section for deploying the frontend app as the environment variables values (' BACKEND_HOST ' and ' BACKEND_PORT '). 3. Deploy MQTT to DB Agent on Kubernetes: This part of deployment will be done in Public Cloud (AWS) K8s tenant. 'MQTT to DB Agent' will subscribe to the MQTT Topic and listen to the incoming sensor data from AWS IoT platform. It will then parse the sensor data and insert it into the MariaDB. The following yaml definition will be used to create the MQTT to DB Agent pods --- apiVersion : apps/v1 kind : Deployment metadata : name : iot-backend-mqtt-db-agent labels : app : iot-backend tier : mqtt-db-agent spec : selector : matchLabels : app : iot-backend-mqtt-db-agent strategy : type : Recreate template : metadata : labels : app : iot-backend-mqtt-db-agent spec : containers : - image : eu.gcr.io/fwardz001-poc-ci1s/mqtt_db_plugin:v9 name : mqtt-db-agent env : - name : DB_PASSWORD valueFrom : secretKeyRef : name : mariadb-root-pass key : password * 3.1: Switch context to AWS EKS Kubernetes Cluster - Change context of kubectl command to access AWS Kubernetes Cluster. kubectl config get-contexts kubectl config use-context <AWS context-name from previous command output> kubectl config get-contexts 3.2: Create DB Password Secret - Use the following command to create a new secret on your kubernetes cluster kubectl create secret generic mariadb-root-pass --from-literal=password=cisco123 3.3: Verify DB Password Secret - Check if the secret was created successfully or not kubectl get secret mariadb-root-pass You should have the output similar to the following screenshot 3.4: Create external service MQTT needs to send data to database that is deployed in IKS on-prem Kubernetes Cluster. MQTT application is configured to contact with MariaDB using following internal DNS name: mariadb-service . We need to configure Kubernetes to resolve this name to a particular LoadBalancer IP that has been allocated to your mariadb-service in IKS on-premise Kubernetes Cluster. For this we will define service and manually add endpoint IP that this service will resolve to. In the Endpoint definition you have to specify your LoadBalancer IP address from on-premise Kubernetes Cluster allocated to mariadb-service . --- apiVersion : v1 kind : Service metadata : name : mariadb-service spec : ports : - name : sql protocol : TCP port : 3306 targetPort : 3306 --- apiVersion : v1 kind : Endpoints metadata : name : mariadb-service subsets : - addresses : - ip : LoadBalancerIP ## Specify mariadb-service LoadBalancer IP from step 1.4 ports : - port : 3306 name : sql Download following definition file: wget https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/MQTT_DB_Agent/mariadb-ext-service-eks.yaml Check string to be replaced by LoadBalancerIP allocated to mariadb-service from Step 1.4.2 cat mariadb-ext-service-eks.yaml Change <mariadb-service_LoadBalancer_IP> with IP address of your load balancer IP, replace 198.18.134.XXX in sed command below with the IP address of LoadBalancer IP allocated to mariadb-service in on-premise Kubernetes Cluster. Tip Copy following command to notepad FIRST, change IP address and then paste it to Terminal. Command to edit: sed -i 's/<mariadb-service_LoadBalancer_IP>/198.18.134.XXX/g' mariadb-ext-service-eks.yaml Check the manifest file after change of IP: cat mariadb-ext-service-eks.yaml Important Make sure that the IP address of mariadb-service you specified is correct. Apply updated manifest to create external service access: kubectl apply -f mariadb-ext-service-eks.yaml Check services and associated endpoints: kubectl get svc,endpoints 3.5: Deploy MQTT to DB Agent - Use the following command to create mqtt-to-db-agent kubernetes deployment kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/MQTT_DB_Agent/mqtt_db_agent_deployment.yaml 3.6: Check Deployment Status - Use the following command to check if the kubernetes deployment was created successfully or not kubectl get deployment iot-backend-mqtt-db-agent You should have the output similar to the following screenshot 3.5: Check Pod Status - Use the following command to check if the 'iot-backend-mqtt-db-agent' pod is in ' Running ' state kubectl get pods 4 Test the REST APIs Exposed by REST API Agent Service: To test the REST API service try to access following url from your web browser (use the node's external ip and service port from the previous section # 2.3) - If you haven't note the IP and port information earlier, please follow those steps: Change kubectl context to CLUS-IKS-1 kubectl config use-context admin@CLUS-IKS-1 kubectl config get-contexts Use the following command to display the port exposed by 'rest-api-agent-service' kubectl get service rest-api-agent-service Use the following command to display the 'External-IP' of you kubernetes nodes kubectl get nodes -o wide Following screenshot highlights the Port and Node IPs in the command outputs Now you have to open Chrome browser and specify URL based on pattern - http://<kubernetes node's external ip>:<nodePort> i.e. http://198.18.134.103:30276 If your REST API Agent is working properly, you should see 'Welcome to the API Service...!' message on your browser as shown in the following screenshot - Following are the other urls that you could test - http://<kubernetes node's external ip>:<nodePort>/cities http://<kubernetes node's external ip>:<nodePort>/temperature http://<kubernetes node's external ip>:<nodePort>/humidity http://<kubernetes node's external ip>:<nodePort>/sensor_data/city","title":"Deploy the Backend Application Components on IKS Kubernetes Cluster (IKS Tenant Cluster)"},{"location":"deploy_backend/#deploy-the-backend-application-components-on-iks-kubernetes-cluster-iks-tenant-cluster","text":"In this section you would deploy the backend components of the IoT Application on the Kubernetes cluster deployed on-prem using Intesight. Following diagram shows the high-level architecture of these backend application containers","title":"Deploy the Backend Application Components on IKS Kubernetes Cluster (IKS Tenant Cluster)"},{"location":"deploy_backend/#login-to-kubernetes-master-cli-shell","text":"SSH into Linux Jumphost using Putty, use predefined session named \"ubuntu-terminal\" (198.18.133.11) - use password C1sco12345. From here you will deploy two microservices in on-premise Kubernetes Cluster and one microservice in AWS EKS Kubernetes cluster. You will see how microservices talks to each other and how to establish necessary communication.","title":"Login to Kubernetes Master CLI Shell:"},{"location":"deploy_backend/#1-deploy-mariadb-databse","text":"MariaDB will be used in the backend to save the sensor data received from AWS IoT platform over MQTT protocol. For this we would create following objects - Secret Persistent Volume Claim (PVC) MariaDB Deployment ClusterIP Service (Headless Service) Following diagram shows the relationship between these objects -","title":"1. Deploy MariaDB Databse:"},{"location":"deploy_backend/#11-create-kubernetes-secret-for-mariadb","text":"A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. Such information might otherwise be put in a Pod specification or in an image; putting it in a Secret object allows for more control over how it is used, and reduces the risk of accidental exposure. The MariaDB container image uses an environment variable named as 'MYSQL_ROOT_PASSWORD', it hold the root password required to access the database. So you would create a new secret with 'password' key (value as 'cisco123') which would later be used in mariaDB deployment yaml file. 1.1.1: Switch context to on-premise Kubernetes Cluster - Change context of kubectl command to access on-premise Kubernetes Cluster. kubectl config use-context admin@CLUS-IKS-1 kubectl config get-contexts 1.1.2: Create DB Password Secret - Use the following command to create a new secret on your kubernetes cluster - kubectl create secret generic mariadb-root-pass --from-literal=password=cisco123 1.1.3: Verify DB Password Secret - Check if the secret was created successfully or not - kubectl get secret mariadb-root-pass You should have the output similar to the following screenshot -","title":"1.1 Create Kubernetes Secret for MariaDB:"},{"location":"deploy_backend/#12-create-kubernetes-persistent-volume-claim-for-mariadb","text":"A Persistent Volume Claim (PVC) is a request for storage by a user. It is similar to a pod. Pods consume node resources and PVCs consume Persistent Volume (PV) resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes (e.g., can be mounted once read/write or many times read-only). To keep the sensor data safe during Pod restarts, you would create a new Persistent Volume Claim. The following yaml definition would be used to create the 'PersistentVolumeClaim' - --- apiVersion : v1 kind : PersistentVolumeClaim metadata : name : mariadb-pv-claim labels : app : iot-backend spec : accessModes : - ReadWriteOnce resources : requests : storage : 2Gi * 1.2.1: Create Persistent Volume Claim - Use the following command to create a new Persistent Volume Claim for MariaDB Pod - kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/Mariadb/mariadb_persistent_volume.yaml 1.2.2: Verify Persistent Volume Claim - Check if the PVC was created successfully or not - kubectl get pvc mariadb-pv-claim You should have the output similar to the following screenshot - Caution It can take up to a few minutes for the PVs to be provisioned. DO NOT procced futher till the PVC deployment gets completed.","title":"1.2 Create Kubernetes Persistent Volume Claim for MariaDB:"},{"location":"deploy_backend/#13-deploy-mariadb-on-kubernetes","text":"MariaDB is a community-developed fork of the MySQL relational database management system intended to remain free under the GNU GPL. Development is led by some of the original developers of MySQL, who forked it due to concerns over its acquisition by Oracle Corporation. MariaDB intends to maintain high compatibility with MySQL, ensuring a drop-in replacement capability with library binary parity and exact matching with MySQL APIs and commands. The following yaml definition will be used to deploy MariaDB pod - --- apiVersion : apps/v1 kind : Deployment metadata : name : iot-backend-mariadb labels : app : iot-backend spec : selector : matchLabels : app : iot-backend tier : mariadb strategy : type : Recreate template : metadata : labels : app : iot-backend tier : mariadb spec : containers : - image : mariadb:10.3 name : mariadb env : - name : MYSQL_ROOT_PASSWORD valueFrom : secretKeyRef : name : mariadb-root-pass key : password ports : - containerPort : 3306 name : mariadb volumeMounts : - name : mariadb-persistent-storage mountPath : /var/lib/mysql volumes : - name : mariadb-persistent-storage persistentVolumeClaim : claimName : mariadb-pv-claim * 1.3.1: Deploy MariaDB - Use the following command to create a MariaDB kubernetes deployment kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/Mariadb/mariadb_deployment.yaml 1.3.2: Check Deployment Status - Use the following command to check if the kubernetes deployment was successfully created or not kubectl get deployment iot-backend-mariadb You should have the output similar to the following screenshot 1.3.3: Check Pod Status - Use the following command to check if the 'iot-backend-mariadb' pod is in ' Running ' state kubectl get pods Caution Kubernetes may take some time to deploy the MariaDB. DO NOT proceed further till the time DB Pod is up.","title":"1.3 Deploy MariaDB on Kubernetes:"},{"location":"deploy_backend/#14-create-kubernetes-loadbalancer-service-for-mariadb","text":"Since the MariaDB will be accessed by other services like 'MQTT to DB Agent' and 'REST API Agent'; you need to expose it externally, since 'MQTT to DB Agent' will be running on another Kubernetes Cluster A LoadBalancer Service provides external access to your application from systems outside of Kubernetes. LoadBalancer service is exposed under dedicated VIP address, routable in external network. Traffic directed to this IP address is load balanced by Kubernetes across Kubernetes nodes. Following yaml definition would be used to create the LoadBalancer Service for MariaDB --- apiVersion : v1 kind : Service metadata : name : mariadb-service labels : app : iot-backend spec : ports : - protocol : TCP port : 3306 selector : app : iot-backend tier : mariadb type : \"LoadBalancer\" 1.4.1: Expose MariaDB to other Pods - Create a new kubernetes service using the following command kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/Mariadb/mariadb_service.yaml 1.4.2: Verify Service Status - Use the following command to check if the kubernetes service was deployed successfully or not kubectl get service mariadb-service You should have the output similar to the following screenshot","title":"1.4 Create Kubernetes LoadBalancer Service for MariaDB:"},{"location":"deploy_backend/#2-deploy-rest-api-agent-on-kubernetes","text":"The 'REST API Agent' would act as the gateway to the backend application. It will listen to the incoming HTTP requests from the frontend application that you will deploy on AWS.","title":"2. Deploy REST API Agent on Kubernetes:"},{"location":"deploy_backend/#21-deploy-rest-api-agent","text":"The following yaml definition will be used to create REST API Agent pods --- apiVersion : apps/v1 kind : Deployment metadata : name : iot-backend-rest-api-agent labels : app : iot-backend-rest-api-agent spec : replicas : 1 selector : matchLabels : app : iot-backend-rest-api-agent tier : rest-api-agent strategy : type : Recreate template : metadata : labels : app : iot-backend-rest-api-agent tier : rest-api-agent spec : containers : - image : pradeesi/rest_api_agent:v1 name : rest-api-agent env : - name : DB_PASSWORD valueFrom : secretKeyRef : name : mariadb-root-pass key : password 2.1.1: Deploy REST API Agent - Use the following command to create the rest-api-agent kubernetes deployment kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/REST_API_Agent/rest_api_agent.yaml 2.1.2: Check Deployment Status - Use the following command to check if the kubernetes deployment was created successfully or not kubectl get deployment iot-backend-rest-api-agent You should have the output similar to the following screenshot 2.1.3: Check Pod Status - Use the following command to check if the 'iot-backend-rest-api-agent' pod is in ' Running ' state kubectl get pods Tip You may check the Pod Logs using the command ' kubectl logs <pod_name> '","title":"2.1 Deploy REST API Agent:"},{"location":"deploy_backend/#22-create-kubernetes-nodeport-service-for-rest-api-agent","text":"Since the frontend app from AWS would access the REST APIs exposed by the 'REST API Agent', you need to create a new kubernetes service for it. The following yaml definition would be used for to create a NodePort Service for the REST API Agent - --- apiVersion : v1 kind : Service metadata : name : rest-api-agent-service labels : app : iot-backend spec : ports : - protocol : TCP port : 5050 selector : app : iot-backend-rest-api-agent tier : rest-api-agent type : \"NodePort\" 2.2.1: Create REST API Agent NodePort Service - You can create a new kubernetes service using the following command kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/REST_API_Agent/rest_api_agent_service_node_port.yaml 2.2.2: Check REST API Agent Service Status - You can use the following command to check if the kubernetes service was created successfully or not kubectl get service rest-api-agent-service You should have the output similar to the following screenshot","title":"2.2 Create Kubernetes NodePort Service for REST API Agent:"},{"location":"deploy_backend/#23-locate-the-ip-and-port-to-access-node-port-service-for-rest-api-agent","text":"You need to find the NodePort and Kubernetes Node external IP to access the 'rest-api-agent. Use the following command to display the port exposed by 'rest-api-agent-service' kubectl get service rest-api-agent-service Use the following command to display the 'External-IP' of you kubernetes nodes kubectl get nodes -o wide Following screenshot highlights the Port and Node IPs in the command outputs Important Note down the Node External IP Address and NodePort Service Port Number. These values will be used in next section for deploying the frontend app as the environment variables values (' BACKEND_HOST ' and ' BACKEND_PORT ').","title":"2.3 Locate the IP and Port to Access Node-Port Service for REST API Agent:"},{"location":"deploy_backend/#3-deploy-mqtt-to-db-agent-on-kubernetes","text":"This part of deployment will be done in Public Cloud (AWS) K8s tenant. 'MQTT to DB Agent' will subscribe to the MQTT Topic and listen to the incoming sensor data from AWS IoT platform. It will then parse the sensor data and insert it into the MariaDB. The following yaml definition will be used to create the MQTT to DB Agent pods --- apiVersion : apps/v1 kind : Deployment metadata : name : iot-backend-mqtt-db-agent labels : app : iot-backend tier : mqtt-db-agent spec : selector : matchLabels : app : iot-backend-mqtt-db-agent strategy : type : Recreate template : metadata : labels : app : iot-backend-mqtt-db-agent spec : containers : - image : eu.gcr.io/fwardz001-poc-ci1s/mqtt_db_plugin:v9 name : mqtt-db-agent env : - name : DB_PASSWORD valueFrom : secretKeyRef : name : mariadb-root-pass key : password * 3.1: Switch context to AWS EKS Kubernetes Cluster - Change context of kubectl command to access AWS Kubernetes Cluster. kubectl config get-contexts kubectl config use-context <AWS context-name from previous command output> kubectl config get-contexts 3.2: Create DB Password Secret - Use the following command to create a new secret on your kubernetes cluster kubectl create secret generic mariadb-root-pass --from-literal=password=cisco123 3.3: Verify DB Password Secret - Check if the secret was created successfully or not kubectl get secret mariadb-root-pass You should have the output similar to the following screenshot 3.4: Create external service MQTT needs to send data to database that is deployed in IKS on-prem Kubernetes Cluster. MQTT application is configured to contact with MariaDB using following internal DNS name: mariadb-service . We need to configure Kubernetes to resolve this name to a particular LoadBalancer IP that has been allocated to your mariadb-service in IKS on-premise Kubernetes Cluster. For this we will define service and manually add endpoint IP that this service will resolve to. In the Endpoint definition you have to specify your LoadBalancer IP address from on-premise Kubernetes Cluster allocated to mariadb-service . --- apiVersion : v1 kind : Service metadata : name : mariadb-service spec : ports : - name : sql protocol : TCP port : 3306 targetPort : 3306 --- apiVersion : v1 kind : Endpoints metadata : name : mariadb-service subsets : - addresses : - ip : LoadBalancerIP ## Specify mariadb-service LoadBalancer IP from step 1.4 ports : - port : 3306 name : sql Download following definition file: wget https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/MQTT_DB_Agent/mariadb-ext-service-eks.yaml Check string to be replaced by LoadBalancerIP allocated to mariadb-service from Step 1.4.2 cat mariadb-ext-service-eks.yaml Change <mariadb-service_LoadBalancer_IP> with IP address of your load balancer IP, replace 198.18.134.XXX in sed command below with the IP address of LoadBalancer IP allocated to mariadb-service in on-premise Kubernetes Cluster. Tip Copy following command to notepad FIRST, change IP address and then paste it to Terminal. Command to edit: sed -i 's/<mariadb-service_LoadBalancer_IP>/198.18.134.XXX/g' mariadb-ext-service-eks.yaml Check the manifest file after change of IP: cat mariadb-ext-service-eks.yaml Important Make sure that the IP address of mariadb-service you specified is correct. Apply updated manifest to create external service access: kubectl apply -f mariadb-ext-service-eks.yaml Check services and associated endpoints: kubectl get svc,endpoints 3.5: Deploy MQTT to DB Agent - Use the following command to create mqtt-to-db-agent kubernetes deployment kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Backend/MQTT_DB_Agent/mqtt_db_agent_deployment.yaml 3.6: Check Deployment Status - Use the following command to check if the kubernetes deployment was created successfully or not kubectl get deployment iot-backend-mqtt-db-agent You should have the output similar to the following screenshot 3.5: Check Pod Status - Use the following command to check if the 'iot-backend-mqtt-db-agent' pod is in ' Running ' state kubectl get pods","title":"3. Deploy MQTT to DB Agent on Kubernetes:"},{"location":"deploy_backend/#4-test-the-rest-apis-exposed-by-rest-api-agent-service","text":"To test the REST API service try to access following url from your web browser (use the node's external ip and service port from the previous section # 2.3) - If you haven't note the IP and port information earlier, please follow those steps: Change kubectl context to CLUS-IKS-1 kubectl config use-context admin@CLUS-IKS-1 kubectl config get-contexts Use the following command to display the port exposed by 'rest-api-agent-service' kubectl get service rest-api-agent-service Use the following command to display the 'External-IP' of you kubernetes nodes kubectl get nodes -o wide Following screenshot highlights the Port and Node IPs in the command outputs Now you have to open Chrome browser and specify URL based on pattern - http://<kubernetes node's external ip>:<nodePort> i.e. http://198.18.134.103:30276 If your REST API Agent is working properly, you should see 'Welcome to the API Service...!' message on your browser as shown in the following screenshot - Following are the other urls that you could test - http://<kubernetes node's external ip>:<nodePort>/cities http://<kubernetes node's external ip>:<nodePort>/temperature http://<kubernetes node's external ip>:<nodePort>/humidity http://<kubernetes node's external ip>:<nodePort>/sensor_data/city","title":"4 Test the REST APIs Exposed by REST API Agent Service:"},{"location":"deploy_frontend-aws/","text":"Deploy the Frontend Application Components on AWS In this section you would deploy the frontend components of the IoT Application on the Amazon Cloud. Following diagram shows the high-level architecture of these frontend application containers Login to Kubernetes Master CLI Shell: SSH into Linux Jumphost using Putty \"ubuntu-terminal\" (198.18.133.11) - use your credentials. Switch to context 'arn:aws:eks:us-east-1:782256189490:cluster/CLUS-EKS-X'. kubectl config get-contexts kubectl config use-context <AWS context-name from previous command output> kubectl config get-contexts 1. Deploy frontend-iot: Frontend is nginx web server connected to frontend-server which connect to REST-API Agent over already created Site-2-Site connection. In this section you will configure: Kubernetes ConfigMap Kubernetes frontend-iot Deployment Kubernetes frontend-iot Load-Balancer AWS Service You will create kubernetes deployment for frontend app and expose it to the internet using Kubernetes Load Balancer Service as shown in the following diagram 1.1 Create ConfigMap Create now configmap which will be used by your FrontEnd deployment. Replace ' BACKEND_HOST ' and ' BACKEND_PORT ' in the configmap command (below) with value noted down in section 2.3 in chapter Deploy REST API Agent on Kubernetes . Following screenshot highlights the Port and Node IPs in the command outputs in on-prem cluster. Warning External IP addresses of your on-prem workers can vary from one visible on figure below. Important Note down the Node External IP Address of one of on-prem WORKER and NodePort Service Port Number. ConfigMap create command - Copy it to notepad FIRST, edit and paste to Terminal: kubectl create configmap iot-frontend-config --from-literal=BACKEND_HOST=<NODE_IP> --from-literal=BACKEND_PORT=<PORT> 1.2 Create new deployment: iot-frontend Now is time to deploy our web frontend. Deployment contain two containers in one POD. The frontend server uses existing Site-2-Site connection to on-prem to access data which will be presented in GUI. Monifest file is represented below: --- apiVersion : \"extensions/v1beta1\" kind : \"Deployment\" metadata : name : \"iot-frontend\" namespace : \"default\" labels : app : \"iot-frontend\" spec : replicas : 3 selector : matchLabels : app : \"iot-frontend\" template : metadata : labels : app : \"iot-frontend\" spec : containers : - name : \"frontend-server\" image : \"eu.gcr.io/fwardz001-poc-ci1s/frontend_server:vegas2022-v5\" env : - name : \"BACKEND_HOST\" valueFrom : configMapKeyRef : key : \"BACKEND_HOST\" name : \"iot-frontend-config\" - name : \"BACKEND_PORT\" valueFrom : configMapKeyRef : key : \"BACKEND_PORT\" name : \"iot-frontend-config\" - name : \"nginx-srvr\" image : \"eu.gcr.io/fwardz001-poc-ci1s/nginx_srvr:latest\" --- Create deployment of frontend using command below: kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Frontend/frontend_and_nginx_deployment.yaml Check deployment status and associated PODs: kubectl get pods,deployment | egrep \"NAME|iot-front\" 2. Expose the Application by Creating Kubernetes Service: Next step of our deployment is exposure of frontend web to the Internet. By using following manifest file, you will be able to create Service type LoadBalancer in AWS Kubernetes cluster. --- apiVersion : v1 kind : Service metadata : name : frontend-iot-service labels : app : iot-frontend spec : ports : - protocol : TCP port : 80 targetPort : 80 selector : app : iot-frontend type : \"LoadBalancer\" using the following command - kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Frontend/frontend-iot-service.yml you get service created and exposed in TCP port 80. 2.1 Check status of newly created service Once service was created, run following command kubectl get svc Copy service name from your output as marked by green frame on the screenshot above. Tip Place the DNS name of service copied to your web browser. It may not work imediately. Please wait few minutes if frontend is not visible yet. 3. Open the Application Dashboard: 3.1: Use \u201chttp\u201d to open the Dashboard for URL captured in step 2.1. You should see the following webpage in the new tab of your browser. Click on the \"Open Dashboard\" button to open the application dashboard as shown in the following screenshot 3.2: If you see the following web-page with charts filled with data, your application is working :-) Congratulations!!!","title":"Deploy the Frontend Application Components on AWS"},{"location":"deploy_frontend-aws/#deploy-the-frontend-application-components-on-aws","text":"In this section you would deploy the frontend components of the IoT Application on the Amazon Cloud. Following diagram shows the high-level architecture of these frontend application containers","title":"Deploy the Frontend Application Components on AWS"},{"location":"deploy_frontend-aws/#login-to-kubernetes-master-cli-shell","text":"SSH into Linux Jumphost using Putty \"ubuntu-terminal\" (198.18.133.11) - use your credentials. Switch to context 'arn:aws:eks:us-east-1:782256189490:cluster/CLUS-EKS-X'. kubectl config get-contexts kubectl config use-context <AWS context-name from previous command output> kubectl config get-contexts","title":"Login to Kubernetes Master CLI Shell:"},{"location":"deploy_frontend-aws/#1-deploy-frontend-iot","text":"Frontend is nginx web server connected to frontend-server which connect to REST-API Agent over already created Site-2-Site connection. In this section you will configure: Kubernetes ConfigMap Kubernetes frontend-iot Deployment Kubernetes frontend-iot Load-Balancer AWS Service You will create kubernetes deployment for frontend app and expose it to the internet using Kubernetes Load Balancer Service as shown in the following diagram","title":"1. Deploy frontend-iot:"},{"location":"deploy_frontend-aws/#11-create-configmap","text":"Create now configmap which will be used by your FrontEnd deployment. Replace ' BACKEND_HOST ' and ' BACKEND_PORT ' in the configmap command (below) with value noted down in section 2.3 in chapter Deploy REST API Agent on Kubernetes . Following screenshot highlights the Port and Node IPs in the command outputs in on-prem cluster. Warning External IP addresses of your on-prem workers can vary from one visible on figure below. Important Note down the Node External IP Address of one of on-prem WORKER and NodePort Service Port Number. ConfigMap create command - Copy it to notepad FIRST, edit and paste to Terminal: kubectl create configmap iot-frontend-config --from-literal=BACKEND_HOST=<NODE_IP> --from-literal=BACKEND_PORT=<PORT>","title":"1.1 Create ConfigMap"},{"location":"deploy_frontend-aws/#12-create-new-deployment-iot-frontend","text":"Now is time to deploy our web frontend. Deployment contain two containers in one POD. The frontend server uses existing Site-2-Site connection to on-prem to access data which will be presented in GUI. Monifest file is represented below: --- apiVersion : \"extensions/v1beta1\" kind : \"Deployment\" metadata : name : \"iot-frontend\" namespace : \"default\" labels : app : \"iot-frontend\" spec : replicas : 3 selector : matchLabels : app : \"iot-frontend\" template : metadata : labels : app : \"iot-frontend\" spec : containers : - name : \"frontend-server\" image : \"eu.gcr.io/fwardz001-poc-ci1s/frontend_server:vegas2022-v5\" env : - name : \"BACKEND_HOST\" valueFrom : configMapKeyRef : key : \"BACKEND_HOST\" name : \"iot-frontend-config\" - name : \"BACKEND_PORT\" valueFrom : configMapKeyRef : key : \"BACKEND_PORT\" name : \"iot-frontend-config\" - name : \"nginx-srvr\" image : \"eu.gcr.io/fwardz001-poc-ci1s/nginx_srvr:latest\" --- Create deployment of frontend using command below: kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Frontend/frontend_and_nginx_deployment.yaml Check deployment status and associated PODs: kubectl get pods,deployment | egrep \"NAME|iot-front\"","title":"1.2 Create new deployment: iot-frontend"},{"location":"deploy_frontend-aws/#2-expose-the-application-by-creating-kubernetes-service","text":"Next step of our deployment is exposure of frontend web to the Internet. By using following manifest file, you will be able to create Service type LoadBalancer in AWS Kubernetes cluster. --- apiVersion : v1 kind : Service metadata : name : frontend-iot-service labels : app : iot-frontend spec : ports : - protocol : TCP port : 80 targetPort : 80 selector : app : iot-frontend type : \"LoadBalancer\" using the following command - kubectl create -f https://raw.githubusercontent.com/marcinduma/WILCLD-2611/main/Kubernetes/Frontend/frontend-iot-service.yml you get service created and exposed in TCP port 80.","title":"2. Expose the Application by Creating Kubernetes Service:"},{"location":"deploy_frontend-aws/#21-check-status-of-newly-created-service","text":"Once service was created, run following command kubectl get svc Copy service name from your output as marked by green frame on the screenshot above. Tip Place the DNS name of service copied to your web browser. It may not work imediately. Please wait few minutes if frontend is not visible yet.","title":"2.1 Check status of newly created service"},{"location":"deploy_frontend-aws/#3-open-the-application-dashboard","text":"3.1: Use \u201chttp\u201d to open the Dashboard for URL captured in step 2.1. You should see the following webpage in the new tab of your browser. Click on the \"Open Dashboard\" button to open the application dashboard as shown in the following screenshot 3.2: If you see the following web-page with charts filled with data, your application is working :-) Congratulations!!!","title":"3. Open the Application Dashboard:"},{"location":"guacamole/","text":"Copy and Paste in and out Guacamole Apache Guacamole serves as a proxy to provide clientless access to other servers. The copy paste function is achieved through browser. Guacamole Menu The guacamole menu is a sidebar which is hidden. On a Windows device with an external keyboard, the guacamole menu is displayed by pressing Ctrl+Alt+Shift On a Mac device with an external keyboard, the guacamole menu is displayed by pressing Ctrl+Command+Shift 3. On a device that doesn\u2019t have a keyboard, e.g. mobile or touchscreen device, the guacamole menu is displayed by swiping right from the left edge of the screen. To Hide the guacamole menu, press Ctrl+Alt+Shift/ Ctrl+Command+Shift or swipe left across the screen again. Guacamole Use The Clipboard text area functions as an interface between the remote clipboard and local clipboard. Text from the local clipboard can be pasted into the text area, and the text is sent to the remote clipboard. 2. After the text is copied to the remote clipboard, the Guacamole menu can be closed. The text from the clipboard can be pasted in the desired location. 3. Similarly, copying text from the remote desktop sends the text to the Clipboard text area on the Guacamole Menu, which can then be copied to the local clipboard.","title":"Copy and Paste in and out Guacamole"},{"location":"guacamole/#copy-and-paste-in-and-out-guacamole","text":"Apache Guacamole serves as a proxy to provide clientless access to other servers. The copy paste function is achieved through browser.","title":"Copy and Paste in and out Guacamole"},{"location":"guacamole/#guacamole-menu","text":"The guacamole menu is a sidebar which is hidden. On a Windows device with an external keyboard, the guacamole menu is displayed by pressing Ctrl+Alt+Shift On a Mac device with an external keyboard, the guacamole menu is displayed by pressing Ctrl+Command+Shift 3. On a device that doesn\u2019t have a keyboard, e.g. mobile or touchscreen device, the guacamole menu is displayed by swiping right from the left edge of the screen. To Hide the guacamole menu, press Ctrl+Alt+Shift/ Ctrl+Command+Shift or swipe left across the screen again.","title":"Guacamole Menu"},{"location":"guacamole/#guacamole-use","text":"The Clipboard text area functions as an interface between the remote clipboard and local clipboard. Text from the local clipboard can be pasted into the text area, and the text is sent to the remote clipboard. 2. After the text is copied to the remote clipboard, the Guacamole menu can be closed. The text from the clipboard can be pasted in the desired location. 3. Similarly, copying text from the remote desktop sends the text to the Clipboard text area on the Guacamole Menu, which can then be copied to the local clipboard.","title":"Guacamole Use"},{"location":"infra-config-check/","text":"","title":"Infra config check"},{"location":"infra-config-msite/","text":"Infrastructure configuration - Multi-Site configuration In this lab section we will use Nexus Dashboard orchestrator to connect 2 ACI Fabrics together. Nexus Dashboard Orchestrator(NDO) 1. NDO Introduction On the Site list hit \"Continue\" and then \"Done\" button to finish Initial Setup from previous chapter. Click \"Go To Dashboard\" in bottom right corner - you should be moved to ND Dashboard with Site Map In the navigation menu on the left go to \"Admin Console\" You will be moved to Admin Console page, where you can see Health of your Nexus Dashboard and status of sites and services. Navigate to \"Sites\" tab and verify if \"Connectivity Status\" of you sites is \"Up\" Navigate to \"Sevices\" tab to see what applications are installed on this Nexus Dashboard Nexus Dashboard Orchestrator is already installed and enabled under \"Service Catalog\" inside \"Installed Service\" Hit \"Open\" button in order to login You will be now redirected to Nexus Dashboard Orchestrator Dashboard. 2. NDO Sites Onboarding In order to configure AWS and Azure sites from NDO, added previously on Nexus Dashboard , sites have to be Maneged and have Site ID assigned. Navigate to Sites : Click on \"Unmanaged\" box under State Column for each site and assign Site ID , confirming with \"Add\" button. For AWS Site - set ID **10** For Azure Site - set ID **20** After this operation - both sites should be visable as Managed 3. Site Connectivity Configuration In Next Step we would configure Infrastructure to connect 2 Cloud ACI Sites togehter. Navigate to Infrastructure -> Site Connectivity -> Configure button On the General Settings Page, scroll down to OSPF Configuration and fill in OSPF Area ID to value 0.0.0.0 , leave other setting as default. Go to tab \"IPSec Tunnel Subnet Pools\" , click Add IP address button and add subnet Subnet: 192.168.255.0/24 Confirm with checkbox button. In the left navigation bar, under the Sites bar, click on first site CNC-AWS-01 Enable the site for MultiSite by checking the checkbox \"ACI Multisite\" also enable \"Contract Based Routing\" Settings: - ACI Multisite - checked - Contract Based Routing - checked Click the \"Add Site\" button to cross-connect 2 sites. Under Connected to Site select Select a Site hyperlink. Select Azure fabric and hit Select For the Connection Type select Public Internet and hit Ok . Leave rest of the setting as default, you can review them. Move to CNC-Azure-01 Site and also enable the site for MultiSite by checking the checkbox \"ACI Multisite\" - simiar as in previous point. Settings: - ACI Multisite - checked - Contract Based Routing - checked Note that second site is already selected for Inter-Site Connectivity Once done, locate the Deploy button on top the screen, click it and Select \"Deploy Only\" , hit Yes for confirmation. As you hit Deploy button, NDO will now configure CNC and Cloud Routes. - IPSec tunnels, full mesh between all 4 routers (2 per Cloud) - OSPF routing over IPSec - BGP EVPN peering for prefixes exchange Inter-site connectivity veryfication It may take 5-10 minutes for configuration to be pushed and Tunnels to be established. At this point we configured this part of our topology diagram: 1. Nexush Dashboard view Nexus Dashboard allows for monitoring of Inter-Site connectivity. On the Left navigation page click \"Dashboard\" to go back to main Connectivity View. Take a look into green line between AWS and Azure site(you can use use magnifying tool for better view). Green line indicated that all is fine with connectivity. On the Left navigation page click \"Infrastructure\" -> \"Site Connectivity\" and scroll down on a page. Under the site list, locate \"Show Connectivity Status\" and click on it. Check the connectivity status for both BGP EVPN as well as Tunnel Status. There should be 4 UP BGP sessions, as well as 4 Tunnels which are UP between Sites. Inter-site connectivity veryfication (Cloud Routers) 1. IPSec tunnel veryfication Open Putty client from desktop and put IP address of Cloud router. Note Cloud Routers IP address are avaibale in POD details IP address schema. Login with username \"csradmin\" and password \"CiscoLive2023!\" Execute command \"show ip int brief | include Tunnel\" show ip int brief | include Tunnel Expected output: ct_routerp_eu-central-1_0_0#show ip int brief | include Tunnel Tunnel0 10.10.0.52 YES unset up up Tunnel1 169.254.112.1 YES NVRAM up up Tunnel6 192.168.255.4 YES NVRAM up up Tunnel7 192.168.255.2 YES NVRAM up up ct_routerp_eu-central-1_0_0# Verify that all 4 tunnels are up/up. Note also that Tunnel6 and Tunnel7 are addressed from subnet which was specified in the beginning of Multiste Configuration. Those 2 tunnel are configured towards two (2) Catalyst 8000V Routers in another site! 2. OSPF adjacency veryfication Execute command \"show ip ospf neighbor\" show ip ospf neighbor Expected output: ct_routerp_eu-central-1_0_0#show ip ospf neighbor Neighbor ID Pri State Dead Time Address Interface 10.20.0.20 0 FULL/ - 00:00:33 192.168.255.3 Tunnel7 10.20.0.68 0 FULL/ - 00:00:38 192.168.255.5 Tunnel6 ct_routerp_eu-central-1_0_0# Verify that both sessions are in FULL State. 2. BGP EVPN veryfication Execute command \"show bgp l2vpn evpn summary\" show bgp l2vpn evpn summary Expected output: ct_routerp_eu-central-1_0_0#show bgp l2vpn evpn summary BGP router identifier 10.10.0.20, local AS number 65110 BGP table version is 1, main routing table version 1 Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd 10.20.0.52 4 65200 96 94 1 0 0 01:22:47 0 10.20.0.116 4 65200 95 94 1 0 0 01:22:52 0 ct_routerp_eu-central-1_0_0# Verify that both sessions are Up (O in \"State/PfxRcd\" column)","title":"Multisite configuration"},{"location":"infra-config-msite/#infrastructure-configuration-multi-site-configuration","text":"In this lab section we will use Nexus Dashboard orchestrator to connect 2 ACI Fabrics together.","title":"Infrastructure configuration - Multi-Site configuration"},{"location":"infra-config-msite/#nexus-dashboard-orchestratorndo","text":"","title":"Nexus Dashboard Orchestrator(NDO)"},{"location":"infra-config-msite/#1-ndo-introduction","text":"On the Site list hit \"Continue\" and then \"Done\" button to finish Initial Setup from previous chapter. Click \"Go To Dashboard\" in bottom right corner - you should be moved to ND Dashboard with Site Map In the navigation menu on the left go to \"Admin Console\" You will be moved to Admin Console page, where you can see Health of your Nexus Dashboard and status of sites and services. Navigate to \"Sites\" tab and verify if \"Connectivity Status\" of you sites is \"Up\" Navigate to \"Sevices\" tab to see what applications are installed on this Nexus Dashboard Nexus Dashboard Orchestrator is already installed and enabled under \"Service Catalog\" inside \"Installed Service\" Hit \"Open\" button in order to login You will be now redirected to Nexus Dashboard Orchestrator Dashboard.","title":"1. NDO Introduction"},{"location":"infra-config-msite/#2-ndo-sites-onboarding","text":"In order to configure AWS and Azure sites from NDO, added previously on Nexus Dashboard , sites have to be Maneged and have Site ID assigned. Navigate to Sites : Click on \"Unmanaged\" box under State Column for each site and assign Site ID , confirming with \"Add\" button. For AWS Site - set ID **10** For Azure Site - set ID **20** After this operation - both sites should be visable as Managed","title":"2. NDO Sites Onboarding"},{"location":"infra-config-msite/#3-site-connectivity-configuration","text":"In Next Step we would configure Infrastructure to connect 2 Cloud ACI Sites togehter. Navigate to Infrastructure -> Site Connectivity -> Configure button On the General Settings Page, scroll down to OSPF Configuration and fill in OSPF Area ID to value 0.0.0.0 , leave other setting as default. Go to tab \"IPSec Tunnel Subnet Pools\" , click Add IP address button and add subnet Subnet: 192.168.255.0/24 Confirm with checkbox button. In the left navigation bar, under the Sites bar, click on first site CNC-AWS-01 Enable the site for MultiSite by checking the checkbox \"ACI Multisite\" also enable \"Contract Based Routing\" Settings: - ACI Multisite - checked - Contract Based Routing - checked Click the \"Add Site\" button to cross-connect 2 sites. Under Connected to Site select Select a Site hyperlink. Select Azure fabric and hit Select For the Connection Type select Public Internet and hit Ok . Leave rest of the setting as default, you can review them. Move to CNC-Azure-01 Site and also enable the site for MultiSite by checking the checkbox \"ACI Multisite\" - simiar as in previous point. Settings: - ACI Multisite - checked - Contract Based Routing - checked Note that second site is already selected for Inter-Site Connectivity Once done, locate the Deploy button on top the screen, click it and Select \"Deploy Only\" , hit Yes for confirmation. As you hit Deploy button, NDO will now configure CNC and Cloud Routes. - IPSec tunnels, full mesh between all 4 routers (2 per Cloud) - OSPF routing over IPSec - BGP EVPN peering for prefixes exchange","title":"3. Site Connectivity Configuration"},{"location":"infra-config-msite/#inter-site-connectivity-veryfication","text":"It may take 5-10 minutes for configuration to be pushed and Tunnels to be established. At this point we configured this part of our topology diagram:","title":"Inter-site connectivity veryfication"},{"location":"infra-config-msite/#1-nexush-dashboard-view","text":"Nexus Dashboard allows for monitoring of Inter-Site connectivity. On the Left navigation page click \"Dashboard\" to go back to main Connectivity View. Take a look into green line between AWS and Azure site(you can use use magnifying tool for better view). Green line indicated that all is fine with connectivity. On the Left navigation page click \"Infrastructure\" -> \"Site Connectivity\" and scroll down on a page. Under the site list, locate \"Show Connectivity Status\" and click on it. Check the connectivity status for both BGP EVPN as well as Tunnel Status. There should be 4 UP BGP sessions, as well as 4 Tunnels which are UP between Sites.","title":"1. Nexush Dashboard view"},{"location":"infra-config-msite/#inter-site-connectivity-veryfication-cloud-routers","text":"","title":"Inter-site connectivity veryfication  (Cloud Routers)"},{"location":"infra-config-msite/#1-ipsec-tunnel-veryfication","text":"Open Putty client from desktop and put IP address of Cloud router. Note Cloud Routers IP address are avaibale in POD details IP address schema. Login with username \"csradmin\" and password \"CiscoLive2023!\" Execute command \"show ip int brief | include Tunnel\" show ip int brief | include Tunnel Expected output: ct_routerp_eu-central-1_0_0#show ip int brief | include Tunnel Tunnel0 10.10.0.52 YES unset up up Tunnel1 169.254.112.1 YES NVRAM up up Tunnel6 192.168.255.4 YES NVRAM up up Tunnel7 192.168.255.2 YES NVRAM up up ct_routerp_eu-central-1_0_0# Verify that all 4 tunnels are up/up. Note also that Tunnel6 and Tunnel7 are addressed from subnet which was specified in the beginning of Multiste Configuration. Those 2 tunnel are configured towards two (2) Catalyst 8000V Routers in another site!","title":"1. IPSec tunnel veryfication"},{"location":"infra-config-msite/#2-ospf-adjacency-veryfication","text":"Execute command \"show ip ospf neighbor\" show ip ospf neighbor Expected output: ct_routerp_eu-central-1_0_0#show ip ospf neighbor Neighbor ID Pri State Dead Time Address Interface 10.20.0.20 0 FULL/ - 00:00:33 192.168.255.3 Tunnel7 10.20.0.68 0 FULL/ - 00:00:38 192.168.255.5 Tunnel6 ct_routerp_eu-central-1_0_0# Verify that both sessions are in FULL State.","title":"2. OSPF adjacency veryfication"},{"location":"infra-config-msite/#2-bgp-evpn-veryfication","text":"Execute command \"show bgp l2vpn evpn summary\" show bgp l2vpn evpn summary Expected output: ct_routerp_eu-central-1_0_0#show bgp l2vpn evpn summary BGP router identifier 10.10.0.20, local AS number 65110 BGP table version is 1, main routing table version 1 Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd 10.20.0.52 4 65200 96 94 1 0 0 01:22:47 0 10.20.0.116 4 65200 95 94 1 0 0 01:22:52 0 ct_routerp_eu-central-1_0_0# Verify that both sessions are Up (O in \"State/PfxRcd\" column)","title":"2. BGP EVPN veryfication"},{"location":"infra-config/","text":"Infrastructure configuration - Site onboarding Find Public IP Address of Nexus Dashboard and Cisco Cloud Network Controller in AWS 1. Open AWS console via browser 2. Select IAM user, provide Account ID and hit \"Next\" 3. Provide Username and password and hit \"Sign In\" 4. In AWS Search Bar type \"EC2\" and Select \"EC2\" Services Tab 5. From \"Resources\" select \"Instances (Running)\" 6. On the Instances List scroll to the right and note down \"Public IPv4 address\" Find Public IP Address of Cisco Cloud Network Controller in Azure 1. Open Azure portal via browser 2. Enter your username along with domain and hit \"Next\" 3. Provide password and hit \"Sign In\" 4. Use \"Skip for now\" option for Account Protection 5. In Search bar look for \"virtual machines\" and select from services 6. On the Virtual Machines list scroll to the right and note down \"Public IP address\" Nexus Dashboard site onboarding 1. Login to Nexus Dasboard using IP collected above and login with provided credentials 2. Hit \"Get started\" and Setup screen will pop-up 3. On the \"Add Sites\" section hit \"Begin\" 4. Hit \"Add Site\" button 5. Add AWS Cloud Network Controller Site Type: \"Cloud Network Controller\" Name: \"CNC-AWS-01\" Hostname/IP Address: \"Public IP of Cloud Network Controller for AWS collected above\" Username: admin Password: CiscoLive2023! Login Domain: empty 6. Hit \"Save\" \"AWS site should be added now, stay on the same page! \" 7. Hit \"Add site button one more time to add Azure site 8. Add Azure site details Site Type: \"Cloud Network Controller\" Name: \"CNC-Azure-01\" Hostname/IP Address: \"Public IP of Cloud Network Controller from Azure collected above\" Username: admin Password: CiscoLive2023! Login Domain: empty 6. Hit \"Save\" Check the site list You should see both sites added under the site list.","title":"Sites onboarding"},{"location":"infra-config/#infrastructure-configuration-site-onboarding","text":"","title":"Infrastructure configuration - Site onboarding"},{"location":"infra-config/#find-public-ip-address-of-nexus-dashboard-and-cisco-cloud-network-controller-in-aws","text":"","title":"Find Public IP Address of Nexus Dashboard and Cisco Cloud Network Controller in AWS"},{"location":"infra-config/#1-open-aws-console-via-browser","text":"","title":"1. Open AWS console via browser"},{"location":"infra-config/#2-select-iam-user-provide-account-id-and-hit-next","text":"","title":"2. Select IAM user,  provide Account ID and hit \"Next\""},{"location":"infra-config/#3-provide-username-and-password-and-hit-sign-in","text":"","title":"3. Provide Username and password and hit \"Sign In\""},{"location":"infra-config/#4-in-aws-search-bar-type-ec2-and-select-ec2-services-tab","text":"","title":"4. In AWS Search Bar type \"EC2\" and Select \"EC2\" Services Tab"},{"location":"infra-config/#5-from-resources-select-instances-running","text":"","title":"5. From \"Resources\" select \"Instances (Running)\""},{"location":"infra-config/#6-on-the-instances-list-scroll-to-the-right-and-note-down-public-ipv4-address","text":"","title":"6. On the Instances List scroll to the right and note down \"Public IPv4 address\""},{"location":"infra-config/#find-public-ip-address-of-cisco-cloud-network-controller-in-azure","text":"","title":"Find Public IP Address of Cisco Cloud Network Controller in Azure"},{"location":"infra-config/#1-open-azure-portal-via-browser","text":"","title":"1. Open Azure portal via browser"},{"location":"infra-config/#2-enter-your-username-along-with-domain-and-hit-next","text":"","title":"2. Enter your username along with domain and hit \"Next\""},{"location":"infra-config/#3-provide-password-and-hit-sign-in","text":"","title":"3. Provide password and hit \"Sign In\""},{"location":"infra-config/#4-use-skip-for-now-option-for-account-protection","text":"","title":"4. Use \"Skip for now\" option for Account Protection"},{"location":"infra-config/#5-in-search-bar-look-for-virtual-machines-and-select-from-services","text":"","title":"5. In Search bar look for \"virtual machines\" and select from services"},{"location":"infra-config/#6-on-the-virtual-machines-list-scroll-to-the-right-and-note-down-public-ip-address","text":"","title":"6. On the Virtual Machines list scroll to the right and note down \"Public IP address\""},{"location":"infra-config/#nexus-dashboard-site-onboarding","text":"","title":"Nexus Dashboard site onboarding"},{"location":"infra-config/#1-login-to-nexus-dasboard-using-ip-collected-above-and-login-with-provided-credentials","text":"","title":"1. Login to Nexus Dasboard using IP collected above and login with provided credentials"},{"location":"infra-config/#2-hit-get-started-and-setup-screen-will-pop-up","text":"","title":"2. Hit \"Get started\" and Setup screen will pop-up"},{"location":"infra-config/#3-on-the-add-sites-section-hit-begin","text":"","title":"3. On the \"Add Sites\" section hit \"Begin\""},{"location":"infra-config/#4-hit-add-site-button","text":"","title":"4. Hit \"Add Site\" button"},{"location":"infra-config/#5-add-aws-cloud-network-controller","text":"Site Type: \"Cloud Network Controller\" Name: \"CNC-AWS-01\" Hostname/IP Address: \"Public IP of Cloud Network Controller for AWS collected above\" Username: admin Password: CiscoLive2023! Login Domain: empty","title":"5. Add AWS Cloud Network Controller"},{"location":"infra-config/#6-hit-save","text":"\"AWS site should be added now, stay on the same page! \"","title":"6. Hit \"Save\""},{"location":"infra-config/#7-hit-add-site-button-one-more-time-to-add-azure-site","text":"","title":"7. Hit \"Add site button one more time to add Azure site"},{"location":"infra-config/#8-add-azure-site-details","text":"Site Type: \"Cloud Network Controller\" Name: \"CNC-Azure-01\" Hostname/IP Address: \"Public IP of Cloud Network Controller from Azure collected above\" Username: admin Password: CiscoLive2023! Login Domain: empty","title":"8. Add Azure site details"},{"location":"infra-config/#6-hit-save_1","text":"","title":"6. Hit \"Save\""},{"location":"infra-config/#check-the-site-list","text":"You should see both sites added under the site list.","title":"Check the site list"},{"location":"intersight/","text":"Explore Cisco Intersight Dashboard 1. Accessing Cisco Intersight Platform Cisco Intersight Platform manages Kuberenetes clusters in the private infrasturcture. You will have access to dedicated instance of Cisco Intersight, from which you will manage your own Kuberenetes Clusters used later on to deploy application. Please find login credentials and URL to your IKS instance below: URL: https://intersight.com/ User name: holcld2611+pod'X'@gmail.com Where X will be provided by proctor User password: CiscoLive!2022 Warning You can explore Cisco Intersight through the GUI, but please do not delete content already created. Access Intersight using following steps: Open WebBrowser (Chrome) and connect to https://intersight.com Select Sign In with Cisco ID option on the first screen - as shown on figure below. Enter e-mail provided by proctor to your POD: Click next and fill password. Tip Credentials for the session can be found in WIL Assistant portal. In case of issues, ask proctor for help. 2. Intersigh Dashboard Target While you are logged to Cisco Intersight, first page you see is Monitor section. Navigate to Admin section in the bottom of Navigation Pane on Left. Click on Target Target section shows already Claimed services - in our case it is Cisco Intersight Assist installed \"on-prem\" together with vCenter. You can see status of claim, date and who executed it. Click on our Intersight Assist - iva.dcloud.cisco.com to see details of the Target. 3. Intersight Dashboard Operate Most information from claimed targets are visible under Operate section. The section is divided to two sub categories: Virtualization and Kuberenetes. Virtualization When exloring first of them, you will see information about all VMs managed by vCenter claimed in Target section. By using three dots button on right column, you are able to manage each of VM: Option when open the Menu : On the other Tabs, you can verify details of Hosts, Storage, etc. Please explore yourself rest of the tab when interested. Kuberenetes Second section contain informations about deployed Intersight Kuberenetes Clusters (IKS). You will see one IKS cluster, which is deployed on-prem. On main screen you see general information about its status - numbers of control plane nodes, workers, associated profile. Using three dots button on most right column you open menu where you are able to download kubeconfig, undeploy Cluster or Open TAC case. Explore deployed IKS cluster Please, click at name of deployed IKS cluster. Now you are inside the selected cluster, where you find more details about deployment. Dashboard shows information about cluster condition, version of the K8s installed, CNI and more. By selecting Operate Tab, you find more information about profile and policy usage. Please explore rest of the Sections to see information under them. 4. Intersight Dashboard Configure Another section in Navigation pane is \"Configure\". Under \"Configure > Profiles\" you find definition of IKS profile used for CLUS-IKS-1 K8s deployment you just explored. IKS Profile wizzard When you open existing profile, you will be able to see progress wizzard. Tip In new deployment you need to create all Pools and Polices for IKS beforehand. In the wizzard below, you will see already selected polices. It is explained in next section. Warning PLEASE, DO NOT EDIT ANY OF THE POLICES UNDER THE PROFILE - IT MAY CAUSE DESTROY OF YOUR IKS CLUSTER. Explore it only and CLOSE in the END - DON'T DEPLOY CHANGES. First page of the wizzard is asking Name of the Cluster Second page contains information about cluster in general. You find there IP Pool, LoadBalancer IP counts, DNS/NTP policy, etc. Next one, its K8s Control Plane node definition. It is possible to configure desired state of CP nodes, minumum/maximum, K8s version, IP pools. It is first place where user must specify VM infra policy which contain vCenter information (basically definition where to deploy CP nodes). VM Machine policy contain information about VM size (RAM, CPU, disk size). Fourth page its definition of worker nodes. Similar to control-plane node definition, its possible to configure size of cluster, VM polices, IP pools, K8s version. Next section contain ADD-Ons definition. Cisco Intersight give you an option to install K8s add-ons automatically during IKS cluster deployments. One of the ADD-on installed is Kuberenetes Dashboard. Final page is summary of the entire profile, where is \"DEPLOY\" button. DO NOT DEPLOY PROFILE, BUT CLOSE IT. IKS Configure Polices Important section when working with Cisco Intersight is Polices . Here user needs to configure parameters of deployments before start doing profiles. In the LAB we use seven polices as shown on the figure below. Table gives general information about Names and Policy Types, time of creation/update. By clicking on the policy name, its possible to check details and edit it.","title":"Explore Cisco Intersight Dashboard"},{"location":"intersight/#explore-cisco-intersight-dashboard","text":"","title":"Explore Cisco Intersight Dashboard"},{"location":"intersight/#1-accessing-cisco-intersight-platform","text":"Cisco Intersight Platform manages Kuberenetes clusters in the private infrasturcture. You will have access to dedicated instance of Cisco Intersight, from which you will manage your own Kuberenetes Clusters used later on to deploy application. Please find login credentials and URL to your IKS instance below: URL: https://intersight.com/ User name: holcld2611+pod'X'@gmail.com Where X will be provided by proctor User password: CiscoLive!2022 Warning You can explore Cisco Intersight through the GUI, but please do not delete content already created.","title":"1. Accessing Cisco Intersight Platform"},{"location":"intersight/#access-intersight-using-following-steps","text":"Open WebBrowser (Chrome) and connect to https://intersight.com Select Sign In with Cisco ID option on the first screen - as shown on figure below. Enter e-mail provided by proctor to your POD: Click next and fill password. Tip Credentials for the session can be found in WIL Assistant portal. In case of issues, ask proctor for help.","title":"Access Intersight using following steps:"},{"location":"intersight/#2-intersigh-dashboard-target","text":"While you are logged to Cisco Intersight, first page you see is Monitor section. Navigate to Admin section in the bottom of Navigation Pane on Left. Click on Target Target section shows already Claimed services - in our case it is Cisco Intersight Assist installed \"on-prem\" together with vCenter. You can see status of claim, date and who executed it. Click on our Intersight Assist - iva.dcloud.cisco.com to see details of the Target.","title":"2. Intersigh Dashboard Target"},{"location":"intersight/#3-intersight-dashboard-operate","text":"Most information from claimed targets are visible under Operate section. The section is divided to two sub categories: Virtualization and Kuberenetes.","title":"3. Intersight Dashboard Operate"},{"location":"intersight/#virtualization","text":"When exloring first of them, you will see information about all VMs managed by vCenter claimed in Target section. By using three dots button on right column, you are able to manage each of VM: Option when open the Menu : On the other Tabs, you can verify details of Hosts, Storage, etc. Please explore yourself rest of the tab when interested.","title":"Virtualization"},{"location":"intersight/#kuberenetes","text":"Second section contain informations about deployed Intersight Kuberenetes Clusters (IKS). You will see one IKS cluster, which is deployed on-prem. On main screen you see general information about its status - numbers of control plane nodes, workers, associated profile. Using three dots button on most right column you open menu where you are able to download kubeconfig, undeploy Cluster or Open TAC case.","title":"Kuberenetes"},{"location":"intersight/#explore-deployed-iks-cluster","text":"Please, click at name of deployed IKS cluster. Now you are inside the selected cluster, where you find more details about deployment. Dashboard shows information about cluster condition, version of the K8s installed, CNI and more. By selecting Operate Tab, you find more information about profile and policy usage. Please explore rest of the Sections to see information under them.","title":"Explore deployed IKS cluster"},{"location":"intersight/#4-intersight-dashboard-configure","text":"Another section in Navigation pane is \"Configure\". Under \"Configure > Profiles\" you find definition of IKS profile used for CLUS-IKS-1 K8s deployment you just explored.","title":"4. Intersight Dashboard Configure"},{"location":"intersight/#iks-profile-wizzard","text":"When you open existing profile, you will be able to see progress wizzard. Tip In new deployment you need to create all Pools and Polices for IKS beforehand. In the wizzard below, you will see already selected polices. It is explained in next section. Warning PLEASE, DO NOT EDIT ANY OF THE POLICES UNDER THE PROFILE - IT MAY CAUSE DESTROY OF YOUR IKS CLUSTER. Explore it only and CLOSE in the END - DON'T DEPLOY CHANGES. First page of the wizzard is asking Name of the Cluster Second page contains information about cluster in general. You find there IP Pool, LoadBalancer IP counts, DNS/NTP policy, etc. Next one, its K8s Control Plane node definition. It is possible to configure desired state of CP nodes, minumum/maximum, K8s version, IP pools. It is first place where user must specify VM infra policy which contain vCenter information (basically definition where to deploy CP nodes). VM Machine policy contain information about VM size (RAM, CPU, disk size). Fourth page its definition of worker nodes. Similar to control-plane node definition, its possible to configure size of cluster, VM polices, IP pools, K8s version. Next section contain ADD-Ons definition. Cisco Intersight give you an option to install K8s add-ons automatically during IKS cluster deployments. One of the ADD-on installed is Kuberenetes Dashboard. Final page is summary of the entire profile, where is \"DEPLOY\" button. DO NOT DEPLOY PROFILE, BUT CLOSE IT.","title":"IKS Profile wizzard"},{"location":"intersight/#iks-configure-polices","text":"Important section when working with Cisco Intersight is Polices . Here user needs to configure parameters of deployments before start doing profiles. In the LAB we use seven polices as shown on the figure below. Table gives general information about Names and Policy Types, time of creation/update. By clicking on the policy name, its possible to check details and edit it.","title":"IKS Configure Polices"},{"location":"kubernetes_basics/","text":"Appendix - 3: Kubernetes Basic Docs 1. Install and Configure kubectl For information on \"kubectl\" installation and configuration use the following document - https://kubernetes.io/docs/tasks/tools/install-kubectl/ 2. Google Cloud Container Registry: Pushing and Pulling Container Images To push or pull the container images to/from Google Container Registry you can follow the following article - https://cloud.google.com/container-registry/docs/pushing-and-pulling Note: You need to login to your Google Cloud account before pushing the container image. You can do it using the following command - gcloud auth login If you have multiple projects, you can select a specific one using the following command - gcloud config set project <Project_ID> Kubernetes Cheat Sheet: https://kubernetes.io/docs/reference/kubectl/cheatsheet/","title":"Appendix - 3: Kubernetes Basic Docs"},{"location":"kubernetes_basics/#appendix-3-kubernetes-basic-docs","text":"","title":"Appendix - 3: Kubernetes Basic Docs"},{"location":"kubernetes_basics/#1-install-and-configure-kubectl","text":"For information on \"kubectl\" installation and configuration use the following document - https://kubernetes.io/docs/tasks/tools/install-kubectl/","title":"1. Install and Configure kubectl"},{"location":"kubernetes_basics/#2-google-cloud-container-registry-pushing-and-pulling-container-images","text":"To push or pull the container images to/from Google Container Registry you can follow the following article - https://cloud.google.com/container-registry/docs/pushing-and-pulling Note: You need to login to your Google Cloud account before pushing the container image. You can do it using the following command - gcloud auth login If you have multiple projects, you can select a specific one using the following command - gcloud config set project <Project_ID>","title":"2. Google Cloud Container Registry: Pushing and Pulling Container Images"},{"location":"kubernetes_basics/#kubernetes-cheat-sheet","text":"https://kubernetes.io/docs/reference/kubectl/cheatsheet/","title":"Kubernetes Cheat Sheet:"},{"location":"lab-check/","text":"","title":"Infastructure veryfication"},{"location":"monitor-aci/","text":"How to pull informations using REST API The REST API is the interface into the management information tree (MIT) and allows manipulation of the object model state. The same REST interface is used by the APIC CLI, GUI, and SDK, so that whenever information is displayed, it is read through the REST API, and when configuration changes are made, they are written through the REST API. The REST API also provides an interface through which other information can be retrieved, including statistics, faults, and audit events. It even provides a means of subscribing to push-based event notification, so that when a change occurs in the MIT, an event can be sent through a web socket. Standard REST methods are supported on the API, which includes POST, GET, and DELETE operations through HTTP. The GET method is nullipotent, meaning that it can be called zero or more times without making any changes (or that it is a read-only operation). 1. Creating the API Command You can invoke an API command or query by sending an HTTP or HTTPS message to the APIC with a URI of this form for an operation on a managed object (MO): {http | https}://host [:port] /api/mo/dn. {json | xml} [?options] Use this form for an operation on an object class: {http | https}://host [:port] /api/class/className. {json | xml} [?options] Note While the preceding examples use /api/mo and /api/class in the URI string, the APIC UI and Visore also use the /api/node/mo and /api/node/class syntax in the URI string. Both formats are valid and are used interchangeably in this document. This example shows a URI for an API operation that involves an MO of class fv:Tenant: https://apic-ip-address/api/mo/uni/tn-ExampleCorp.xml 1.1 The URI for an API Operation on an MO https://apic-ip-address/api/mo/uni/tn-ExampleCorp.xml 1.2 The URI for an API Operation on an Node MO In an API operation to access an MO on a specific node device in the fabric, the resource path consists of /mo/topology/pod-number/node-number/sys/ followed by the node component. For example, to access a board sensor in chassis slot b of node-1 in pod-1, use this URI: GET https://apic-ip-address/api/mo/topology/pod-1/node-1/sys/ch/bslot/board/sensor-3.json 1.3. The URI for an API Operation on a Class In an API operation to get information about a class, the resource path consists of /class/ followed by the name of the class as described in the Cisco APIC Management Information Model Reference. In the URI, the colon in the class name is removed. For example, this URI specifies a query on the class aaa:User: GET https://apic-ip-address/api/class/aaaUser.json 2. Pulling data from created Tenant In the restAPI section you created tenant called \"Postman-collection-1\" . That tenant have 22 EPGs and BDs and now you would like to pull data about one of them. POTENTIAL EXCERCISE: Server Team asked you to add new static binding for server addressed in subnet 172.16.59.0/24. You don't know what is name of EPG and now you have to do reverse engineering to find correct EPG. You know that GW IP address of BD is 172.16.59.1/24. Your task is to find BD name using GET API query and by having BD, find name of EPG associated to it. Where to start? Think about how to find class-name of Bridge Domain. During the day you used few methods - \"Save As\" object, \"Show Debug Info\" in APIC, \"API Inspector\", \"Open in Object Store Browser\". Choice method you prefer and find class-name for BD. Once you have it, compose URL, Answer below: https://{{apic}}/api/class/fvBD.json Create Postman Request to GET information from class-name you specify above. Result should shows you \"totalCount\": \"28\" meaning you found 28 bridge domains and you listed their attributes. Still you are not able to find out the correct one. Now list all sub-tree of object fvBD. Do it with adding KEY parameter to your GET query: https://{{apic}}/api/class/fvBD.json?query-target=subtree You listed all Bridge Domains with all MIT for them, now results with more than 200 items in APIC response. Having information about IP address of GW, you can narrow it to one, having that specific IP. Let's try: https://{{apic}}/api/class/fvBD.json?query-target=subtree&query-target-filter=and(wcard(fvSubnet.ip,\"172.16.59.1/24\")) Function which contain IP addresses under BD is fvSubnet. The attribute of the function is ip . By using filter query-target-filter=and(wcard(fvSubnet.ip,\"172.16.59.1/24\")) you get information about DN of function. The DN contains Bridge-Domain name. Results on the figure below: Please list informations about Bridge Domain you found: BD-59 . https://{{apic}}/api/class/fvBD.json?query-target=subtree&query-target-filter=and(eq(fvBD.name,\"BD-59\")) Still you don't have EPG name. Why you don't see sub-tree of the fvBD in your GET response?? You need to specify RSP-SUBTREE in your GET. https://{{apic}}/api/class/fvBD.json?query-target=subtree&rsp-subtree=full&query-target-filter=and(eq(fvBD.name,\"BD-59\")) In the output you should look for fvRtBd class-name. It contains information about EPG to BD relation. Now try yourself to narrow down the output. https://{{apic}}/api/class/fvRtBd.json?query-target=subtree&query-target-filter=and(wcard(fvRtBd.dn,\"BD-59\")) Using URL above you are able to filter response from APIC for class-name fvRtBd and DN path which contain BD-59 in. fvRtBd.dn { \"fvRtBd\" : { \"attributes\" : { \"childAction\" : \"\" , \"dn\" : \"uni/tn-Postman-collection-1/BD-BD-59/rtbd-[uni/tn-Postman-collection-1/ap-APPLICATION-PROFILE-1/epg-EPG-59]\" , \"lcOwn\" : \"local\" , \"modTs\" : \"2022-11-04T18:22:39.217+00:00\" , \"status\" : \"\" , \"tCl\" : \"fvAEPg\" , \"tDn\" : \"uni/tn-Postman-collection-1/ap-APPLICATION-PROFILE-1/epg-EPG-59\" } } } CONGRATULATIONS , you found correct EPG: EPG-59 Please see the Collection of REST API URLs for further details.","title":"How to pull informations using REST API"},{"location":"monitor-aci/#how-to-pull-informations-using-rest-api","text":"The REST API is the interface into the management information tree (MIT) and allows manipulation of the object model state. The same REST interface is used by the APIC CLI, GUI, and SDK, so that whenever information is displayed, it is read through the REST API, and when configuration changes are made, they are written through the REST API. The REST API also provides an interface through which other information can be retrieved, including statistics, faults, and audit events. It even provides a means of subscribing to push-based event notification, so that when a change occurs in the MIT, an event can be sent through a web socket. Standard REST methods are supported on the API, which includes POST, GET, and DELETE operations through HTTP. The GET method is nullipotent, meaning that it can be called zero or more times without making any changes (or that it is a read-only operation).","title":"How to pull informations using REST API"},{"location":"monitor-aci/#1-creating-the-api-command","text":"You can invoke an API command or query by sending an HTTP or HTTPS message to the APIC with a URI of this form for an operation on a managed object (MO): {http | https}://host [:port] /api/mo/dn. {json | xml} [?options] Use this form for an operation on an object class: {http | https}://host [:port] /api/class/className. {json | xml} [?options] Note While the preceding examples use /api/mo and /api/class in the URI string, the APIC UI and Visore also use the /api/node/mo and /api/node/class syntax in the URI string. Both formats are valid and are used interchangeably in this document. This example shows a URI for an API operation that involves an MO of class fv:Tenant: https://apic-ip-address/api/mo/uni/tn-ExampleCorp.xml","title":"1. Creating the API Command"},{"location":"monitor-aci/#11-the-uri-for-an-api-operation-on-an-mo","text":"https://apic-ip-address/api/mo/uni/tn-ExampleCorp.xml","title":"1.1 The URI for an API Operation on an MO"},{"location":"monitor-aci/#12-the-uri-for-an-api-operation-on-an-node-mo","text":"In an API operation to access an MO on a specific node device in the fabric, the resource path consists of /mo/topology/pod-number/node-number/sys/ followed by the node component. For example, to access a board sensor in chassis slot b of node-1 in pod-1, use this URI: GET https://apic-ip-address/api/mo/topology/pod-1/node-1/sys/ch/bslot/board/sensor-3.json","title":"1.2 The URI for an API Operation on an Node MO"},{"location":"monitor-aci/#13-the-uri-for-an-api-operation-on-a-class","text":"In an API operation to get information about a class, the resource path consists of /class/ followed by the name of the class as described in the Cisco APIC Management Information Model Reference. In the URI, the colon in the class name is removed. For example, this URI specifies a query on the class aaa:User: GET https://apic-ip-address/api/class/aaaUser.json","title":"1.3. The URI for an API Operation on a Class"},{"location":"monitor-aci/#2-pulling-data-from-created-tenant","text":"In the restAPI section you created tenant called \"Postman-collection-1\" . That tenant have 22 EPGs and BDs and now you would like to pull data about one of them. POTENTIAL EXCERCISE: Server Team asked you to add new static binding for server addressed in subnet 172.16.59.0/24. You don't know what is name of EPG and now you have to do reverse engineering to find correct EPG. You know that GW IP address of BD is 172.16.59.1/24. Your task is to find BD name using GET API query and by having BD, find name of EPG associated to it. Where to start? Think about how to find class-name of Bridge Domain. During the day you used few methods - \"Save As\" object, \"Show Debug Info\" in APIC, \"API Inspector\", \"Open in Object Store Browser\". Choice method you prefer and find class-name for BD. Once you have it, compose URL, Answer below: https://{{apic}}/api/class/fvBD.json Create Postman Request to GET information from class-name you specify above. Result should shows you \"totalCount\": \"28\" meaning you found 28 bridge domains and you listed their attributes. Still you are not able to find out the correct one. Now list all sub-tree of object fvBD. Do it with adding KEY parameter to your GET query: https://{{apic}}/api/class/fvBD.json?query-target=subtree You listed all Bridge Domains with all MIT for them, now results with more than 200 items in APIC response. Having information about IP address of GW, you can narrow it to one, having that specific IP. Let's try: https://{{apic}}/api/class/fvBD.json?query-target=subtree&query-target-filter=and(wcard(fvSubnet.ip,\"172.16.59.1/24\")) Function which contain IP addresses under BD is fvSubnet. The attribute of the function is ip . By using filter query-target-filter=and(wcard(fvSubnet.ip,\"172.16.59.1/24\")) you get information about DN of function. The DN contains Bridge-Domain name. Results on the figure below: Please list informations about Bridge Domain you found: BD-59 . https://{{apic}}/api/class/fvBD.json?query-target=subtree&query-target-filter=and(eq(fvBD.name,\"BD-59\")) Still you don't have EPG name. Why you don't see sub-tree of the fvBD in your GET response?? You need to specify RSP-SUBTREE in your GET. https://{{apic}}/api/class/fvBD.json?query-target=subtree&rsp-subtree=full&query-target-filter=and(eq(fvBD.name,\"BD-59\")) In the output you should look for fvRtBd class-name. It contains information about EPG to BD relation. Now try yourself to narrow down the output. https://{{apic}}/api/class/fvRtBd.json?query-target=subtree&query-target-filter=and(wcard(fvRtBd.dn,\"BD-59\")) Using URL above you are able to filter response from APIC for class-name fvRtBd and DN path which contain BD-59 in. fvRtBd.dn { \"fvRtBd\" : { \"attributes\" : { \"childAction\" : \"\" , \"dn\" : \"uni/tn-Postman-collection-1/BD-BD-59/rtbd-[uni/tn-Postman-collection-1/ap-APPLICATION-PROFILE-1/epg-EPG-59]\" , \"lcOwn\" : \"local\" , \"modTs\" : \"2022-11-04T18:22:39.217+00:00\" , \"status\" : \"\" , \"tCl\" : \"fvAEPg\" , \"tDn\" : \"uni/tn-Postman-collection-1/ap-APPLICATION-PROFILE-1/epg-EPG-59\" } } }","title":"2. Pulling data from created Tenant"},{"location":"monitor-aci/#congratulations-you-found-correct-epg-epg-59","text":"","title":" CONGRATULATIONS, you found correct EPG:  EPG-59 "},{"location":"monitor-aci/#please-see-the-collection-of-rest-api-urls-for-further-details","text":"","title":" Please see the Collection of REST API URLs for further details. "},{"location":"ndo-tenant/","text":"Nexus Dasboard Orchestrator Tenant configuration In this section we will configure our first Tenant, streteched between Azure and AWS. As Cloud Network Controller will be used for configuration, we need to make sure that CNC has privillages to do so. Will learn how to do it. Tenant Creation on NDO On the Left navigation page click \"Application Management\" -> \"Tenant\" and then \"Add Tenant\" Fill in Tenant details for name and description Display Name: Tenant-01 Descrption: Cisco Live 2023 AMS Tenant Associate Tenant to both Site by checking the checkbox next to it. Note For now you are not able to Save this configuration with red marking on both Sites. Click the Pencil button at the end of each site line to finish configuration. Additional setting are needed for CNC, so it knows which subscribtion to use on Azure and which Tenant ID on AWS. CNC-Azure-01 site configuration For Azure site, we will be using the same Subscription as the one used for CNC deployment - select Mode as \"Select Shared\" and use existing subscription from drop-down. Leave security domains empty. Hit Save . CNC-AWS-01 site configuration For AWS site we have 2 options - Untrusted with Cloud Access key and Secret or Trusted . In our case we would be using Trusted configuration. Each Tenant create on NDO with AWS Site association required sepearete Account ID on AWS site. You can find your in POD Details. Fill in with your POD AWS User-Account ID and select Access Type as Trusted , hit Save . Now configuration can be saved, leave assocaited user list empty as there are no additional users and hit Save . On the Tenant list you should see Tenant-01 created an assigned to 2 Sites.","title":"NDO Tenant configuration"},{"location":"ndo-tenant/#nexus-dasboard-orchestrator-tenant-configuration","text":"In this section we will configure our first Tenant, streteched between Azure and AWS. As Cloud Network Controller will be used for configuration, we need to make sure that CNC has privillages to do so. Will learn how to do it.","title":"Nexus Dasboard Orchestrator Tenant configuration"},{"location":"ndo-tenant/#tenant-creation-on-ndo","text":"On the Left navigation page click \"Application Management\" -> \"Tenant\" and then \"Add Tenant\" Fill in Tenant details for name and description Display Name: Tenant-01 Descrption: Cisco Live 2023 AMS Tenant Associate Tenant to both Site by checking the checkbox next to it. Note For now you are not able to Save this configuration with red marking on both Sites. Click the Pencil button at the end of each site line to finish configuration. Additional setting are needed for CNC, so it knows which subscribtion to use on Azure and which Tenant ID on AWS. CNC-Azure-01 site configuration For Azure site, we will be using the same Subscription as the one used for CNC deployment - select Mode as \"Select Shared\" and use existing subscription from drop-down. Leave security domains empty. Hit Save . CNC-AWS-01 site configuration For AWS site we have 2 options - Untrusted with Cloud Access key and Secret or Trusted . In our case we would be using Trusted configuration. Each Tenant create on NDO with AWS Site association required sepearete Account ID on AWS site. You can find your in POD Details. Fill in with your POD AWS User-Account ID and select Access Type as Trusted , hit Save . Now configuration can be saved, leave assocaited user list empty as there are no additional users and hit Save . On the Tenant list you should see Tenant-01 created an assigned to 2 Sites.","title":"Tenant Creation on NDO"},{"location":"object-map/","text":"","title":"ACI to Public Cloud Object mapping"},{"location":"postman/","text":"Install and Operate Postman Postman is an API platform for building and using APIs. Postman simplifies each step of the API lifecycle and streamlines collaboration so you can create better APIs\u2014faster. 1. Install Postman in your workstation First step to do in our dCloud lab is download and install of Postman. Login to your dCloud workstation and from the Chrome connect to: https://www.postman.com/downloads/ Select Windows 64-bit to download the installation file. Download the file to local folder of your choice. Start installation - it will automatically install and open application. Note You installed and run Postman in Scratch Pad version. If you would like to use workspace version, it is required to setup Free account. It will keep your Postman Collections in cloud, helps to collaborate and backup work you do. 2 Initial setup of Postman Once the Postman is up and running you should create few things. First, setup your Environment data. Environment definition can be reused later, it is also good way to keep secured sensitive data like credentials. Second, create your Request Collection or import one from .json file. 2.1 Create Environment for our dCloud lab Environment definition is a place where you can store information about IP address of APIC and credentials. Once you run your restAPI queries, Postman will automatically use variables defined here. Lets do it then for our scenario. Navigate to Environment section in Postman Dashboard as marked on figure below. In this place you can select existing Environment, edit/delete it or create new one. To review existing Environment use icon most on right in red marked section on Figure above. It will open section shown at next figure. Click Add to create new entry. Add button will move you to new Postman Tab Change name of your Environment to: ACI-dcloud Configure VARIABLES: apic, user, password INITIAL VALUES: 198.18.133.200, admin, C1sco12345 Change Type of data for password to secret Save it. Now you have ready Environment you can choice from drop-down menu. You will use it for rest of Day1 exercises. 2.2 Create New Collection Click on Link Create Collection indicated in the figure below: Name your New Collection ACI dCloud Set Authorization type to Basic Auth . Once done, use Ctrl+S or Save button on dashboard - Floppy Disk icon. Do not type anything to \"Username\" and \"Password\". Those data will be pulled from your Environment - created in previous task. Now you have Environment and Collection ready. We can start working with our restAPI requests.","title":"Install and Operate Postman"},{"location":"postman/#install-and-operate-postman","text":"Postman is an API platform for building and using APIs. Postman simplifies each step of the API lifecycle and streamlines collaboration so you can create better APIs\u2014faster.","title":"Install and Operate Postman"},{"location":"postman/#1-install-postman-in-your-workstation","text":"First step to do in our dCloud lab is download and install of Postman. Login to your dCloud workstation and from the Chrome connect to: https://www.postman.com/downloads/ Select Windows 64-bit to download the installation file. Download the file to local folder of your choice. Start installation - it will automatically install and open application. Note You installed and run Postman in Scratch Pad version. If you would like to use workspace version, it is required to setup Free account. It will keep your Postman Collections in cloud, helps to collaborate and backup work you do.","title":"1. Install Postman in your workstation"},{"location":"postman/#2-initial-setup-of-postman","text":"Once the Postman is up and running you should create few things. First, setup your Environment data. Environment definition can be reused later, it is also good way to keep secured sensitive data like credentials. Second, create your Request Collection or import one from .json file.","title":"2 Initial setup of Postman"},{"location":"postman/#21-create-environment-for-our-dcloud-lab","text":"Environment definition is a place where you can store information about IP address of APIC and credentials. Once you run your restAPI queries, Postman will automatically use variables defined here. Lets do it then for our scenario. Navigate to Environment section in Postman Dashboard as marked on figure below. In this place you can select existing Environment, edit/delete it or create new one. To review existing Environment use icon most on right in red marked section on Figure above. It will open section shown at next figure. Click Add to create new entry. Add button will move you to new Postman Tab Change name of your Environment to: ACI-dcloud Configure VARIABLES: apic, user, password INITIAL VALUES: 198.18.133.200, admin, C1sco12345 Change Type of data for password to secret Save it. Now you have ready Environment you can choice from drop-down menu. You will use it for rest of Day1 exercises.","title":"2.1 Create Environment for our dCloud lab"},{"location":"postman/#22-create-new-collection","text":"Click on Link Create Collection indicated in the figure below: Name your New Collection ACI dCloud Set Authorization type to Basic Auth . Once done, use Ctrl+S or Save button on dashboard - Floppy Disk icon. Do not type anything to \"Username\" and \"Password\". Those data will be pulled from your Environment - created in previous task. Now you have Environment and Collection ready. We can start working with our restAPI requests.","title":"2.2 Create New Collection"},{"location":"resources/","text":"Resources from the LAB In this section you can find necessary files used during the Lab to download. Cisco API Documentation Cisco ACI API Configuration guide Cisco ACI Authentication API CSV Files used for deployments tenant-create.csv - used for deployment of 22 BDs/EPGs in new Tenant Use-Case 1 tenant-common.csv - used for use-cases, configuration of Common Tenant var-data-uc1 - used for Use-Case1 custom tenant definition Use-Case 2 tenant-use-case-2.json - Tenant definition var-data-uc2 - used for Use-Case2 custom tenant definition JSON additional files to deploy ACI configuration Fabric Switches Provisioning JSON interface leaf profile JSON switch leaf profile JSON switch VPC policy JSON Tenant-1 Ready Postman Environment and Collection - FULL LIST ACI-dcloud Environment ACI dCloud Collection Collection of REST API md file to download ACI Usefull REST API","title":"Resources"},{"location":"resources/#resources-from-the-lab","text":"In this section you can find necessary files used during the Lab to download.","title":"Resources from the LAB"},{"location":"resources/#cisco-api-documentation","text":"Cisco ACI API Configuration guide Cisco ACI Authentication API","title":"Cisco API Documentation"},{"location":"resources/#csv-files-used-for-deployments","text":"tenant-create.csv - used for deployment of 22 BDs/EPGs in new Tenant","title":"CSV Files used for deployments"},{"location":"resources/#use-case-1","text":"tenant-common.csv - used for use-cases, configuration of Common Tenant var-data-uc1 - used for Use-Case1 custom tenant definition","title":"Use-Case 1"},{"location":"resources/#use-case-2","text":"tenant-use-case-2.json - Tenant definition var-data-uc2 - used for Use-Case2 custom tenant definition","title":"Use-Case 2"},{"location":"resources/#json-additional-files-to-deploy-aci-configuration","text":"Fabric Switches Provisioning JSON interface leaf profile JSON switch leaf profile JSON switch VPC policy JSON Tenant-1 Ready","title":"JSON additional files to deploy ACI configuration"},{"location":"resources/#postman-environment-and-collection-full-list","text":"ACI-dcloud Environment ACI dCloud Collection","title":"Postman Environment and Collection - FULL LIST"},{"location":"resources/#collection-of-rest-api-md-file-to-download","text":"ACI Usefull REST API","title":"Collection of REST API md file to download"},{"location":"restAPI/","text":"Cisco ACI rest API In this lab section you will create and execute list of API calls to configure ACI managed objects as well as read data from existing polices and devices. After this section you should feel comfortable with running simple automation tasks and build your own tasks for further customizations. You will be familiar with Postman dashboard and general idea of running request individually or as a collection defined. 1 Define restAPI calls under Collection Now is a time to work with our requests to GET or POST restAPI. You will define couple of them during the LAB. Figure below shows where to navigate to create new request in your collection. Every API command have predefined structure. Administrator needs to specify URI for correct object are you going to work with. Everything is described in details under the following link: Cisco ACI API Configuration guide . The figure below shows structure of every API call: 1.1 Create ACI login request First request when working with ACI must be authentication. Without having your session authenticated, no API calls can be successfully executed. Information about Authentication request structure can be found here: Cisco ACI Authentication API . Define it in Postman: In New Request section specify: 1) Name of Request ACI Login 2) Select Method to be POST 3) Enter request URL https://{{apic}}/api/aaaLogin.json Note Please notice that it's first place you use your Environment Variable = apic. In JSON definition every variable is closed by {{ }} . 4) Move to Body section for further work with your request Once you open Body section, you need to select type of data to be raw and coding to JSON . Now you can copy code defined below to body section of your request. aaaLogin { \"aaaUser\" : { \"attributes\" : { \"name\" : \"{{user}}\" , \"pwd\" : \"{{password}}\" } } } Yellow highlighted text in the json code above will use variables you defined in Environment at the beginning of the Lab. Now your Request should looks like on the figure below. It's time to TEST it! . Click on Send button. Make sure your environment ACI-dcloud is selected - above the Send button. When works as expected you will be authenticated to APIC. Responce from the APIC is visible in the section at your screen. Let's go together across results you see on the screen. First is a Status 200 OK - means APIC server accepted your request, execute it and responce with data. In the REPLY Body you see JSON structure data. You can see token which is your authentication token. RefreshTimeoutSeconds set to 600 seconds = 10 minutes. Token is valid for 10 minutes and after that period will expire. Please scroll down across the response body yourself. Look for such information: userName sessionId aaaWriteRoles Write them down to notepad. Register your ACI Fabric Switches in APIC Lab is clear and leaf/spines requires registration. You can automate it using Postman. Configure new Postman POST Request with code downloaded from here use URI: https://{{apic}}/api/node/class/fabricNode.json Contact Instructor in case of issues. 1.2 Get Information About a Node POST is not only one request type. You can configure your ACI fabric, but you can use restAPI to pull data you are interest to analyse. This excercise is about writing a GET Request under already created ACI collection. As an example you will pull information about Leaf node-101 system in json . Name your new Request Node-101-topsystem check Copy the URI to GET request: https://{{apic}}/api/mo/topology/pod-1/node-101/sys.json Save new Request and Click Send . Note It may happen that your API Token expired - remember its valid only 10 minutes. If you experience it, go to ACI Login Request and Send it again to re-authenticate. In the Body responce you can verify topsystem information about Leaf in your Fabric. You can pull data about AccessPolices, Interfaces as well as Logical construct - Tenant, EPGs, VRFs etc. The idea is always same - specify correct URL. Another test will be quering information about APIC node and current firmware version running on it. Please Create another Request under your Collection with following setup: Name of Request: Get Running Firmware GET request URI: https://{{apic}}/api/mo/topology/pod-1/node-1/sys/ctrlrfwstatuscont.json?query-target=subtree&target-subtree-class=firmwareCtrlrRunning Please observe that current query is composed with parameters. Before we pulled data directly from top-system of our ACI Leaf. Now you are narrowing the output to only sub-class you are intrested about. Try to delete part of URI ?query-target=subtree&target-subtree-class=firmwareCtrlrRunning and do Send again. 2 ACI Access Polices Every ACI Fabric configuration starts with Access Polices. You need to define ACI objects which represent single physical port configurations. To remind relationships between objects, please check following figure. Usually Interface Polices are configured once and re-used like LLDP, LACP, CDP, etc. However our LAB is empty and you need to start from zero. Following sections will help you to prepare your API requests. 2.1 Interface Policies You will configure LACP Policy, LLDP_ON, LINK-10G. Copy each of the Policy json definition to individual POST requests under your ACI Collection in Postman. Use URI to POST: https://{{apic}}/api/node/mo/uni.json LACP_ACTIVE { \"lacpLagPol\" : { \"attributes\" : { \"dn\" : \"uni/infra/lacplagp-LACP_ACTIVE\" , \"ctrl\" : \"fast-sel-hot-stdby,graceful-conv,susp-individual\" , \"name\" : \"LACP_ACTIVE\" , \"mode\" : \"active\" , \"rn\" : \"lacplagp-LACP_ACTIVE\" , \"status\" : \"created\" }, \"children\" : [] } } LLDP_ON { \"lldpIfPol\" : { \"attributes\" : { \"dn\" : \"uni/infra/lldpIfP-LLDP_ON\" , \"name\" : \"LLDP_ON\" , \"rn\" : \"lldpIfP-LLDP_ON\" , \"status\" : \"created\" }, \"children\" : [] } } LINK-10G { \"fabricHIfPol\" : { \"attributes\" : { \"dn\" : \"uni/infra/hintfpol-LINK-10G\" , \"name\" : \"LINK-10G\" , \"speed\" : \"10G\" , \"rn\" : \"hintfpol-LINK-10G\" , \"status\" : \"created\" }, \"children\" : [] } } 2.2 VLANs, Domains and AAEPs Create VLAN pool with vlan-range, associate it with Physical Domain and AAEP. Now you will change an approach. Your task is to create VLAN Pool with APIC GUI and capture json code from API Inspector . Connect to your APIC using bookmark in dCloud workstation Chrome. Navigate to Access Polices -> Pools -> VLAN. When you are there, Click on Help and Tools menu in APIC Dashboard and Click on Show API Inspector . In new window you will open API Inspector, where you can capture every API call send to the APIC server. Now you can start ADD VLAN-pool in APIC GUI. VLAN POOL NAME: vlan-pool Allocation Mode: Static Encap Blocks : 100-200, Inherit Allocation mode from parent, Role: External or On the wire encapsulations SUBMIT and open API Inspector in the another Chrome Window. Search for POST method. COPY now ALL POST URI from API Inspector to Postman request Body. You should capture data as shown in code below. h tt ps : //apic1.dcloud.cisco.com/api/node/mo/uni/infra/vlanns-[vlan-pool]-static.json payload { \"fvnsVlanInstP\" : { \"attributes\" : { \"dn\" : \"uni/infra/vlanns-[vlan-pool]-static\" , \"name\" : \"vlan-pool\" , \"allocMode\" : \"static\" , \"rn\" : \"vlanns-[vlan-pool]-static\" , \"status\" : \"created\" }, \"children\" : [ { \"fvnsEncapBlk\" : { \"attributes\" : { \"dn\" : \"uni/infra/vlanns-[vlan-pool]-static/from-[vlan-100]-to-[vlan-200]\" , \"from\" : \"vlan-100\" , \"to\" : \"vlan-200\" , \"rn\" : \"from-[vlan-100]-to-[vlan-200]\" , \"status\" : \"created\" }, \"children\" : [] } } ] } } Now it's time to build a request from data captured. You will use that request in the future for adding more vlan pools. First, copy URL and place it in Postman URL. Replace apic1.dcloud.cisco.com with our variable {{apic}}. Next replace vlanpool name in [ ] brakets. New URI looks like that: https://{{apic}}/api/node/mo/uni/infra/vlanns-[{{vlanpoolname}}]-static.json VLAN POOL { \"fvnsVlanInstP\" : { \"attributes\" : { \"dn\" : \"uni/infra/vlanns-[{{vlanpoolname}}]-static\" , \"name\" : \"{{vlanpoolname}}\" , \"allocMode\" : \"static\" , \"rn\" : \"vlanns-[{{vlanpoolname}}]-static\" , \"status\" : \"created\" }, \"children\" : [ { \"fvnsEncapBlk\" : { \"attributes\" : { \"dn\" : \"uni/infra/vlanns-[{{vlanpoolname}}]-static/from-[vlan-{{vlanstart}}]-to-[vlan-{{vlanend}}]\" , \"from\" : \"vlan-{{vlanstart}}\" , \"to\" : \"vlan-{{vlanend}}\" , \"rn\" : \"from-[vlan-{{vlanstart}}]-to-[vlan-{{vlanend}]\" , \"status\" : \"created\" }, \"children\" : [] } } ] } } Variables {{vlanpoolname}}, {{vlanstart}}, {{vlanend}} you can replace by hardcoded values in your code, or define values in Environment. Later today you will see the powerfull of variables using csv file for input data. Same approach please use in Domain and AAEP creation - Go to GUI, do Domain - PHY-DOM, associate with vlan-pool and AAEP - dcloud-AAEP. Observe API Inspector, capture the code and build those two requests in Postman. URI for AAEP Function: https://{{apic}}/api/node/mo/uni/infra.json AAEP { \"infraInfra\" : { \"attributes\" : { \"dn\" : \"uni/infra\" , \"status\" : \"modified\" }, \"children\" : [ { \"infraAttEntityP\" : { \"attributes\" : { \"dn\" : \"uni/infra/attentp-{{aaep}}\" , \"name\" : \"{{aaep}}\" , \"rn\" : \"attentp-{{aaep}}\" , \"status\" : \"created\" }, \"children\" : [] } }, { \"infraFuncP\" : { \"attributes\" : { \"dn\" : \"uni/infra/funcprof\" , \"status\" : \"modified\" }, \"children\" : [] } } ] } } Last one is Domain. Create Physical domain PHY-DOM with vlan pool associated vlan-pool and AAEP dcloud-AAEP . https://{{apic}}/api/node/mo/uni/phys-{{domain}}.json Domain { \"physDomP\" : { \"attributes\" : { \"dn\" : \"uni/phys-{{domain}}\" , \"name\" : \"{{domain}}\" , \"rn\" : \"phys-{{domain}}\" , \"status\" : \"created\" }, \"children\" : [ { \"infraRsVlanNs\" : { \"attributes\" : { \"tDn\" : \"uni/infra/vlanns-[{{vlanpoolname}}]-static\" , \"status\" : \"created\" }, \"children\" : [] } } ] } } Interesting fact about domain to AAEP association. Domain is sub-object to AAEP, you will not see AAEP under Domain definition, even you specify it in APIC GUI. In API you need to add it under AAEP. https://{{apic}}/api/node/mo/uni/infra.json Domain to AAEP association { \"infraAttEntityP\" : { \"attributes\" : { \"annotation\" : \"\" , \"descr\" : \"\" , \"dn\" : \"uni/infra/attentp-{{aaep}}\" , \"name\" : \"{{aaep}}\" , \"nameAlias\" : \"\" , \"ownerKey\" : \"\" , \"ownerTag\" : \"\" , \"userdom\" : \":all:\" }, \"children\" : [ { \"infraRsDomP\" : { \"attributes\" : { \"annotation\" : \"\" , \"tDn\" : \"uni/phys-{{domain}}\" , \"userdom\" : \":all:\" } } } ] } } 2.3 Interface Policy Group Lets now create one of the Interface policy group and use all created policy. Interface Policy Group VPC { \"infraAccBndlGrp\" : { \"attributes\" : { \"dn\" : \"uni/infra/funcprof/accbundle-intpolgrp-vpc-server1\" , \"lagT\" : \"node\" , \"name\" : \"intpolgrp-vpc-server1\" , \"rn\" : \"accbundle-intpolgrp-vpc-server1\" , \"status\" : \"created\" }, \"children\" : [ { \"infraRsAttEntP\" : { \"attributes\" : { \"tDn\" : \"uni/infra/attentp-dcloud-AAEP\" , \"status\" : \"created,modified\" }, \"children\" : [] } }, { \"infraRsHIfPol\" : { \"attributes\" : { \"tnFabricHIfPolName\" : \"LINK-10G\" , \"status\" : \"created,modified\" }, \"children\" : [] } }, { \"infraRsLldpIfPol\" : { \"attributes\" : { \"tnLldpIfPolName\" : \"LLDP_ON\" , \"status\" : \"created,modified\" }, \"children\" : [] } }, { \"infraRsLacpPol\" : { \"attributes\" : { \"tnLacpLagPolName\" : \"LACP_ACTIVE\" , \"status\" : \"created,modified\" }, \"children\" : [] } } ] } } Replace intpolgrp-vpc-server1 with intpolgrp-vpc-server2 to observe how simple you can add new VPC policy group using Postman. Warning Download three files to your dcloud workstation and using post in Postman or in APIC GUI upload it to access-polices. Without them, your VPC won't instanciate and cannot be used in later stage of the lab. JSON VPC Policy . JSON interface leaf profile . JSON switch leaf profile . Ask instructor for assistance if not clear. 3 ACI Tenant Upon now you created ACI AccessPolicies for your Tenant. Having JSON code for every object you will be able to unified variables across Postman Collection and then run all together using Variables from Environment or much better from CSV file. Now is time to create your tenant using JSON. It will be simple Tenant definition with one VRF and two bridge domains associated with two EPGs. Every EPG will be associated with created domain and statically binded to both VPC created, with VLAN from VLAN pool done by you perviously. Figure below shows our Logical Tenant. You need to define Tenant, VRF, Bridge-Domains, Application-Profile, EPGs with Domain association and static binding under EPG. Quite many of Requests to do in Postman. You can define all of them under one Request. Please, download a JSON file from JSON Tenant Ready . Once you have it on dcloud workstation, please create new Request under ACI Collection: NAME: CREATE TENANT-1 Method: POST URL: https://{{apic}}/api/node/mo/uni.json COPY now content from downloaded file to Body of the new request, Save and Send it. Note Please check your authentication token if still valid. Tenant-1 will be created with all structure shown in figure above. 3.1 Tenant components This section contain JSON codes necessary to create Tenant objects. 3.1.1 Tenant and VRF Code below can be used to create new Tenant and VRF. https://{{apic}}/api/node/mo/uni.json Tenant and VRF from CSV { \"fvTenant\" : { \"attributes\" : { \"annotation\" : \"\" , \"descr\" : \"\" , \"dn\" : \"uni/tn-{{tenantname}}\" , \"name\" : \"{{tenantname}}\" }, \"children\" : [ { \"fvCtx\" : { \"attributes\" : { \"name\" : \"{{vrfname}}\" }, \"children\" : [] } } ] } } 3.1.2 BD in existing tenant Code below can be used to create new Bridge-Domain in existing Tenant. https://{{apic}}/api/node/mo/uni.json Bridge-domain L3 from CSV { \"fvBD\" : { \"attributes\" : { \"name\" : \"{{bdname}}\" , \"dn\" : \"uni/tn-{{tenantname}}/BD-{{bdname}}\" , \"rn\" : \"BD-{{bdname}}\" , \"type\" : \"regular\" }, \"children\" : [ { \"fvSubnet\" : { \"attributes\" : { \"ip\" : \"{{ipaddress}}\" , \"ipDPLearning\" : \"enabled\" , \"scope\" : \"private\" } } }, { \"fvRsCtx\" : { \"attributes\" : { \"tnFvCtxName\" : \"{{vrfname}}\" } } } ] } } In case you need to add BD without IP address and unicast routing disabled, use code below. https://{{apic}}/api/node/mo/uni.json Bridge-domain L2 from CSV { \"fvTenant\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}\" , \"status\" : \"modified\" }, \"children\" : [ { \"fvBD\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}/BD-{{bdname}}\" , \"mac\" : \"00:22:BD:F8:19:FF\" , \"name\" : \"{{bdname}}\" , \"rn\" : \"BD-{{bdname}}\" , \"arpFlood\" : \"yes\" , \"ipLearning\" : \"no\" , \"unicastRoute\" : \"no\" , \"unkMacUcastAct\" : \"flood\" , \"type\" : \"regular\" , \"unkMcastAct\" : \"flood\" , \"status\" : \"created\" }, \"children\" : [ { \"fvRsCtx\" : { \"attributes\" : { \"tnFvCtxName\" : \"{{vrfname}}\" , \"status\" : \"created,modified\" }, \"children\" : [] } } ] } } ] } } Highlighted rows are specific for L2 Bridge-Domain. 3.1.3 Application Profile Component which contain all EPGs in the Tenant. https://{{apic}}/api/node/mo/uni.json Application Profile from CSV { \"fvAp\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}/ap-{{ap}}\" , \"name\" : \"{{ap}}\" , \"rn\" : \"ap-{{ap}}\" , \"status\" : \"created\" } } } 3.1.4 EPGs in existing tenant/appprofiles and associated with domain https://{{apic}}/api/node/mo/uni.json Create EPG under existing application profile from CSV { \"fvAEPg\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}/ap-{{ap}}/epg-{{epgname}}\" , \"name\" : \"{{epgname}}\" , \"rn\" : \"epg-{{epgname}}\" , \"status\" : \"created\" }, \"children\" : [ { \"fvRsBd\" : { \"attributes\" : { \"tnFvBDname\" : \"{{bdname}}\" , \"status\" : \"created,modified\" }, \"children\" : [] } }, { \"fvRsDomAtt\" : { \"attributes\" : { \"tDn\" : \"uni/phys-{{domain}}\" , \"status\" : \"created\" }, \"children\" : [] } } ] } } 4 Use CSV file for input data You are about to re-use work you did during all excercises. Untill now you run every request for static data, only once. Adding 100 Bridge-Domains, by changing \"Variables\" within the code would be problematic. Postman is giving an option to use external variable file to execute created requests in the collection. By running five requests from your Collection and using CSV file from location FILE CSV TO DOWNLOAD you will add new Tenant with one VRF, Applicatioin Profile, 22 Bridge-Domains and 22 EPGs associated to Physical Domain created before. All of this will be done in one Postman collection Run. 4.1 Run Collection requests Follow the instruction from the figure below: 1) Click on dots icon at your ACI dCloud collection folder 2) Select Run Collection You will be moved to new dashboard, where you will select Requests and input file. Deselect All, and then Select only needed requests: When you select Requests, go to Data \"Select File\" and choice downloaded CSV file - tenant-create.csv . Confirm that your file is selected as on the figure above. You can confirm amount of iterations in the top field Iterations . Note Do not forget to authenticate your Postman session using ACI Login request. I recommend to do it out of collection Run -- beforehand. Click Run ACI dCloud Blue button. 4.2 Verification after runing the collection When you successfully execute your collection, in Summary at the screen you will see results of every Iteration. When Request was accepted by APIC, result will be 200 OK . Every Iteration contain all five seleceted requests. You probably noticed that one of them is getting 400 Bad Request - \"Bridge Domain L3 from CSV\". Reason for that, is missing children attribute fvSubnet for all L2 bridge-domain in our CSV file. Now connect to APIC GUI and verify if tenant exists, check dashboard status. You successfully Created New Tenant with 22 EPGs and 22 BDs. Please, edit CSV file in notepad++ editor at your dCloud workstation, replace Tenantname with new and run your Postman Collection once again. Do you understand now power of simple automation?","title":"Cisco ACI rest API"},{"location":"restAPI/#cisco-aci-rest-api","text":"In this lab section you will create and execute list of API calls to configure ACI managed objects as well as read data from existing polices and devices. After this section you should feel comfortable with running simple automation tasks and build your own tasks for further customizations. You will be familiar with Postman dashboard and general idea of running request individually or as a collection defined.","title":"Cisco ACI rest API"},{"location":"restAPI/#1-define-restapi-calls-under-collection","text":"Now is a time to work with our requests to GET or POST restAPI. You will define couple of them during the LAB. Figure below shows where to navigate to create new request in your collection. Every API command have predefined structure. Administrator needs to specify URI for correct object are you going to work with. Everything is described in details under the following link: Cisco ACI API Configuration guide . The figure below shows structure of every API call:","title":"1 Define restAPI calls under Collection"},{"location":"restAPI/#11-create-aci-login-request","text":"First request when working with ACI must be authentication. Without having your session authenticated, no API calls can be successfully executed. Information about Authentication request structure can be found here: Cisco ACI Authentication API . Define it in Postman: In New Request section specify: 1) Name of Request ACI Login 2) Select Method to be POST 3) Enter request URL https://{{apic}}/api/aaaLogin.json Note Please notice that it's first place you use your Environment Variable = apic. In JSON definition every variable is closed by {{ }} . 4) Move to Body section for further work with your request Once you open Body section, you need to select type of data to be raw and coding to JSON . Now you can copy code defined below to body section of your request. aaaLogin { \"aaaUser\" : { \"attributes\" : { \"name\" : \"{{user}}\" , \"pwd\" : \"{{password}}\" } } } Yellow highlighted text in the json code above will use variables you defined in Environment at the beginning of the Lab. Now your Request should looks like on the figure below. It's time to TEST it! . Click on Send button. Make sure your environment ACI-dcloud is selected - above the Send button. When works as expected you will be authenticated to APIC. Responce from the APIC is visible in the section at your screen. Let's go together across results you see on the screen. First is a Status 200 OK - means APIC server accepted your request, execute it and responce with data. In the REPLY Body you see JSON structure data. You can see token which is your authentication token. RefreshTimeoutSeconds set to 600 seconds = 10 minutes. Token is valid for 10 minutes and after that period will expire. Please scroll down across the response body yourself. Look for such information: userName sessionId aaaWriteRoles Write them down to notepad.","title":"1.1 Create ACI login request"},{"location":"restAPI/#register-your-aci-fabric-switches-in-apic","text":"Lab is clear and leaf/spines requires registration. You can automate it using Postman. Configure new Postman POST Request with code downloaded from here use URI: https://{{apic}}/api/node/class/fabricNode.json Contact Instructor in case of issues.","title":"Register your ACI Fabric Switches in APIC"},{"location":"restAPI/#12-get-information-about-a-node","text":"POST is not only one request type. You can configure your ACI fabric, but you can use restAPI to pull data you are interest to analyse. This excercise is about writing a GET Request under already created ACI collection. As an example you will pull information about Leaf node-101 system in json . Name your new Request Node-101-topsystem check Copy the URI to GET request: https://{{apic}}/api/mo/topology/pod-1/node-101/sys.json Save new Request and Click Send . Note It may happen that your API Token expired - remember its valid only 10 minutes. If you experience it, go to ACI Login Request and Send it again to re-authenticate. In the Body responce you can verify topsystem information about Leaf in your Fabric. You can pull data about AccessPolices, Interfaces as well as Logical construct - Tenant, EPGs, VRFs etc. The idea is always same - specify correct URL. Another test will be quering information about APIC node and current firmware version running on it. Please Create another Request under your Collection with following setup: Name of Request: Get Running Firmware GET request URI: https://{{apic}}/api/mo/topology/pod-1/node-1/sys/ctrlrfwstatuscont.json?query-target=subtree&target-subtree-class=firmwareCtrlrRunning Please observe that current query is composed with parameters. Before we pulled data directly from top-system of our ACI Leaf. Now you are narrowing the output to only sub-class you are intrested about. Try to delete part of URI ?query-target=subtree&target-subtree-class=firmwareCtrlrRunning and do Send again.","title":"1.2 Get Information About a Node"},{"location":"restAPI/#2-aci-access-polices","text":"Every ACI Fabric configuration starts with Access Polices. You need to define ACI objects which represent single physical port configurations. To remind relationships between objects, please check following figure. Usually Interface Polices are configured once and re-used like LLDP, LACP, CDP, etc. However our LAB is empty and you need to start from zero. Following sections will help you to prepare your API requests.","title":"2  ACI Access Polices"},{"location":"restAPI/#21-interface-policies","text":"You will configure LACP Policy, LLDP_ON, LINK-10G. Copy each of the Policy json definition to individual POST requests under your ACI Collection in Postman. Use URI to POST: https://{{apic}}/api/node/mo/uni.json LACP_ACTIVE { \"lacpLagPol\" : { \"attributes\" : { \"dn\" : \"uni/infra/lacplagp-LACP_ACTIVE\" , \"ctrl\" : \"fast-sel-hot-stdby,graceful-conv,susp-individual\" , \"name\" : \"LACP_ACTIVE\" , \"mode\" : \"active\" , \"rn\" : \"lacplagp-LACP_ACTIVE\" , \"status\" : \"created\" }, \"children\" : [] } } LLDP_ON { \"lldpIfPol\" : { \"attributes\" : { \"dn\" : \"uni/infra/lldpIfP-LLDP_ON\" , \"name\" : \"LLDP_ON\" , \"rn\" : \"lldpIfP-LLDP_ON\" , \"status\" : \"created\" }, \"children\" : [] } } LINK-10G { \"fabricHIfPol\" : { \"attributes\" : { \"dn\" : \"uni/infra/hintfpol-LINK-10G\" , \"name\" : \"LINK-10G\" , \"speed\" : \"10G\" , \"rn\" : \"hintfpol-LINK-10G\" , \"status\" : \"created\" }, \"children\" : [] } }","title":"2.1 Interface Policies"},{"location":"restAPI/#22-vlans-domains-and-aaeps","text":"Create VLAN pool with vlan-range, associate it with Physical Domain and AAEP. Now you will change an approach. Your task is to create VLAN Pool with APIC GUI and capture json code from API Inspector . Connect to your APIC using bookmark in dCloud workstation Chrome. Navigate to Access Polices -> Pools -> VLAN. When you are there, Click on Help and Tools menu in APIC Dashboard and Click on Show API Inspector . In new window you will open API Inspector, where you can capture every API call send to the APIC server. Now you can start ADD VLAN-pool in APIC GUI. VLAN POOL NAME: vlan-pool Allocation Mode: Static Encap Blocks : 100-200, Inherit Allocation mode from parent, Role: External or On the wire encapsulations SUBMIT and open API Inspector in the another Chrome Window. Search for POST method. COPY now ALL POST URI from API Inspector to Postman request Body. You should capture data as shown in code below. h tt ps : //apic1.dcloud.cisco.com/api/node/mo/uni/infra/vlanns-[vlan-pool]-static.json payload { \"fvnsVlanInstP\" : { \"attributes\" : { \"dn\" : \"uni/infra/vlanns-[vlan-pool]-static\" , \"name\" : \"vlan-pool\" , \"allocMode\" : \"static\" , \"rn\" : \"vlanns-[vlan-pool]-static\" , \"status\" : \"created\" }, \"children\" : [ { \"fvnsEncapBlk\" : { \"attributes\" : { \"dn\" : \"uni/infra/vlanns-[vlan-pool]-static/from-[vlan-100]-to-[vlan-200]\" , \"from\" : \"vlan-100\" , \"to\" : \"vlan-200\" , \"rn\" : \"from-[vlan-100]-to-[vlan-200]\" , \"status\" : \"created\" }, \"children\" : [] } } ] } } Now it's time to build a request from data captured. You will use that request in the future for adding more vlan pools. First, copy URL and place it in Postman URL. Replace apic1.dcloud.cisco.com with our variable {{apic}}. Next replace vlanpool name in [ ] brakets. New URI looks like that: https://{{apic}}/api/node/mo/uni/infra/vlanns-[{{vlanpoolname}}]-static.json VLAN POOL { \"fvnsVlanInstP\" : { \"attributes\" : { \"dn\" : \"uni/infra/vlanns-[{{vlanpoolname}}]-static\" , \"name\" : \"{{vlanpoolname}}\" , \"allocMode\" : \"static\" , \"rn\" : \"vlanns-[{{vlanpoolname}}]-static\" , \"status\" : \"created\" }, \"children\" : [ { \"fvnsEncapBlk\" : { \"attributes\" : { \"dn\" : \"uni/infra/vlanns-[{{vlanpoolname}}]-static/from-[vlan-{{vlanstart}}]-to-[vlan-{{vlanend}}]\" , \"from\" : \"vlan-{{vlanstart}}\" , \"to\" : \"vlan-{{vlanend}}\" , \"rn\" : \"from-[vlan-{{vlanstart}}]-to-[vlan-{{vlanend}]\" , \"status\" : \"created\" }, \"children\" : [] } } ] } } Variables {{vlanpoolname}}, {{vlanstart}}, {{vlanend}} you can replace by hardcoded values in your code, or define values in Environment. Later today you will see the powerfull of variables using csv file for input data. Same approach please use in Domain and AAEP creation - Go to GUI, do Domain - PHY-DOM, associate with vlan-pool and AAEP - dcloud-AAEP. Observe API Inspector, capture the code and build those two requests in Postman. URI for AAEP Function: https://{{apic}}/api/node/mo/uni/infra.json AAEP { \"infraInfra\" : { \"attributes\" : { \"dn\" : \"uni/infra\" , \"status\" : \"modified\" }, \"children\" : [ { \"infraAttEntityP\" : { \"attributes\" : { \"dn\" : \"uni/infra/attentp-{{aaep}}\" , \"name\" : \"{{aaep}}\" , \"rn\" : \"attentp-{{aaep}}\" , \"status\" : \"created\" }, \"children\" : [] } }, { \"infraFuncP\" : { \"attributes\" : { \"dn\" : \"uni/infra/funcprof\" , \"status\" : \"modified\" }, \"children\" : [] } } ] } } Last one is Domain. Create Physical domain PHY-DOM with vlan pool associated vlan-pool and AAEP dcloud-AAEP . https://{{apic}}/api/node/mo/uni/phys-{{domain}}.json Domain { \"physDomP\" : { \"attributes\" : { \"dn\" : \"uni/phys-{{domain}}\" , \"name\" : \"{{domain}}\" , \"rn\" : \"phys-{{domain}}\" , \"status\" : \"created\" }, \"children\" : [ { \"infraRsVlanNs\" : { \"attributes\" : { \"tDn\" : \"uni/infra/vlanns-[{{vlanpoolname}}]-static\" , \"status\" : \"created\" }, \"children\" : [] } } ] } } Interesting fact about domain to AAEP association. Domain is sub-object to AAEP, you will not see AAEP under Domain definition, even you specify it in APIC GUI. In API you need to add it under AAEP. https://{{apic}}/api/node/mo/uni/infra.json Domain to AAEP association { \"infraAttEntityP\" : { \"attributes\" : { \"annotation\" : \"\" , \"descr\" : \"\" , \"dn\" : \"uni/infra/attentp-{{aaep}}\" , \"name\" : \"{{aaep}}\" , \"nameAlias\" : \"\" , \"ownerKey\" : \"\" , \"ownerTag\" : \"\" , \"userdom\" : \":all:\" }, \"children\" : [ { \"infraRsDomP\" : { \"attributes\" : { \"annotation\" : \"\" , \"tDn\" : \"uni/phys-{{domain}}\" , \"userdom\" : \":all:\" } } } ] } }","title":"2.2 VLANs, Domains and AAEPs"},{"location":"restAPI/#23-interface-policy-group","text":"Lets now create one of the Interface policy group and use all created policy. Interface Policy Group VPC { \"infraAccBndlGrp\" : { \"attributes\" : { \"dn\" : \"uni/infra/funcprof/accbundle-intpolgrp-vpc-server1\" , \"lagT\" : \"node\" , \"name\" : \"intpolgrp-vpc-server1\" , \"rn\" : \"accbundle-intpolgrp-vpc-server1\" , \"status\" : \"created\" }, \"children\" : [ { \"infraRsAttEntP\" : { \"attributes\" : { \"tDn\" : \"uni/infra/attentp-dcloud-AAEP\" , \"status\" : \"created,modified\" }, \"children\" : [] } }, { \"infraRsHIfPol\" : { \"attributes\" : { \"tnFabricHIfPolName\" : \"LINK-10G\" , \"status\" : \"created,modified\" }, \"children\" : [] } }, { \"infraRsLldpIfPol\" : { \"attributes\" : { \"tnLldpIfPolName\" : \"LLDP_ON\" , \"status\" : \"created,modified\" }, \"children\" : [] } }, { \"infraRsLacpPol\" : { \"attributes\" : { \"tnLacpLagPolName\" : \"LACP_ACTIVE\" , \"status\" : \"created,modified\" }, \"children\" : [] } } ] } } Replace intpolgrp-vpc-server1 with intpolgrp-vpc-server2 to observe how simple you can add new VPC policy group using Postman. Warning Download three files to your dcloud workstation and using post in Postman or in APIC GUI upload it to access-polices. Without them, your VPC won't instanciate and cannot be used in later stage of the lab. JSON VPC Policy . JSON interface leaf profile . JSON switch leaf profile . Ask instructor for assistance if not clear.","title":"2.3 Interface Policy Group"},{"location":"restAPI/#3-aci-tenant","text":"Upon now you created ACI AccessPolicies for your Tenant. Having JSON code for every object you will be able to unified variables across Postman Collection and then run all together using Variables from Environment or much better from CSV file. Now is time to create your tenant using JSON. It will be simple Tenant definition with one VRF and two bridge domains associated with two EPGs. Every EPG will be associated with created domain and statically binded to both VPC created, with VLAN from VLAN pool done by you perviously. Figure below shows our Logical Tenant. You need to define Tenant, VRF, Bridge-Domains, Application-Profile, EPGs with Domain association and static binding under EPG. Quite many of Requests to do in Postman. You can define all of them under one Request. Please, download a JSON file from JSON Tenant Ready . Once you have it on dcloud workstation, please create new Request under ACI Collection: NAME: CREATE TENANT-1 Method: POST URL: https://{{apic}}/api/node/mo/uni.json COPY now content from downloaded file to Body of the new request, Save and Send it. Note Please check your authentication token if still valid. Tenant-1 will be created with all structure shown in figure above.","title":"3  ACI Tenant"},{"location":"restAPI/#31-tenant-components","text":"This section contain JSON codes necessary to create Tenant objects.","title":"3.1 Tenant components"},{"location":"restAPI/#311-tenant-and-vrf","text":"Code below can be used to create new Tenant and VRF. https://{{apic}}/api/node/mo/uni.json Tenant and VRF from CSV { \"fvTenant\" : { \"attributes\" : { \"annotation\" : \"\" , \"descr\" : \"\" , \"dn\" : \"uni/tn-{{tenantname}}\" , \"name\" : \"{{tenantname}}\" }, \"children\" : [ { \"fvCtx\" : { \"attributes\" : { \"name\" : \"{{vrfname}}\" }, \"children\" : [] } } ] } }","title":"3.1.1 Tenant and VRF"},{"location":"restAPI/#312-bd-in-existing-tenant","text":"Code below can be used to create new Bridge-Domain in existing Tenant. https://{{apic}}/api/node/mo/uni.json Bridge-domain L3 from CSV { \"fvBD\" : { \"attributes\" : { \"name\" : \"{{bdname}}\" , \"dn\" : \"uni/tn-{{tenantname}}/BD-{{bdname}}\" , \"rn\" : \"BD-{{bdname}}\" , \"type\" : \"regular\" }, \"children\" : [ { \"fvSubnet\" : { \"attributes\" : { \"ip\" : \"{{ipaddress}}\" , \"ipDPLearning\" : \"enabled\" , \"scope\" : \"private\" } } }, { \"fvRsCtx\" : { \"attributes\" : { \"tnFvCtxName\" : \"{{vrfname}}\" } } } ] } } In case you need to add BD without IP address and unicast routing disabled, use code below. https://{{apic}}/api/node/mo/uni.json Bridge-domain L2 from CSV { \"fvTenant\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}\" , \"status\" : \"modified\" }, \"children\" : [ { \"fvBD\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}/BD-{{bdname}}\" , \"mac\" : \"00:22:BD:F8:19:FF\" , \"name\" : \"{{bdname}}\" , \"rn\" : \"BD-{{bdname}}\" , \"arpFlood\" : \"yes\" , \"ipLearning\" : \"no\" , \"unicastRoute\" : \"no\" , \"unkMacUcastAct\" : \"flood\" , \"type\" : \"regular\" , \"unkMcastAct\" : \"flood\" , \"status\" : \"created\" }, \"children\" : [ { \"fvRsCtx\" : { \"attributes\" : { \"tnFvCtxName\" : \"{{vrfname}}\" , \"status\" : \"created,modified\" }, \"children\" : [] } } ] } } ] } } Highlighted rows are specific for L2 Bridge-Domain.","title":"3.1.2 BD in existing tenant"},{"location":"restAPI/#313-application-profile","text":"Component which contain all EPGs in the Tenant. https://{{apic}}/api/node/mo/uni.json Application Profile from CSV { \"fvAp\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}/ap-{{ap}}\" , \"name\" : \"{{ap}}\" , \"rn\" : \"ap-{{ap}}\" , \"status\" : \"created\" } } }","title":"3.1.3 Application Profile"},{"location":"restAPI/#314-epgs-in-existing-tenantappprofiles-and-associated-with-domain","text":"https://{{apic}}/api/node/mo/uni.json Create EPG under existing application profile from CSV { \"fvAEPg\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}/ap-{{ap}}/epg-{{epgname}}\" , \"name\" : \"{{epgname}}\" , \"rn\" : \"epg-{{epgname}}\" , \"status\" : \"created\" }, \"children\" : [ { \"fvRsBd\" : { \"attributes\" : { \"tnFvBDname\" : \"{{bdname}}\" , \"status\" : \"created,modified\" }, \"children\" : [] } }, { \"fvRsDomAtt\" : { \"attributes\" : { \"tDn\" : \"uni/phys-{{domain}}\" , \"status\" : \"created\" }, \"children\" : [] } } ] } }","title":"3.1.4 EPGs in existing tenant/appprofiles and associated with domain"},{"location":"restAPI/#4-use-csv-file-for-input-data","text":"You are about to re-use work you did during all excercises. Untill now you run every request for static data, only once. Adding 100 Bridge-Domains, by changing \"Variables\" within the code would be problematic. Postman is giving an option to use external variable file to execute created requests in the collection. By running five requests from your Collection and using CSV file from location FILE CSV TO DOWNLOAD you will add new Tenant with one VRF, Applicatioin Profile, 22 Bridge-Domains and 22 EPGs associated to Physical Domain created before. All of this will be done in one Postman collection Run.","title":"4 Use CSV file for input data"},{"location":"restAPI/#41-run-collection-requests","text":"Follow the instruction from the figure below: 1) Click on dots icon at your ACI dCloud collection folder 2) Select Run Collection You will be moved to new dashboard, where you will select Requests and input file. Deselect All, and then Select only needed requests: When you select Requests, go to Data \"Select File\" and choice downloaded CSV file - tenant-create.csv . Confirm that your file is selected as on the figure above. You can confirm amount of iterations in the top field Iterations . Note Do not forget to authenticate your Postman session using ACI Login request. I recommend to do it out of collection Run -- beforehand. Click Run ACI dCloud Blue button.","title":"4.1 Run Collection requests"},{"location":"restAPI/#42-verification-after-runing-the-collection","text":"When you successfully execute your collection, in Summary at the screen you will see results of every Iteration. When Request was accepted by APIC, result will be 200 OK . Every Iteration contain all five seleceted requests. You probably noticed that one of them is getting 400 Bad Request - \"Bridge Domain L3 from CSV\". Reason for that, is missing children attribute fvSubnet for all L2 bridge-domain in our CSV file. Now connect to APIC GUI and verify if tenant exists, check dashboard status. You successfully Created New Tenant with 22 EPGs and 22 BDs. Please, edit CSV file in notepad++ editor at your dCloud workstation, replace Tenantname with new and run your Postman Collection once again. Do you understand now power of simple automation?","title":"4.2 Verification after runing the collection"},{"location":"security_policies/","text":"Applying Kubernetes Network Policies to Secure the Application A network policy is a specification of how groups of pods are allowed to communicate with each other and other network endpoints. NetworkPolicy resources use labels to select pods and define rules which specify what traffic is allowed to the selected pods. By now you should have your Hybrid Cloud IoT Application working end to end. Let's apply some Kubernetes Network Policies to allow traffic from the frontend app to port '5050' only (REST API AGENT container accepts HTTP requests on port '5050'). 1. Apply Deny All Network Policy: Following Kubernetes Network Policy yaml definition would block all the traffic coming towards REST API Agent - apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all-rest-api-agent spec: podSelector: matchLabels: app: iot-backend-rest-api-agent tier: rest-api-agent policyTypes: - Ingress 1.0: Prerequisite - set kubectl context to on-prem-1 and make sure that your default namespace is set to your student ID. kubectl config use-context admin@on-prem-backend kubectl config get-contexts 1.1: Execute the following command on Kubernetes master node to apply this Network Policy on your REST API Agent - kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/Network_Policies/deny_all_rest_api_agent.yaml 1.2: Now try to referesh your Frontend App webpage. It should stop working. 2. Apply Permit Port 5111 Network Policy: Following Kubernetes Network Policy yaml definition would allow the traffic on port 5111 to REST API Agent - apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: permit-port-5111-rest-api-agent spec: podSelector: matchLabels: app: iot-backend-rest-api-agent tier: rest-api-agent policyTypes: - Ingress ingress: - from: [] ports: - protocol: TCP port: 5111 2.1: Execute the following command on Kubernetes master node to apply this Network Policy on your REST API Agent - kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/Network_Policies/permit_port_5111_rest_api_agent.yaml 2.2: Now try to referesh your Frontend App webpage. Does it work? Why? 3. Apply Permit Port 5050 Network Policy: Following Kubernetes Network Policy yaml definition would allow the traffic on port 5050 to REST API Agent - apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: permit-port-5050-rest-api-agent spec: podSelector: matchLabels: app: iot-backend-rest-api-agent tier: rest-api-agent policyTypes: - Ingress ingress: - from: [] ports: - protocol: TCP port: 5050 3.1: Execute the following command on Kubernetes master node to apply this Network Policy on your REST API Agent - kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/Network_Policies/permit_port_5050_rest_api_agent.yaml 3.2: Now try to referesh your Frontend App webpage. Does it work? Why? Note: Other command related to Network Policy that you may use - Display Kubernetes Network Policies - kubectl get NetworkPolicies Display Network Policy Details - kubectl describe NetworkPolicy <Network Policy Name> Delete Network Policy - kubectl delete NetworkPolicy <Network Policy Name>","title":"Applying Kubernetes Network Policies to Secure the Application"},{"location":"security_policies/#applying-kubernetes-network-policies-to-secure-the-application","text":"A network policy is a specification of how groups of pods are allowed to communicate with each other and other network endpoints. NetworkPolicy resources use labels to select pods and define rules which specify what traffic is allowed to the selected pods. By now you should have your Hybrid Cloud IoT Application working end to end. Let's apply some Kubernetes Network Policies to allow traffic from the frontend app to port '5050' only (REST API AGENT container accepts HTTP requests on port '5050').","title":"Applying Kubernetes Network Policies to Secure the Application"},{"location":"security_policies/#1-apply-deny-all-network-policy","text":"Following Kubernetes Network Policy yaml definition would block all the traffic coming towards REST API Agent - apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all-rest-api-agent spec: podSelector: matchLabels: app: iot-backend-rest-api-agent tier: rest-api-agent policyTypes: - Ingress 1.0: Prerequisite - set kubectl context to on-prem-1 and make sure that your default namespace is set to your student ID. kubectl config use-context admin@on-prem-backend kubectl config get-contexts 1.1: Execute the following command on Kubernetes master node to apply this Network Policy on your REST API Agent - kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/Network_Policies/deny_all_rest_api_agent.yaml 1.2: Now try to referesh your Frontend App webpage. It should stop working.","title":"1. Apply Deny All Network Policy:"},{"location":"security_policies/#2-apply-permit-port-5111-network-policy","text":"Following Kubernetes Network Policy yaml definition would allow the traffic on port 5111 to REST API Agent - apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: permit-port-5111-rest-api-agent spec: podSelector: matchLabels: app: iot-backend-rest-api-agent tier: rest-api-agent policyTypes: - Ingress ingress: - from: [] ports: - protocol: TCP port: 5111 2.1: Execute the following command on Kubernetes master node to apply this Network Policy on your REST API Agent - kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/Network_Policies/permit_port_5111_rest_api_agent.yaml 2.2: Now try to referesh your Frontend App webpage. Does it work? Why?","title":"2. Apply Permit Port 5111 Network Policy:"},{"location":"security_policies/#3-apply-permit-port-5050-network-policy","text":"Following Kubernetes Network Policy yaml definition would allow the traffic on port 5050 to REST API Agent - apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: permit-port-5050-rest-api-agent spec: podSelector: matchLabels: app: iot-backend-rest-api-agent tier: rest-api-agent policyTypes: - Ingress ingress: - from: [] ports: - protocol: TCP port: 5050 3.1: Execute the following command on Kubernetes master node to apply this Network Policy on your REST API Agent - kubectl create -f https://raw.githubusercontent.com/marcinduma/HOLCLD-2101/main/Kubernetes/Backend/Network_Policies/permit_port_5050_rest_api_agent.yaml 3.2: Now try to referesh your Frontend App webpage. Does it work? Why? Note: Other command related to Network Policy that you may use - Display Kubernetes Network Policies - kubectl get NetworkPolicies Display Network Policy Details - kubectl describe NetworkPolicy <Network Policy Name> Delete Network Policy - kubectl delete NetworkPolicy <Network Policy Name>","title":"3. Apply Permit Port 5050 Network Policy:"},{"location":"terraform-basis/","text":"Basic use-cases with Terraform In this lab the focus will be on using Terraform as a way to provide operational rigor to your network device configurations. You will learn how to install and use Terraform on Cisco network solutions like ACI. 1. Install Visual Studio Code Terraform uses structured files that require editing. To assist you in this process the best tool is some editor that understands the Terraform semantics. For this lab you will be using a web based integrated development environment that uses the code that is inside Microsofts Visual Studio Code. When Microsoft wrote Visual Studio Code it built the IDE on top of a platform called electron. This allowed for cross platform development and is based on javascript. Visual Studio Code can be installed on variety of operating systems, please download the package suitable for your environment and follow the installation process in the wizard: https://code.visualstudio.com/ Visual Studio Code has three panes that you will be using: The left pane with the files The right pane with the file contents The bottom pane will be leveraging the Terminal to issue commands 1.1 Open a new terminal in the IDE To get started, first you have to open a terminal windows in the IDE to access the underlying operating system and work the lab. On the menu bar click the Terminal tab to open New Terminal Terminal will open at the bottom of the screen. This will be the area that you will execute all the terraform commands after making changes to terraform files. 2 Create the working directory Terraform uses directory structures as a way to organize its automation structure. This is due to the fact that Terraform treats everything in a directory as a unit and reads all the .tf files before execution. The first step is to create a directory called ACI for the terraform files. Using the IDE you can create folders. This directory will live under the ACI folder. There are various ways to create these in visual studio code: using the contextual menu (1a and 1b) or using the icons (2): In that directory create the first terraform file called main.tf using the icon: 3. Configure ACI Provider One of the first things we should do before writing configuration for ACI is to add the Terraform Provider for ACI. The definition will be placed in the main.tf file and we will use the username/password construct. It is also possible and recommended to use certificates based authentication, but for this lab we are using the simpler method. ACI Provider documentation can be found on the official Terraform Registry website: https://registry.terraform.io/providers/CiscoDevNet/aci/latest/docs In also includes instruction on how to use this provider. After clicking on \"Use provider\" small section with code will appear. The code that we will use in this lab contains IP address of APIC in the lab and default admin user/password. It should be copied to the main.tf file: terraform { required_providers { aci = { source = \"CiscoDevNet/aci\" version = \"2.5.2\" } } } provider \"aci\" { # Cisco ACI user name username = \"admin\" password = \"C1sco12345\" url = \"https://198.18.133.200\" insecure = true } 3.1 Run Terraform Init Once the provider is configured and our APIC IP and credentials are present in the file, we can proceed with the first step of Terraform workflow and initialize the directory to include proper terraform provider modules needed for execution. In the terminal window on the bottom pane of Visual Studio make sure you are in the correct directory (your ACI folder) and then execute terraform init . The output should show that the provider plugin hsa been downlaoded: juchowan@JUCHOWAN-M-D2P2 ACI-simulator % terraform init Initializing the backend... Initializing provider plugins... - Finding ciscodevnet/aci versions matching \"2.5.2\"... - Installing ciscodevnet/aci v2.5.2... - Installed ciscodevnet/aci v2.5.2 (signed by a HashiCorp partner, key ID 433649E2C56309DE) Partner and community providers are signed by their developers. If you'd like to know more about provider signing, you can read about it here: https://www.terraform.io/docs/cli/plugins/signing.html Terraform has created a lock file .terraform.lock.hcl to record the provider selections it made above. Include this file in your version control repository so that Terraform can guarantee to make the same selections by default when you run \"terraform init\" in the future. Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. Next you can try running terraform plan , but since our main.tf file has no resource confguration, you will see that there is no change needed: juchowan@JUCHOWAN-M-D2P2 ACI-simulator % terraform plan No changes. Your infrastructure matches the configuration. Terraform has compared your real infrastructure against your configuration and found no differences, so no changes are needed. 3.2 (Optional) Switch registration At this point leaf and spine switches should be registered in the lab using Postman the day before. Registration of switch is possible to be done using resource aci_fabric_node_member . Documentation of this resource is present in the official Terraform Registry documentation . In the Example Usage section you can find example code to register new node In the Argument Reference you will see possible arguments/parameters that can be used for switch registration with indication if argument is required or optional and additional information about it In the Attribute Reference you will see what is the attribute that this resource exports, and in case of ACI resources it will be always id set to the DN of the VLAN Pool. Importing describes how to import existing object to the terraform resource For reference the following resource configuration in Terraform would register new switch. If the switches are already registered, this task can be skipped resource \"aci_fabric_node_member\" \"Leaf-101\" { name = \"Leaf-101\" serial = \"TEP-1-101\" node_id = \"101\" pod_id = \"1\" role = \"leaf\" } resource \"aci_fabric_node_member\" \"Leaf-102\" { name = \"Leaf-102\" serial = \"TEP-1-102\" node_id = \"102\" pod_id = \"1\" role = \"leaf\" } resource \"aci_fabric_node_member\" \"Spine-103\" { name = \"Spine-103\" serial = \"TEP-1-103\" node_id = \"103\" pod_id = \"1\" role = \"spine\" } resource \"aci_fabric_node_member\" \"Spine-104\" { name = \"Spine-104\" serial = \"TEP-1-104\" node_id = \"104\" pod_id = \"1\" role = \"spine\" } 4 ACI Access Policies Every ACI Fabric configuration starts with Access Polices. You need to define ACI objects which represent single physical port configurations. To remind relationships between objects, please check following figure. Usually Interface Polices are configured once and re-used like LLDP, LACP, CDP, etc. In our LAB some of the policies already exist from the day before and Postman configuration, so different names should be used for Terraform lab to create new objects. Following sections will help you to prepare your API requests. For reference full configuration of access-policies can be found here: Terraform Tenant Ready . 4.1 Interface Policies You will configure LACP Policy, LLDP and speed policies. Let's separate our terraform files and create separate config file for out interface policies. Under your folder in Visual Studio create new file calles interface-policies.tf : Copy each of the Policy resource definition to the new interface-policies.tf file LACP_ACTIVE resource \"aci_lacp_policy\" \"LACP_ACTIVE\" { na me = \"LACP_ACTIVE\" c trl = [ \"susp-individual\" , \"fast-sel-hot-stdby\" , \"graceful-conv\" ] mode = \"active\" } LLDP_ON resource \"aci_lldp_interface_policy\" \"LLDP_ON\" { na me = \"LLDP_ON\" admi n _rx_s t = \"enabled\" admi n _ t x_s t = \"enabled\" } LINK-10G resource \"aci_fabric_if_pol\" \"LINK-10G\" { na me = \"LINK-10G\" au t o_ ne g = \"on\" speed = \"10G\" } The documentation for each of the resources can be found in Terraform Registry: aci_lacp_policy resource aci_lldp_interface_policy resource aci_fabric_if_pol resource 4.2 Run Terraform Plan & Apply At this point we have our first three resources in the configuration so it's time to deploy them on our APIC. To do so we will follow the Terraform workload init -> plan -> apply. In previous step, after configuring ACI provider you should have already done terraform init , so it is not needed to run it again now, as it would not bring any change. Our first step to deploy config is to run terraform plan command in the terminal window. The output will show us what changes Terraform needs to do on the fabric, to bring it to the required configuration. The output should look similar to the following: juchowan@JUCHOWAN-M-D2P2 ACI-simulator % terraform plan aci_fabric_node_member.Leaf-102: Refreshing state... [id=uni/controller/nodeidentpol/nodep-TEP-1-102] aci_fabric_node_member.Leaf-101: Refreshing state... [id=uni/controller/nodeidentpol/nodep-TEP-1-101] aci_fabric_node_member.Spine-103: Refreshing state... [id=uni/controller/nodeidentpol/nodep-TEP-1-103] aci_fabric_node_member.Spine-104: Refreshing state... [id=uni/controller/nodeidentpol/nodep-TEP-1-104] Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # aci_fabric_if_pol.auto-10G will be created + resource \"aci_fabric_if_pol\" \"auto-10G\" { + annotation = \"orchestrator:terraform\" + auto_neg = \"on\" + description = (known after apply) + fec_mode = (known after apply) + id = (known after apply) + link_debounce = (known after apply) + name = \"LINK-10G\" + name_alias = (known after apply) + speed = \"10G\" } # aci_lldp_interface_policy.LLDP_ON will be created + resource \"aci_lldp_interface_policy\" \"LLDP_ON\" { + admin_rx_st = \"enabled\" + admin_tx_st = \"enabled\" + annotation = \"orchestrator:terraform\" + description = (known after apply) + id = (known after apply) + name = \"LLDP_ON\" + name_alias = (known after apply) } # aci_lacp_policy.LACP_ACTIVE will be created + resource \"aci_lacp_policy\" \"LACP_ACTIVE\" { + annotation = \"orchestrator:terraform\" + ctrl = [ + \"susp-individual\", + \"fast-sel-hot-stdby\", + \"graceful-conv\", ] + description = \"done by terraform\" + id = (known after apply) + max_links = (known after apply) + min_links = (known after apply) + mode = \"active\" + name = \"LACP_ACTIVE\" + name_alias = (known after apply) } Plan: 3 to add, 0 to change, 0 to destroy. \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Note: You didn't use the -out option to save this plan, so Terraform can't guarantee to take exactly these actions if you run \"terraform apply\" now. In the plan you can see all the plus signs beside the elements that are going to be added to the fabric. Terraform read the fabric state and compared with required configuration, and then decided which elements need to be added. At this point no configuration has been added yet to the fabric, Terraform only lists the changes it is planning to do. As the Note says at the bottom, this plan was not saved to a file, but only displayed in the console. If you change command to terraform plan -out interface-policies.plan you will see that a file was generated with the contents of the plan. Now you can perform the terraform apply command. This command will run plan again, ask for approval and then it will go into the fabric and perform these actions. <plan omitted> Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes aci_lldp_interface_policy.LLDP_ON: Creating... aci_fabric_if_pol.auto-10G: Creating... aci_lacp_policy.LACP_ACTIVE: Creating... aci_lldp_interface_policy.LLDP_ON: Creation complete after 0s [id=uni/infra/lldpIfP-LLDP_ON] aci_fabric_if_pol.auto-10G: Creation complete after 0s [id=uni/infra/hintfpol-LINK-10G] aci_lacp_policy.LACP_ACTIVE: Creation complete after 0s [id=uni/infra/lacplagp-LACP_ACTIVE] Apply complete! Resources: 3 added, 0 changed, 0 destroyed. If you run plan command again now, you would see the output is different because now it will only show the changes it plans to do over the previous push. Since there are no changes in the config yet, the plan will not show anything new to be added: juchowan@JUCHOWAN-M-D2P2 ACI-simulator % terraform plan aci_fabric_node_member.Spine-103: Refreshing state... [id=uni/controller/nodeidentpol/nodep-TEP-1-103] aci_fabric_node_member.Leaf-102: Refreshing state... [id=uni/controller/nodeidentpol/nodep-TEP-1-102] aci_lacp_policy.LACP_ACTIVE: Refreshing state... [id=uni/infra/lacplagp-LACP_ACTIVE] aci_fabric_if_pol.auto-10G: Refreshing state... [id=uni/infra/hintfpol-LINK-10G] aci_fabric_node_member.Spine-104: Refreshing state... [id=uni/controller/nodeidentpol/nodep-TEP-1-104] aci_lldp_interface_policy.LLDP_ON: Refreshing state... [id=uni/infra/lldpIfP-LLDP_ON] aci_fabric_node_member.Leaf-101: Refreshing state... [id=uni/controller/nodeidentpol/nodep-TEP-1-101] No changes. Your infrastructure matches the configuration. 4.3 VLANs, Domains and AAEPs Create VLAN pool with vlan-range, associate it with Physical Domain and AAEP. Let's put these policies in a separate, new file called access-policies.tf We will create two VLAN pools, one for L2 Physical Domain, and second one for L3 Routed Domain. In the same way we will create two AEP, one for L2 connections and second one for L3 connections. Each AEP will map one domain: VLAN_POOL_L2 <-- PHYSDOM_1 <-- AEP_L2 VLAN_POOL_L3 <-- EXTRTDOM_1 <-- AEP_L3 For VLAN pool there are two resources that need to be created: VLAN Pool and VLAN Range. In this case VLAN Range object is a child of VLAN Pool object, which means that the Range resource needs to have a reference to the VLAN Pool resource. This reference can be done in terraform by using the resource name followed by the attribute, in this case id . resource \"aci_vlan_pool\" \"VLAN_POOL_L2\" { name = \"VLAN_POOL_L2\" alloc_mode = \"static\" } resource \"aci_ranges\" \"RANGE_100-200\" { vlan_pool_dn = aci_vlan_pool.VLAN_POOL_L2.id from = \"vlan-100\" to = \"vlan-200\" alloc_mode = \"inherit\" } resource \"aci_vlan_pool\" \"VLAN_POOL_L3\" { name = \"VLAN_POOL_L3\" alloc_mode = \"static\" } resource \"aci_ranges\" \"RANGE_201-300\" { vlan_pool_dn = aci_vlan_pool.VLAN_POOL_L3.id from = \"vlan-201\" to = \"vlan-300\" alloc_mode = \"inherit\" } Two domains, physical and routed, will be created as two resources, and each of them needs to have a reference to a VLAN Pool. This reference will be done by special attribute relation_infra_rs_vlan_ns , by referencing the resource name and id attribute: resource \"aci_physical_domain\" \"PHYSDOM_1\" { name = \"PHYSDOM_1\" relation_infra_rs_vlan_ns = aci_vlan_pool.VLAN_POOL_L2.id } resource \"aci_l3_domain_profile\" \"EXTRTDOM_1\" { name = \"EXTRTDOM_1\" relation_infra_rs_vlan_ns = aci_vlan_pool.VLAN_POOL_L3.id } Two AAEP will be created, one for L2 connections and another one for L3 connections. Between AAEP and Domains there is also relationship. AEP needs to be related to a domain. This relation can be done in two ways: separate resource aci_aaep_to_domain which is shown in the example below for AEP_L2 and PHYSDOM_1 relation. This resource needs to reference id values of both AEP and Physical Domain argument within aci_attachable_access_entity_profile called relation_infra_rs_dom_p which is shown in the example below for AEP_L3 and EXTRTDOM_1 relation. This argument needs to reference id values of both AEP and Routed Domain, but needs to be embedded in [] as the documentation says it's Type -[Set of String] resource \"aci_attachable_access_entity_profile\" \"AEP_L2\" { description = \"AAEP for L2 connections\" name = \"AEP_L2\" } resource \"aci_aaep_to_domain\" \"AEP_L2_to_PHYSDOM_1\" { attachable_access_entity_profile_dn = aci_attachable_access_entity_profile.AEP_L2.id domain_dn = aci_physical_domain.PHYSDOM_1.id } resource \"aci_attachable_access_entity_profile\" \"AEP_L3\" { description = \"AAEP for L3 connections\" name = \"AEP_L3\" relation_infra_rs_dom_p = [aci_l3_domain_profile.EXTRTDOM_1.id] } You can plan and apply to see the result on ACI. Documentation of resources: aci_vlan_pool resource aci_ranges resource aci_physical_domain resource aci_l3_domain_profile resource aci_attachable_access_entity_profile resource aci_aaep_to_domain resource 4.4 Interface Policy group Let's now create two Interface Policy Groups, one for server (L2) and another one for router (L3) and use all created policies: IntPolGrp_VPC_server1 VPC policy with AEP_L2 IntPolGrp_Router1 access port policy with AEP_L3 Those interface policy groups will have relation to all the access policies we created before by referencing the id of those policies: resource \"aci_leaf_access_bundle_policy_group\" \"IntPolGrp_VPC_server1\" { name = \"IntPolGrp_VPC_server1\" lag_t = \"node\" relation_infra_rs_lldp_if_pol = aci_lldp_interface_policy.LLDP_ON.id relation_infra_rs_lacp_pol = aci_lacp_policy.LACP_ACTIVE.id relation_infra_rs_h_if_pol = aci_fabric_if_pol.LINK-10G.id relation_infra_rs_att_ent_p = aci_attachable_access_entity_profile.AEP_L2.id } resource \"aci_leaf_access_bundle_policy_group\" \"IntPolGrp_Router1\" { name = \"IntPolGrp_Router1\" lag_t = \"node\" relation_infra_rs_lldp_if_pol = aci_lldp_interface_policy.LLDP_ON.id relation_infra_rs_lacp_pol = aci_lacp_policy.LACP_ACTIVE.id relation_infra_rs_h_if_pol = aci_fabric_if_pol.LINK-10G.id relation_infra_rs_att_ent_p = aci_attachable_access_entity_profile.AEP_L3.id } You can plan and apply to see the result on ACI. Documentation of resources: aci_leaf_access_bundle_policy_group resource aci_leaf_access_port_policy_group resource 4.5 Switch & Interface Profiles Let's assign our created interface policy groups to the newly created switch and interface profiles. We will create: switch profile interface profile IntProf-101-102 with two interface selectors eth1/1 connecting server eth1/2 connecting router First we create interface profile within this profile two interface selectors. Interface selectors need to have reference to the Interface Profile ( IntProf-101-102 ) and relation to the interface policy groups created for our end devices: IntPolGrp_VPC_server1 IntPolGrp_Router1 resource \"aci_leaf_interface_profile\" \"IntProf-101-102\" { name = \"IntProf-101-102\" } resource \"aci_access_port_selector\" \"leaf-101-102-eth-1\" { leaf_interface_profile_dn = aci_leaf_interface_profile.IntProf-101-102.id name = \"leaf-101-102-eth-1\" access_port_selector_type = \"range\" relation_infra_rs_acc_base_grp = aci_leaf_access_bundle_policy_group.IntPolGrp_VPC_server1.id } resource \"aci_access_port_block\" \"leaf-101-102-eth-1-block\" { access_port_selector_dn = aci_access_port_selector.leaf-101-102-eth-1.id name = \"leaf-101-102-eth-1-block\" from_card = \"1\" from_port = \"1\" to_card = \"1\" to_port = \"1\" } resource \"aci_access_port_selector\" \"leaf-101-102-eth-2\" { leaf_interface_profile_dn = aci_leaf_interface_profile.IntProf-101-102.id name = \"leaf-101-102-eth-2\" access_port_selector_type = \"range\" relation_infra_rs_acc_base_grp = aci_leaf_access_bundle_policy_group.IntPolGrp_Router1.id } resource \"aci_access_port_block\" \"leaf-101-102-eth-2-block\" { access_port_selector_dn = aci_access_port_selector.leaf-101-102-eth-2.id name = \"leaf-101-102-eth-2-block\" from_card = \"1\" from_port = \"2\" to_card = \"1\" to_port = \"2\" } Next we have our switch profile configuration which needs to have a reference to the leaf interface profile ( IntProf-101-102 ) as well as mark the nodes which will be used in this switch profile (nodes 101 & 102) resource \"aci_leaf_profile\" \"SwProf-101-102\" { name = \"SwProf-101-102\" relation_infra_rs_acc_port_p = [aci_leaf_interface_profile.IntProf-101-102.id] } resource \"aci_leaf_selector\" \"Sel-101-102\" { leaf_profile_dn = aci_leaf_profile.SwProf-101-102.id name = \"Sel-101-102\" switch_association_type = \"range\" } resource \"aci_node_block\" \"Sel-101-102-Block\" { switch_association_dn = aci_leaf_selector.Sel-101-102.id name = \"Sel-101-102-Block\" from_ = \"101\" to_ = \"102\" } You can plan and apply to see the result on ACI. 4.6 VPC Protection Groups For a pair of switches that are expected to work as VPC it is required to configure a VPC protection group. resource \"aci_vpc_explicit_protection_group\" \"vpc-101-102\" { name = \"vpc-101-102\" switch1 = \"101\" switch2 = \"102\" vpc_explicit_protection_group_id = \"101\" } 5 ACI Tenant Upon now you created ACI AccessPolicies for your Tenant. Now is time to create your tenant using Terraform. It will be simple Tenant definition with one VRF, one bridge domains associated with one EPG and one L3out. EPG will be associated with L2 domain and statically binded to VPC created, with VLAN from L2 VLAN pool done by you perviously. The L3out will be associated to L3 domain and router interface, with VLAN from L3 VLAN pool. Figure below shows our Logical Tenant. You need to define Tenant, VRF, Bridge-Domain, Application-Profile, EPG with Domain association, static binding under EPG and L3out. Quite many of resources to do in Terraform. We will define all of them in a single, new config file called dcloud-tenant-1.tf Please, download a terraform file from Terraform Tenant Ready . 5.1 Tenant components This section contains Terraform codes necessary to create Tenant objects 5.1.1 Tenant and VRF Following code can be used to create new Tenant and VRFs. The VRF resource needs to reference its parent object - tenant id. resource \"aci_tenant\" \"dcloud-tenant-1\" { name = \"dcloud-tenant-1\" } resource \"aci_vrf\" \"vrf1\" { tenant_dn = aci_tenant.dcloud-tenant-1.id name = \"vrf1\" } 5.1.2 BD in existing tenant Following code can be used to create new Bridge Domain in existing Tenant. Bridge Domain needs to have a reference to parent objects: tenant ID and VRF ID. There is also a child object of bridge domain which is a Subnet with reference to bridge domain ID. resource \"aci_bridge_domain\" \"bd-network-1\" { tenant_dn = aci_tenant.dcloud-tenant-1.id relation_fv_rs_ctx = aci_vrf.vrf1.id name = \"bd-network-1\" } resource \"aci_subnet\" \"subnet-1\" { parent_dn = aci_bridge_domain.bd-network-1.id ip = \"10.0.0.1/24\" scope = [\"public\"] } You can plan and apply to see the result on ACI. 5.1.3 Application Profile with EPG and static bindings Following code can be used to create new Application Profile with EPG. Application Profile needs a reference to parent object tenant ID, while EPG needs reference to AP ID ad Bridge Domain ID. The next two resources reflect the physical domain PHYSDOM_1 attached to that EPG, and one static binding towards our server1 with encapsulation of vlan 100. resource \"aci_application_profile\" \"ap1\" { tenant_dn = aci_tenant.dcloud-tenant-1.id name = \"ap1\" } resource \"aci_application_epg\" \"epg1\" { application_profile_dn = aci_application_profile.ap1.id name = \"epg1\" relation_fv_rs_bd = aci_bridge_domain.bd-network-1.id } resource \"aci_epg_to_domain\" \"epg1-to-physdom_1\" { application_epg_dn = aci_application_epg.epg1.id tdn = aci_physical_domain.PHYSDOM_1.id } resource \"aci_epg_to_static_path\" \"static-binding-1\" { application_epg_dn = aci_application_epg.epg1.id tdn = \"topology/pod-1/protpaths-101-102/pathep-[IntPolGrp_VPC_server1]\" encap = \"vlan-100\" mode = \"regular\" } You can plan and apply to see the result on ACI. 5.1.4 L3Out Following code can be used to create new L3out. As you can see there are plenty of resources which reflect node profile and node mapping, interface profile and path used for it, external EPG with subnet. resource \"aci_l3_outside\" \"l3out-1\" { tenant_dn = aci_tenant.dcloud-tenant-1.id name = \"l3out-1\" relation_l3ext_rs_l3_dom_att = aci_l3_domain_profile.EXTRTDOM_1.id relation_l3ext_rs_ectx = aci_vrf.vrf1.id } resource \"aci_logical_node_profile\" \"np-101-102\" { l3_outside_dn = aci_l3_outside.l3out-1.id name = \"np-101-102\" } resource \"aci_logical_node_to_fabric_node\" \"np-101-102-to-node-101\" { logical_node_profile_dn = aci_logical_node_profile.np-101-102.id tdn = \"topology/pod-1/node-101\" rtr_id = \"1.1.1.1\" } resource \"aci_logical_node_to_fabric_node\" \"np-101-102-to-node-102\" { logical_node_profile_dn = aci_logical_node_profile.np-101-102.id tdn = \"topology/pod-1/node-102\" rtr_id = \"1.1.1.2\" } resource \"aci_logical_interface_profile\" \"ip-101-102\" { logical_node_profile_dn = aci_logical_node_profile.np-101-102.id name = \"ip-101-102\" } resource \"aci_l3out_path_attachment\" \"l3ip-path\" { logical_interface_profile_dn = aci_logical_interface_profile.ip-101-102.id target_dn = \"topology/pod-1/protpaths-101-102/pathep-[IntPolGrp_Router1]\" if_inst_t = \"ext-svi\" addr = \"10.1.1.1/24\" encap = \"vlan-201\" encap_scope = \"ctx\" mode = \"regular\" mtu = \"inherit\" } resource \"aci_external_network_instance_profile\" \"extepg\" { l3_outside_dn = aci_l3_outside.l3out-1.id name = \"extepg\" } resource \"aci_l3_ext_subnet\" \"extepg-subnet\" { external_network_instance_profile_dn = aci_external_network_instance_profile.extepg.id ip = \"0.0.0.0/0\" scope = [\"import-security\",\"export-rtctrl\"] } You can plan and apply to see the result on ACI. Documentation of resources: aci_tenant resource aci_bridge_domain aci_subnet resource aci_application_profile aci_application_epg aci_epg_to_domain aci_l3_outside","title":"Basic use-cases with Terraform"},{"location":"terraform-basis/#basic-use-cases-with-terraform","text":"In this lab the focus will be on using Terraform as a way to provide operational rigor to your network device configurations. You will learn how to install and use Terraform on Cisco network solutions like ACI.","title":"Basic use-cases with Terraform"},{"location":"terraform-basis/#1-install-visual-studio-code","text":"Terraform uses structured files that require editing. To assist you in this process the best tool is some editor that understands the Terraform semantics. For this lab you will be using a web based integrated development environment that uses the code that is inside Microsofts Visual Studio Code. When Microsoft wrote Visual Studio Code it built the IDE on top of a platform called electron. This allowed for cross platform development and is based on javascript. Visual Studio Code can be installed on variety of operating systems, please download the package suitable for your environment and follow the installation process in the wizard: https://code.visualstudio.com/ Visual Studio Code has three panes that you will be using: The left pane with the files The right pane with the file contents The bottom pane will be leveraging the Terminal to issue commands","title":"1. Install Visual Studio Code"},{"location":"terraform-basis/#11-open-a-new-terminal-in-the-ide","text":"To get started, first you have to open a terminal windows in the IDE to access the underlying operating system and work the lab. On the menu bar click the Terminal tab to open New Terminal Terminal will open at the bottom of the screen. This will be the area that you will execute all the terraform commands after making changes to terraform files.","title":"1.1 Open a new terminal in the IDE"},{"location":"terraform-basis/#2-create-the-working-directory","text":"Terraform uses directory structures as a way to organize its automation structure. This is due to the fact that Terraform treats everything in a directory as a unit and reads all the .tf files before execution. The first step is to create a directory called ACI for the terraform files. Using the IDE you can create folders. This directory will live under the ACI folder. There are various ways to create these in visual studio code: using the contextual menu (1a and 1b) or using the icons (2): In that directory create the first terraform file called main.tf using the icon:","title":"2 Create the working directory"},{"location":"terraform-basis/#3-configure-aci-provider","text":"One of the first things we should do before writing configuration for ACI is to add the Terraform Provider for ACI. The definition will be placed in the main.tf file and we will use the username/password construct. It is also possible and recommended to use certificates based authentication, but for this lab we are using the simpler method. ACI Provider documentation can be found on the official Terraform Registry website: https://registry.terraform.io/providers/CiscoDevNet/aci/latest/docs In also includes instruction on how to use this provider. After clicking on \"Use provider\" small section with code will appear. The code that we will use in this lab contains IP address of APIC in the lab and default admin user/password. It should be copied to the main.tf file: terraform { required_providers { aci = { source = \"CiscoDevNet/aci\" version = \"2.5.2\" } } } provider \"aci\" { # Cisco ACI user name username = \"admin\" password = \"C1sco12345\" url = \"https://198.18.133.200\" insecure = true }","title":"3. Configure ACI Provider"},{"location":"terraform-basis/#31-run-terraform-init","text":"Once the provider is configured and our APIC IP and credentials are present in the file, we can proceed with the first step of Terraform workflow and initialize the directory to include proper terraform provider modules needed for execution. In the terminal window on the bottom pane of Visual Studio make sure you are in the correct directory (your ACI folder) and then execute terraform init . The output should show that the provider plugin hsa been downlaoded: juchowan@JUCHOWAN-M-D2P2 ACI-simulator % terraform init Initializing the backend... Initializing provider plugins... - Finding ciscodevnet/aci versions matching \"2.5.2\"... - Installing ciscodevnet/aci v2.5.2... - Installed ciscodevnet/aci v2.5.2 (signed by a HashiCorp partner, key ID 433649E2C56309DE) Partner and community providers are signed by their developers. If you'd like to know more about provider signing, you can read about it here: https://www.terraform.io/docs/cli/plugins/signing.html Terraform has created a lock file .terraform.lock.hcl to record the provider selections it made above. Include this file in your version control repository so that Terraform can guarantee to make the same selections by default when you run \"terraform init\" in the future. Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. Next you can try running terraform plan , but since our main.tf file has no resource confguration, you will see that there is no change needed: juchowan@JUCHOWAN-M-D2P2 ACI-simulator % terraform plan No changes. Your infrastructure matches the configuration. Terraform has compared your real infrastructure against your configuration and found no differences, so no changes are needed.","title":"3.1 Run Terraform Init"},{"location":"terraform-basis/#32-optional-switch-registration","text":"At this point leaf and spine switches should be registered in the lab using Postman the day before. Registration of switch is possible to be done using resource aci_fabric_node_member . Documentation of this resource is present in the official Terraform Registry documentation . In the Example Usage section you can find example code to register new node In the Argument Reference you will see possible arguments/parameters that can be used for switch registration with indication if argument is required or optional and additional information about it In the Attribute Reference you will see what is the attribute that this resource exports, and in case of ACI resources it will be always id set to the DN of the VLAN Pool. Importing describes how to import existing object to the terraform resource For reference the following resource configuration in Terraform would register new switch. If the switches are already registered, this task can be skipped resource \"aci_fabric_node_member\" \"Leaf-101\" { name = \"Leaf-101\" serial = \"TEP-1-101\" node_id = \"101\" pod_id = \"1\" role = \"leaf\" } resource \"aci_fabric_node_member\" \"Leaf-102\" { name = \"Leaf-102\" serial = \"TEP-1-102\" node_id = \"102\" pod_id = \"1\" role = \"leaf\" } resource \"aci_fabric_node_member\" \"Spine-103\" { name = \"Spine-103\" serial = \"TEP-1-103\" node_id = \"103\" pod_id = \"1\" role = \"spine\" } resource \"aci_fabric_node_member\" \"Spine-104\" { name = \"Spine-104\" serial = \"TEP-1-104\" node_id = \"104\" pod_id = \"1\" role = \"spine\" }","title":"3.2 (Optional) Switch registration"},{"location":"terraform-basis/#4-aci-access-policies","text":"Every ACI Fabric configuration starts with Access Polices. You need to define ACI objects which represent single physical port configurations. To remind relationships between objects, please check following figure. Usually Interface Polices are configured once and re-used like LLDP, LACP, CDP, etc. In our LAB some of the policies already exist from the day before and Postman configuration, so different names should be used for Terraform lab to create new objects. Following sections will help you to prepare your API requests. For reference full configuration of access-policies can be found here: Terraform Tenant Ready .","title":"4 ACI Access Policies"},{"location":"terraform-basis/#41-interface-policies","text":"You will configure LACP Policy, LLDP and speed policies. Let's separate our terraform files and create separate config file for out interface policies. Under your folder in Visual Studio create new file calles interface-policies.tf : Copy each of the Policy resource definition to the new interface-policies.tf file LACP_ACTIVE resource \"aci_lacp_policy\" \"LACP_ACTIVE\" { na me = \"LACP_ACTIVE\" c trl = [ \"susp-individual\" , \"fast-sel-hot-stdby\" , \"graceful-conv\" ] mode = \"active\" } LLDP_ON resource \"aci_lldp_interface_policy\" \"LLDP_ON\" { na me = \"LLDP_ON\" admi n _rx_s t = \"enabled\" admi n _ t x_s t = \"enabled\" } LINK-10G resource \"aci_fabric_if_pol\" \"LINK-10G\" { na me = \"LINK-10G\" au t o_ ne g = \"on\" speed = \"10G\" } The documentation for each of the resources can be found in Terraform Registry: aci_lacp_policy resource aci_lldp_interface_policy resource aci_fabric_if_pol resource","title":"4.1 Interface Policies"},{"location":"terraform-basis/#42-run-terraform-plan-apply","text":"At this point we have our first three resources in the configuration so it's time to deploy them on our APIC. To do so we will follow the Terraform workload init -> plan -> apply. In previous step, after configuring ACI provider you should have already done terraform init , so it is not needed to run it again now, as it would not bring any change. Our first step to deploy config is to run terraform plan command in the terminal window. The output will show us what changes Terraform needs to do on the fabric, to bring it to the required configuration. The output should look similar to the following: juchowan@JUCHOWAN-M-D2P2 ACI-simulator % terraform plan aci_fabric_node_member.Leaf-102: Refreshing state... [id=uni/controller/nodeidentpol/nodep-TEP-1-102] aci_fabric_node_member.Leaf-101: Refreshing state... [id=uni/controller/nodeidentpol/nodep-TEP-1-101] aci_fabric_node_member.Spine-103: Refreshing state... [id=uni/controller/nodeidentpol/nodep-TEP-1-103] aci_fabric_node_member.Spine-104: Refreshing state... [id=uni/controller/nodeidentpol/nodep-TEP-1-104] Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # aci_fabric_if_pol.auto-10G will be created + resource \"aci_fabric_if_pol\" \"auto-10G\" { + annotation = \"orchestrator:terraform\" + auto_neg = \"on\" + description = (known after apply) + fec_mode = (known after apply) + id = (known after apply) + link_debounce = (known after apply) + name = \"LINK-10G\" + name_alias = (known after apply) + speed = \"10G\" } # aci_lldp_interface_policy.LLDP_ON will be created + resource \"aci_lldp_interface_policy\" \"LLDP_ON\" { + admin_rx_st = \"enabled\" + admin_tx_st = \"enabled\" + annotation = \"orchestrator:terraform\" + description = (known after apply) + id = (known after apply) + name = \"LLDP_ON\" + name_alias = (known after apply) } # aci_lacp_policy.LACP_ACTIVE will be created + resource \"aci_lacp_policy\" \"LACP_ACTIVE\" { + annotation = \"orchestrator:terraform\" + ctrl = [ + \"susp-individual\", + \"fast-sel-hot-stdby\", + \"graceful-conv\", ] + description = \"done by terraform\" + id = (known after apply) + max_links = (known after apply) + min_links = (known after apply) + mode = \"active\" + name = \"LACP_ACTIVE\" + name_alias = (known after apply) } Plan: 3 to add, 0 to change, 0 to destroy. \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Note: You didn't use the -out option to save this plan, so Terraform can't guarantee to take exactly these actions if you run \"terraform apply\" now. In the plan you can see all the plus signs beside the elements that are going to be added to the fabric. Terraform read the fabric state and compared with required configuration, and then decided which elements need to be added. At this point no configuration has been added yet to the fabric, Terraform only lists the changes it is planning to do. As the Note says at the bottom, this plan was not saved to a file, but only displayed in the console. If you change command to terraform plan -out interface-policies.plan you will see that a file was generated with the contents of the plan. Now you can perform the terraform apply command. This command will run plan again, ask for approval and then it will go into the fabric and perform these actions. <plan omitted> Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes aci_lldp_interface_policy.LLDP_ON: Creating... aci_fabric_if_pol.auto-10G: Creating... aci_lacp_policy.LACP_ACTIVE: Creating... aci_lldp_interface_policy.LLDP_ON: Creation complete after 0s [id=uni/infra/lldpIfP-LLDP_ON] aci_fabric_if_pol.auto-10G: Creation complete after 0s [id=uni/infra/hintfpol-LINK-10G] aci_lacp_policy.LACP_ACTIVE: Creation complete after 0s [id=uni/infra/lacplagp-LACP_ACTIVE] Apply complete! Resources: 3 added, 0 changed, 0 destroyed. If you run plan command again now, you would see the output is different because now it will only show the changes it plans to do over the previous push. Since there are no changes in the config yet, the plan will not show anything new to be added: juchowan@JUCHOWAN-M-D2P2 ACI-simulator % terraform plan aci_fabric_node_member.Spine-103: Refreshing state... [id=uni/controller/nodeidentpol/nodep-TEP-1-103] aci_fabric_node_member.Leaf-102: Refreshing state... [id=uni/controller/nodeidentpol/nodep-TEP-1-102] aci_lacp_policy.LACP_ACTIVE: Refreshing state... [id=uni/infra/lacplagp-LACP_ACTIVE] aci_fabric_if_pol.auto-10G: Refreshing state... [id=uni/infra/hintfpol-LINK-10G] aci_fabric_node_member.Spine-104: Refreshing state... [id=uni/controller/nodeidentpol/nodep-TEP-1-104] aci_lldp_interface_policy.LLDP_ON: Refreshing state... [id=uni/infra/lldpIfP-LLDP_ON] aci_fabric_node_member.Leaf-101: Refreshing state... [id=uni/controller/nodeidentpol/nodep-TEP-1-101] No changes. Your infrastructure matches the configuration.","title":"4.2 Run Terraform Plan &amp; Apply"},{"location":"terraform-basis/#43-vlans-domains-and-aaeps","text":"Create VLAN pool with vlan-range, associate it with Physical Domain and AAEP. Let's put these policies in a separate, new file called access-policies.tf We will create two VLAN pools, one for L2 Physical Domain, and second one for L3 Routed Domain. In the same way we will create two AEP, one for L2 connections and second one for L3 connections. Each AEP will map one domain: VLAN_POOL_L2 <-- PHYSDOM_1 <-- AEP_L2 VLAN_POOL_L3 <-- EXTRTDOM_1 <-- AEP_L3 For VLAN pool there are two resources that need to be created: VLAN Pool and VLAN Range. In this case VLAN Range object is a child of VLAN Pool object, which means that the Range resource needs to have a reference to the VLAN Pool resource. This reference can be done in terraform by using the resource name followed by the attribute, in this case id . resource \"aci_vlan_pool\" \"VLAN_POOL_L2\" { name = \"VLAN_POOL_L2\" alloc_mode = \"static\" } resource \"aci_ranges\" \"RANGE_100-200\" { vlan_pool_dn = aci_vlan_pool.VLAN_POOL_L2.id from = \"vlan-100\" to = \"vlan-200\" alloc_mode = \"inherit\" } resource \"aci_vlan_pool\" \"VLAN_POOL_L3\" { name = \"VLAN_POOL_L3\" alloc_mode = \"static\" } resource \"aci_ranges\" \"RANGE_201-300\" { vlan_pool_dn = aci_vlan_pool.VLAN_POOL_L3.id from = \"vlan-201\" to = \"vlan-300\" alloc_mode = \"inherit\" } Two domains, physical and routed, will be created as two resources, and each of them needs to have a reference to a VLAN Pool. This reference will be done by special attribute relation_infra_rs_vlan_ns , by referencing the resource name and id attribute: resource \"aci_physical_domain\" \"PHYSDOM_1\" { name = \"PHYSDOM_1\" relation_infra_rs_vlan_ns = aci_vlan_pool.VLAN_POOL_L2.id } resource \"aci_l3_domain_profile\" \"EXTRTDOM_1\" { name = \"EXTRTDOM_1\" relation_infra_rs_vlan_ns = aci_vlan_pool.VLAN_POOL_L3.id } Two AAEP will be created, one for L2 connections and another one for L3 connections. Between AAEP and Domains there is also relationship. AEP needs to be related to a domain. This relation can be done in two ways: separate resource aci_aaep_to_domain which is shown in the example below for AEP_L2 and PHYSDOM_1 relation. This resource needs to reference id values of both AEP and Physical Domain argument within aci_attachable_access_entity_profile called relation_infra_rs_dom_p which is shown in the example below for AEP_L3 and EXTRTDOM_1 relation. This argument needs to reference id values of both AEP and Routed Domain, but needs to be embedded in [] as the documentation says it's Type -[Set of String] resource \"aci_attachable_access_entity_profile\" \"AEP_L2\" { description = \"AAEP for L2 connections\" name = \"AEP_L2\" } resource \"aci_aaep_to_domain\" \"AEP_L2_to_PHYSDOM_1\" { attachable_access_entity_profile_dn = aci_attachable_access_entity_profile.AEP_L2.id domain_dn = aci_physical_domain.PHYSDOM_1.id } resource \"aci_attachable_access_entity_profile\" \"AEP_L3\" { description = \"AAEP for L3 connections\" name = \"AEP_L3\" relation_infra_rs_dom_p = [aci_l3_domain_profile.EXTRTDOM_1.id] } You can plan and apply to see the result on ACI. Documentation of resources: aci_vlan_pool resource aci_ranges resource aci_physical_domain resource aci_l3_domain_profile resource aci_attachable_access_entity_profile resource aci_aaep_to_domain resource","title":"4.3 VLANs, Domains and AAEPs"},{"location":"terraform-basis/#44-interface-policy-group","text":"Let's now create two Interface Policy Groups, one for server (L2) and another one for router (L3) and use all created policies: IntPolGrp_VPC_server1 VPC policy with AEP_L2 IntPolGrp_Router1 access port policy with AEP_L3 Those interface policy groups will have relation to all the access policies we created before by referencing the id of those policies: resource \"aci_leaf_access_bundle_policy_group\" \"IntPolGrp_VPC_server1\" { name = \"IntPolGrp_VPC_server1\" lag_t = \"node\" relation_infra_rs_lldp_if_pol = aci_lldp_interface_policy.LLDP_ON.id relation_infra_rs_lacp_pol = aci_lacp_policy.LACP_ACTIVE.id relation_infra_rs_h_if_pol = aci_fabric_if_pol.LINK-10G.id relation_infra_rs_att_ent_p = aci_attachable_access_entity_profile.AEP_L2.id } resource \"aci_leaf_access_bundle_policy_group\" \"IntPolGrp_Router1\" { name = \"IntPolGrp_Router1\" lag_t = \"node\" relation_infra_rs_lldp_if_pol = aci_lldp_interface_policy.LLDP_ON.id relation_infra_rs_lacp_pol = aci_lacp_policy.LACP_ACTIVE.id relation_infra_rs_h_if_pol = aci_fabric_if_pol.LINK-10G.id relation_infra_rs_att_ent_p = aci_attachable_access_entity_profile.AEP_L3.id } You can plan and apply to see the result on ACI. Documentation of resources: aci_leaf_access_bundle_policy_group resource aci_leaf_access_port_policy_group resource","title":"4.4 Interface Policy group"},{"location":"terraform-basis/#45-switch-interface-profiles","text":"Let's assign our created interface policy groups to the newly created switch and interface profiles. We will create: switch profile interface profile IntProf-101-102 with two interface selectors eth1/1 connecting server eth1/2 connecting router First we create interface profile within this profile two interface selectors. Interface selectors need to have reference to the Interface Profile ( IntProf-101-102 ) and relation to the interface policy groups created for our end devices: IntPolGrp_VPC_server1 IntPolGrp_Router1 resource \"aci_leaf_interface_profile\" \"IntProf-101-102\" { name = \"IntProf-101-102\" } resource \"aci_access_port_selector\" \"leaf-101-102-eth-1\" { leaf_interface_profile_dn = aci_leaf_interface_profile.IntProf-101-102.id name = \"leaf-101-102-eth-1\" access_port_selector_type = \"range\" relation_infra_rs_acc_base_grp = aci_leaf_access_bundle_policy_group.IntPolGrp_VPC_server1.id } resource \"aci_access_port_block\" \"leaf-101-102-eth-1-block\" { access_port_selector_dn = aci_access_port_selector.leaf-101-102-eth-1.id name = \"leaf-101-102-eth-1-block\" from_card = \"1\" from_port = \"1\" to_card = \"1\" to_port = \"1\" } resource \"aci_access_port_selector\" \"leaf-101-102-eth-2\" { leaf_interface_profile_dn = aci_leaf_interface_profile.IntProf-101-102.id name = \"leaf-101-102-eth-2\" access_port_selector_type = \"range\" relation_infra_rs_acc_base_grp = aci_leaf_access_bundle_policy_group.IntPolGrp_Router1.id } resource \"aci_access_port_block\" \"leaf-101-102-eth-2-block\" { access_port_selector_dn = aci_access_port_selector.leaf-101-102-eth-2.id name = \"leaf-101-102-eth-2-block\" from_card = \"1\" from_port = \"2\" to_card = \"1\" to_port = \"2\" } Next we have our switch profile configuration which needs to have a reference to the leaf interface profile ( IntProf-101-102 ) as well as mark the nodes which will be used in this switch profile (nodes 101 & 102) resource \"aci_leaf_profile\" \"SwProf-101-102\" { name = \"SwProf-101-102\" relation_infra_rs_acc_port_p = [aci_leaf_interface_profile.IntProf-101-102.id] } resource \"aci_leaf_selector\" \"Sel-101-102\" { leaf_profile_dn = aci_leaf_profile.SwProf-101-102.id name = \"Sel-101-102\" switch_association_type = \"range\" } resource \"aci_node_block\" \"Sel-101-102-Block\" { switch_association_dn = aci_leaf_selector.Sel-101-102.id name = \"Sel-101-102-Block\" from_ = \"101\" to_ = \"102\" } You can plan and apply to see the result on ACI.","title":"4.5 Switch &amp; Interface Profiles"},{"location":"terraform-basis/#46-vpc-protection-groups","text":"For a pair of switches that are expected to work as VPC it is required to configure a VPC protection group. resource \"aci_vpc_explicit_protection_group\" \"vpc-101-102\" { name = \"vpc-101-102\" switch1 = \"101\" switch2 = \"102\" vpc_explicit_protection_group_id = \"101\" }","title":"4.6 VPC Protection Groups"},{"location":"terraform-basis/#5-aci-tenant","text":"Upon now you created ACI AccessPolicies for your Tenant. Now is time to create your tenant using Terraform. It will be simple Tenant definition with one VRF, one bridge domains associated with one EPG and one L3out. EPG will be associated with L2 domain and statically binded to VPC created, with VLAN from L2 VLAN pool done by you perviously. The L3out will be associated to L3 domain and router interface, with VLAN from L3 VLAN pool. Figure below shows our Logical Tenant. You need to define Tenant, VRF, Bridge-Domain, Application-Profile, EPG with Domain association, static binding under EPG and L3out. Quite many of resources to do in Terraform. We will define all of them in a single, new config file called dcloud-tenant-1.tf Please, download a terraform file from Terraform Tenant Ready .","title":"5 ACI Tenant"},{"location":"terraform-basis/#51-tenant-components","text":"This section contains Terraform codes necessary to create Tenant objects","title":"5.1 Tenant components"},{"location":"terraform-basis/#511-tenant-and-vrf","text":"Following code can be used to create new Tenant and VRFs. The VRF resource needs to reference its parent object - tenant id. resource \"aci_tenant\" \"dcloud-tenant-1\" { name = \"dcloud-tenant-1\" } resource \"aci_vrf\" \"vrf1\" { tenant_dn = aci_tenant.dcloud-tenant-1.id name = \"vrf1\" }","title":"5.1.1 Tenant and VRF"},{"location":"terraform-basis/#512-bd-in-existing-tenant","text":"Following code can be used to create new Bridge Domain in existing Tenant. Bridge Domain needs to have a reference to parent objects: tenant ID and VRF ID. There is also a child object of bridge domain which is a Subnet with reference to bridge domain ID. resource \"aci_bridge_domain\" \"bd-network-1\" { tenant_dn = aci_tenant.dcloud-tenant-1.id relation_fv_rs_ctx = aci_vrf.vrf1.id name = \"bd-network-1\" } resource \"aci_subnet\" \"subnet-1\" { parent_dn = aci_bridge_domain.bd-network-1.id ip = \"10.0.0.1/24\" scope = [\"public\"] } You can plan and apply to see the result on ACI.","title":"5.1.2 BD in existing tenant"},{"location":"terraform-basis/#513-application-profile-with-epg-and-static-bindings","text":"Following code can be used to create new Application Profile with EPG. Application Profile needs a reference to parent object tenant ID, while EPG needs reference to AP ID ad Bridge Domain ID. The next two resources reflect the physical domain PHYSDOM_1 attached to that EPG, and one static binding towards our server1 with encapsulation of vlan 100. resource \"aci_application_profile\" \"ap1\" { tenant_dn = aci_tenant.dcloud-tenant-1.id name = \"ap1\" } resource \"aci_application_epg\" \"epg1\" { application_profile_dn = aci_application_profile.ap1.id name = \"epg1\" relation_fv_rs_bd = aci_bridge_domain.bd-network-1.id } resource \"aci_epg_to_domain\" \"epg1-to-physdom_1\" { application_epg_dn = aci_application_epg.epg1.id tdn = aci_physical_domain.PHYSDOM_1.id } resource \"aci_epg_to_static_path\" \"static-binding-1\" { application_epg_dn = aci_application_epg.epg1.id tdn = \"topology/pod-1/protpaths-101-102/pathep-[IntPolGrp_VPC_server1]\" encap = \"vlan-100\" mode = \"regular\" } You can plan and apply to see the result on ACI.","title":"5.1.3 Application Profile with EPG and static bindings"},{"location":"terraform-basis/#514-l3out","text":"Following code can be used to create new L3out. As you can see there are plenty of resources which reflect node profile and node mapping, interface profile and path used for it, external EPG with subnet. resource \"aci_l3_outside\" \"l3out-1\" { tenant_dn = aci_tenant.dcloud-tenant-1.id name = \"l3out-1\" relation_l3ext_rs_l3_dom_att = aci_l3_domain_profile.EXTRTDOM_1.id relation_l3ext_rs_ectx = aci_vrf.vrf1.id } resource \"aci_logical_node_profile\" \"np-101-102\" { l3_outside_dn = aci_l3_outside.l3out-1.id name = \"np-101-102\" } resource \"aci_logical_node_to_fabric_node\" \"np-101-102-to-node-101\" { logical_node_profile_dn = aci_logical_node_profile.np-101-102.id tdn = \"topology/pod-1/node-101\" rtr_id = \"1.1.1.1\" } resource \"aci_logical_node_to_fabric_node\" \"np-101-102-to-node-102\" { logical_node_profile_dn = aci_logical_node_profile.np-101-102.id tdn = \"topology/pod-1/node-102\" rtr_id = \"1.1.1.2\" } resource \"aci_logical_interface_profile\" \"ip-101-102\" { logical_node_profile_dn = aci_logical_node_profile.np-101-102.id name = \"ip-101-102\" } resource \"aci_l3out_path_attachment\" \"l3ip-path\" { logical_interface_profile_dn = aci_logical_interface_profile.ip-101-102.id target_dn = \"topology/pod-1/protpaths-101-102/pathep-[IntPolGrp_Router1]\" if_inst_t = \"ext-svi\" addr = \"10.1.1.1/24\" encap = \"vlan-201\" encap_scope = \"ctx\" mode = \"regular\" mtu = \"inherit\" } resource \"aci_external_network_instance_profile\" \"extepg\" { l3_outside_dn = aci_l3_outside.l3out-1.id name = \"extepg\" } resource \"aci_l3_ext_subnet\" \"extepg-subnet\" { external_network_instance_profile_dn = aci_external_network_instance_profile.extepg.id ip = \"0.0.0.0/0\" scope = [\"import-security\",\"export-rtctrl\"] } You can plan and apply to see the result on ACI. Documentation of resources: aci_tenant resource aci_bridge_domain aci_subnet resource aci_application_profile aci_application_epg aci_epg_to_domain aci_l3_outside","title":"5.1.4 L3Out"},{"location":"terraform-custom/","text":"Create sample use-cases with terraform Use-Case no.1 Customer place SHARED network component in ACI shared tenant common . Custom tenants contain dedicated VRF, BDs and EPGs, Domain associations and static-bindings for particular department. L3out configuration for routing with external networks. Use existing PHYSDOM_1 physical domain and existing EXTRTDOM_1 routed domain (TIP: you can use data sources for this) Use following static-bindings for devices: WAN router: node 101, interface eth1/5, vlan 250 INTERNET router: topology/pod-1/protpaths-101-102/pathep-[IntPolGrp_Router1], vlan 251 UCS/vmware server: topology/pod-1/protpaths-101-102/pathep-[IntPolGrp_VPC_server1] EPG_HA - vlan 152 EPG_UC - vlan 150 EPG_DMZ - vlan 151 Based on experience gained, build yourself a use-case in Terraform. You can specify each resource directly, or you can use variable file.","title":"Create sample use-cases with terraform"},{"location":"terraform-custom/#create-sample-use-cases-with-terraform","text":"","title":"Create sample use-cases with terraform"},{"location":"terraform-custom/#use-case-no1","text":"Customer place SHARED network component in ACI shared tenant common . Custom tenants contain dedicated VRF, BDs and EPGs, Domain associations and static-bindings for particular department. L3out configuration for routing with external networks. Use existing PHYSDOM_1 physical domain and existing EXTRTDOM_1 routed domain (TIP: you can use data sources for this) Use following static-bindings for devices: WAN router: node 101, interface eth1/5, vlan 250 INTERNET router: topology/pod-1/protpaths-101-102/pathep-[IntPolGrp_Router1], vlan 251 UCS/vmware server: topology/pod-1/protpaths-101-102/pathep-[IntPolGrp_VPC_server1] EPG_HA - vlan 152 EPG_UC - vlan 150 EPG_DMZ - vlan 151 Based on experience gained, build yourself a use-case in Terraform. You can specify each resource directly, or you can use variable file.","title":"Use-Case no.1"},{"location":"terraform-installation/","text":"Terraform setup HashiCorp Terraform is an infrastructure as code tool that lets you define both cloud and on-prem resources in human-readable configuration files that you can version, reuse, and share. You can then use a consistent workflow to provision and manage all of your infrastructure throughout its lifecycle. Terraform can manage low-level components like compute, storage, and networking resources, as well as high-level components like DNS entries and SaaS features. 1. Install Teraform in your workstation Note Participants can use virtual machine with Windows or work on personal laptop. Please follow installation instructions for the OS on your laptops. To use Terraform you will need to install it. HashiCorp distributes Terraform as a binary package. You can also install Terraform using popular package managers. 1.1 For CentOS/RHEL distribution: Install yum-config-manager to manage your repositories. sudo yum install -y yum-utils Use yum-config-manager to add the official HashiCorp Linux repository. sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo Install Terraform from the new repository. sudo yum -y install terraform 1.2. For Windows OS: To install Terraform, find the appropriate package for your system and download it as a zip archive: https://releases.hashicorp.com/terraform/1.3.3/terraform_1.3.3_windows_amd64.zip After downloading Terraform, unzip the package. Terraform runs as a single binary named terraform. Any other files in the package can be safely removed and Terraform will still function. Copy the terraform.bin to C:\\Windows folder. Finally, make sure that the terraform binary is available on your PATH. The path can be edited through: 1) Find Advanced system settings -> Advanced -> Environment Variables 2) Find PATH variable and click edit to change 3) Verify if C:\\Windows exists in the path and if not add it to the list 1.3 For Macbook with OS X: Homebrew is a free and open-source package management system for Mac OS X. Install the official Terraform formula from the terminal. First, install the HashiCorp tap, a repository of all our Homebrew packages. brew tap hashicorp/tap Now, install Terraform with hashicorp/tap/terraform. brew install hashicorp/tap/terraform Note This installs a signed binary and is automatically updated with every new official release. To update to the latest version of Terraform, first update Homebrew. brew update Then, run the upgrade command to download and use the latest Terraform version. brew upgrade hashicorp/tap/terraform ==> Upgrading 1 outdated package: hashicorp/tap/terraform 0.15.3 -> 1.0.0 ==> Upgrading hashicorp/tap/terraform 0.15.3 -> 1.0.0 1.3 Verify the installation Verify that the installation worked by opening a new terminal session and listing Terraform's available subcommands. terraform -help Usage: terraform [-version] [-help] <command> [args] The available commands for execution are listed below. The most common, useful commands are shown first, followed by less common or more advanced commands. If you're just getting started with Terraform, stick with the common commands. For the other commands, please read the help and docs before usage. #... Add any subcommand to terraform -help to learn more about what it does and available options. terraform -help plan","title":"Inter-Tenant routing"},{"location":"terraform-installation/#terraform-setup","text":"HashiCorp Terraform is an infrastructure as code tool that lets you define both cloud and on-prem resources in human-readable configuration files that you can version, reuse, and share. You can then use a consistent workflow to provision and manage all of your infrastructure throughout its lifecycle. Terraform can manage low-level components like compute, storage, and networking resources, as well as high-level components like DNS entries and SaaS features.","title":"Terraform setup"},{"location":"terraform-installation/#1-install-teraform-in-your-workstation","text":"Note Participants can use virtual machine with Windows or work on personal laptop. Please follow installation instructions for the OS on your laptops. To use Terraform you will need to install it. HashiCorp distributes Terraform as a binary package. You can also install Terraform using popular package managers.","title":"1. Install Teraform in your workstation"},{"location":"terraform-installation/#11-for-centosrhel-distribution","text":"Install yum-config-manager to manage your repositories. sudo yum install -y yum-utils Use yum-config-manager to add the official HashiCorp Linux repository. sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo Install Terraform from the new repository. sudo yum -y install terraform","title":"1.1 For CentOS/RHEL distribution:"},{"location":"terraform-installation/#12-for-windows-os","text":"To install Terraform, find the appropriate package for your system and download it as a zip archive: https://releases.hashicorp.com/terraform/1.3.3/terraform_1.3.3_windows_amd64.zip After downloading Terraform, unzip the package. Terraform runs as a single binary named terraform. Any other files in the package can be safely removed and Terraform will still function. Copy the terraform.bin to C:\\Windows folder. Finally, make sure that the terraform binary is available on your PATH. The path can be edited through: 1) Find Advanced system settings -> Advanced -> Environment Variables 2) Find PATH variable and click edit to change 3) Verify if C:\\Windows exists in the path and if not add it to the list","title":"1.2. For Windows OS:"},{"location":"terraform-installation/#13-for-macbook-with-os-x","text":"Homebrew is a free and open-source package management system for Mac OS X. Install the official Terraform formula from the terminal. First, install the HashiCorp tap, a repository of all our Homebrew packages. brew tap hashicorp/tap Now, install Terraform with hashicorp/tap/terraform. brew install hashicorp/tap/terraform Note This installs a signed binary and is automatically updated with every new official release. To update to the latest version of Terraform, first update Homebrew. brew update Then, run the upgrade command to download and use the latest Terraform version. brew upgrade hashicorp/tap/terraform ==> Upgrading 1 outdated package: hashicorp/tap/terraform 0.15.3 -> 1.0.0 ==> Upgrading hashicorp/tap/terraform 0.15.3 -> 1.0.0","title":"1.3 For Macbook with OS X:"},{"location":"terraform-installation/#13-verify-the-installation","text":"Verify that the installation worked by opening a new terminal session and listing Terraform's available subcommands. terraform -help Usage: terraform [-version] [-help] <command> [args] The available commands for execution are listed below. The most common, useful commands are shown first, followed by less common or more advanced commands. If you're just getting started with Terraform, stick with the common commands. For the other commands, please read the help and docs before usage. #... Add any subcommand to terraform -help to learn more about what it does and available options. terraform -help plan","title":"1.3 Verify the installation"},{"location":"terraform-modules/","text":"Terraform modules As you can see deploying objects like L3out require a lot of repeatable configuration. Such configs can be simplified by using Terraform modules. In such module we can write source configuration and re-use it multiple times, with standardized configuration. In this lab we will use existing module for L3out configuration. This module consists of three files: main.tf with the resource configuration variables.tf with th variables definition outputs.tf with the output ID value that this module would return Each three files should be placed under modules/aci_l3out/ folder structure. Apart from those files, we still require our own configuration, so again we will create main.tf , variables.tf and terraform.tfvars under main folder. The whole structure should look like this: ACI-4/ main.tf variables.tf terraform.tfvars modules/ aci_l3out/ main.tf variables.tf outputs.tf The complete folder can be downloaded: here We will walk over the config files. Module files Inside the module folder we have three files. In the variables.tf we define every variable that we will be using. In case of this module those variables don't have any default values assigned, they mostly specify the type of parameters to be used in maps. In the main.tf file we create our complex l3out configuration and relations between multiple objects. This configuration looks similar to your manual L3out configuration, but this time it is reusable and you wouldn't have to write it multiple times for multiple L3outs. We have set of resources like aci_l3out , aci_logical_node_profile , aci_logical_interface_profile etc. This module adds also BGP peer connectivity which is something we haven't done yet in the previous labs. The new thing you can spot in this module is the initial part with the locals block. This block defined a local value. Local value is a temporary variable that is constructed with an expression on other variables. If you run expressions on variables, for example loops, you would always get some temporary variable that you can use. However, what a local value does, is that it assigns a name to that expression and you will be able to reuse it multiple times within your module, instead of repeating the expression. You can treat it similarly to a function in programming which returns some value. In case of local value external_epg_subnets it is performing flatten function. (Flatten function)[https://developer.hashicorp.com/terraform/language/functions/flatten] is useful for us when we want to run loops inside a map of map. The result of this flatten function would be a list of all subnets that need to be configured under external EPG with their parameters. locals { external_epg_subnets = flatten([ for l3epg_key, l3epg in var.external_epgs : [ for subnet_key, subnet in l3epg.subnets : { external_epg_key = l3epg_key external_epg_name = l3epg.name subnet_key = subnet_key subnet_ip = subnet.ip subnet_scope = subnet.scope } ] ]) } Later in the code we can see that this is used with for function in aci_l3_ext_subnet resource: resource \"aci_l3_ext_subnet\" \"l3instp_subnet\" { for_each = { for subnet in local.external_epg_subnets : \"${subnet.external_epg_key}.${subnet.subnet_key}\" => subnet } external_network_instance_profile_dn = aci_external_network_instance_profile.l3instp[each.value.external_epg_key].id ip = each.value.subnet_ip scope = each.value.subnet_scope } In the value.tf file we specify the output value for this module. Output values are similar to return values in programming languages. In this case they will return ID of L3out we created, and that ID in ACI will be the path of this object. As you may remember, our standard resources from official ACI provider also return ID value of this resource in ACI, and it is always the path to the object. Main folder files In the main folder the files use same configuration as in previous labs, so you should be already familiar with the way we configure objects using variables and how we configure bridge domains with conditionals and for_each loop. The new part is the way we configure L3Out using module. As first part you can see that we are using data source to import existing configuration of Routed Domain. It would have to be used for configuration of L3out, so we need to have a way to reference it in our files. Next we have our block for configuring the L3out itself. The highlithed lines show how we can write configuration and reference the module. Instead of a resource block, we are using module block and then in second line we have to reference the source path where this module is located ./modules/aci_l3out . module \"l3out_core\" { source = \"./modules/aci_l3out\" tenant _id = aci_ tenant . tenant .id na me = \"core_l3out\" vr f _id = aci_vr f .mai n .id l 3 _ex t _domai n _id = da ta .aci_l 3 _domai n _pro f ile.EXTRTDOM_ 1. id n odes = { 101 = { pod_id = \"1\" n ode_id = \"101\" r tr _id = \"51.1.1.1\" r tr _id_loopback = \"no\" } } pa t hs = { \"101.1.10\" = { pod_id = \"1\" n ode_id = \"101\" por t _id = \"eth1/10\" ip_addr = \"5.5.1.2/24\" mode = \"regular\" m tu = \"inherit\" } } bgp_peers = { asa = { peer_addr = \"5.5.1.1\" peer_as n = \"65099\" local_as n = \"65001\" } } ex ternal _epgs = { de fault = { na me = \"default_l3epg\" pre f Grp = \"exclude\" provided_co ntra c ts = [ \"default\" ] co nsu med_co ntra c ts = [] sub nets = { \"0.0.0.0\" = { ip = \"0.0.0.0/0\" scope = [ \"import-security\" ] } } } } } The remaining part is the actual config parameters that we want to use for our L3out. Each of the parameters that we are assigning is defined in the module itself. In this file we simply assign values to them, either manually by typing it (i.e. name = \"core_l3out\") or by referencing other variables and objects (i.e tenant_id = aci_tenant.tenant.id). Lab Customer creates Custom2 tenant with dedicated VRF, bridge domains, EPGs, domain associations and static-bindings for particular departaments. There are also two L3outs for MPLS and Internet connectivity. Use existing PHYSDOM_1 physcial domain and existing EXTRTDOM_1 routed domain. Use following static bindings: WAN Router: node 101, interface eth1/10, routed interface INTERNET Router: node 102, interface eth1/11, routed interface UCS/vmware server: topology/pod-1/protpaths-101-102/pathep-[IntPolGrp_VPC_server1] EPG_UC - vlan 160 EPG_DMZ - vlan 161","title":"Terraform modules"},{"location":"terraform-modules/#terraform-modules","text":"As you can see deploying objects like L3out require a lot of repeatable configuration. Such configs can be simplified by using Terraform modules. In such module we can write source configuration and re-use it multiple times, with standardized configuration. In this lab we will use existing module for L3out configuration. This module consists of three files: main.tf with the resource configuration variables.tf with th variables definition outputs.tf with the output ID value that this module would return Each three files should be placed under modules/aci_l3out/ folder structure. Apart from those files, we still require our own configuration, so again we will create main.tf , variables.tf and terraform.tfvars under main folder. The whole structure should look like this: ACI-4/ main.tf variables.tf terraform.tfvars modules/ aci_l3out/ main.tf variables.tf outputs.tf The complete folder can be downloaded: here We will walk over the config files. Module files Inside the module folder we have three files. In the variables.tf we define every variable that we will be using. In case of this module those variables don't have any default values assigned, they mostly specify the type of parameters to be used in maps. In the main.tf file we create our complex l3out configuration and relations between multiple objects. This configuration looks similar to your manual L3out configuration, but this time it is reusable and you wouldn't have to write it multiple times for multiple L3outs. We have set of resources like aci_l3out , aci_logical_node_profile , aci_logical_interface_profile etc. This module adds also BGP peer connectivity which is something we haven't done yet in the previous labs. The new thing you can spot in this module is the initial part with the locals block. This block defined a local value. Local value is a temporary variable that is constructed with an expression on other variables. If you run expressions on variables, for example loops, you would always get some temporary variable that you can use. However, what a local value does, is that it assigns a name to that expression and you will be able to reuse it multiple times within your module, instead of repeating the expression. You can treat it similarly to a function in programming which returns some value. In case of local value external_epg_subnets it is performing flatten function. (Flatten function)[https://developer.hashicorp.com/terraform/language/functions/flatten] is useful for us when we want to run loops inside a map of map. The result of this flatten function would be a list of all subnets that need to be configured under external EPG with their parameters. locals { external_epg_subnets = flatten([ for l3epg_key, l3epg in var.external_epgs : [ for subnet_key, subnet in l3epg.subnets : { external_epg_key = l3epg_key external_epg_name = l3epg.name subnet_key = subnet_key subnet_ip = subnet.ip subnet_scope = subnet.scope } ] ]) } Later in the code we can see that this is used with for function in aci_l3_ext_subnet resource: resource \"aci_l3_ext_subnet\" \"l3instp_subnet\" { for_each = { for subnet in local.external_epg_subnets : \"${subnet.external_epg_key}.${subnet.subnet_key}\" => subnet } external_network_instance_profile_dn = aci_external_network_instance_profile.l3instp[each.value.external_epg_key].id ip = each.value.subnet_ip scope = each.value.subnet_scope } In the value.tf file we specify the output value for this module. Output values are similar to return values in programming languages. In this case they will return ID of L3out we created, and that ID in ACI will be the path of this object. As you may remember, our standard resources from official ACI provider also return ID value of this resource in ACI, and it is always the path to the object. Main folder files In the main folder the files use same configuration as in previous labs, so you should be already familiar with the way we configure objects using variables and how we configure bridge domains with conditionals and for_each loop. The new part is the way we configure L3Out using module. As first part you can see that we are using data source to import existing configuration of Routed Domain. It would have to be used for configuration of L3out, so we need to have a way to reference it in our files. Next we have our block for configuring the L3out itself. The highlithed lines show how we can write configuration and reference the module. Instead of a resource block, we are using module block and then in second line we have to reference the source path where this module is located ./modules/aci_l3out . module \"l3out_core\" { source = \"./modules/aci_l3out\" tenant _id = aci_ tenant . tenant .id na me = \"core_l3out\" vr f _id = aci_vr f .mai n .id l 3 _ex t _domai n _id = da ta .aci_l 3 _domai n _pro f ile.EXTRTDOM_ 1. id n odes = { 101 = { pod_id = \"1\" n ode_id = \"101\" r tr _id = \"51.1.1.1\" r tr _id_loopback = \"no\" } } pa t hs = { \"101.1.10\" = { pod_id = \"1\" n ode_id = \"101\" por t _id = \"eth1/10\" ip_addr = \"5.5.1.2/24\" mode = \"regular\" m tu = \"inherit\" } } bgp_peers = { asa = { peer_addr = \"5.5.1.1\" peer_as n = \"65099\" local_as n = \"65001\" } } ex ternal _epgs = { de fault = { na me = \"default_l3epg\" pre f Grp = \"exclude\" provided_co ntra c ts = [ \"default\" ] co nsu med_co ntra c ts = [] sub nets = { \"0.0.0.0\" = { ip = \"0.0.0.0/0\" scope = [ \"import-security\" ] } } } } } The remaining part is the actual config parameters that we want to use for our L3out. Each of the parameters that we are assigning is defined in the module itself. In this file we simply assign values to them, either manually by typing it (i.e. name = \"core_l3out\") or by referencing other variables and objects (i.e tenant_id = aci_tenant.tenant.id).","title":"Terraform modules"},{"location":"terraform-modules/#lab","text":"Customer creates Custom2 tenant with dedicated VRF, bridge domains, EPGs, domain associations and static-bindings for particular departaments. There are also two L3outs for MPLS and Internet connectivity. Use existing PHYSDOM_1 physcial domain and existing EXTRTDOM_1 routed domain. Use following static bindings: WAN Router: node 101, interface eth1/10, routed interface INTERNET Router: node 102, interface eth1/11, routed interface UCS/vmware server: topology/pod-1/protpaths-101-102/pathep-[IntPolGrp_VPC_server1] EPG_UC - vlan 160 EPG_DMZ - vlan 161","title":"Lab"},{"location":"terraform-state/","text":"Terraform State 1. View Terraform State Let's focus now on the state of our deployments. By now you should have three folders with different Terraform configuration. Please change to ACI folder in terraform terminal and run terraform state list command. The output should list all the objects that were deployed by Terraform modules and will look similar to this: juchowan@JUCHOWAN-M-D2P2 ACI % terraform state list aci_aaep_to_domain.AEP_L2_to_PHYSDOM_1 aci_access_port_block.leaf-101-102-eth-1-block aci_access_port_block.leaf-101-102-eth-2-block aci_access_port_selector.leaf-101-102-eth-1 aci_access_port_selector.leaf-101-102-eth-2 aci_application_epg.epg1 aci_application_profile.ap1 aci_attachable_access_entity_profile.AEP_L2 aci_attachable_access_entity_profile.AEP_L3 aci_bridge_domain.bd-network-1 aci_bridge_domain.bd_192_168_1_0 aci_bridge_domain.bd_192_168_2_0 aci_epg_to_domain.epg1-to-physdom_1 aci_epg_to_static_path.static-binding-1 aci_external_network_instance_profile.extepg aci_fabric_if_pol.LINK_10G aci_fabric_node_member.Leaf-101 aci_fabric_node_member.Leaf-102 aci_fabric_node_member.Spine-103 aci_fabric_node_member.Spine-104 aci_l3_domain_profile.EXTRTDOM_1 aci_l3_ext_subnet.extepg-subnet aci_l3_outside.l3out-1 aci_l3out_path_attachment.l3ip-path aci_lacp_policy.LACP_ACTIVE aci_leaf_access_bundle_policy_group.IntPolGrp_Router1 aci_leaf_access_bundle_policy_group.IntPolGrp_VPC_server1 aci_leaf_interface_profile.IntProf-101-102 aci_leaf_profile.SwProf-101-102 aci_leaf_selector.Sel-101-102 aci_lldp_interface_policy.LLDP_ON aci_logical_interface_profile.ip-101-102 aci_logical_node_profile.np-101-102 aci_logical_node_to_fabric_node.np-101-102-to-node-101 aci_logical_node_to_fabric_node.np-101-102-to-node-102 aci_node_block.Sel-101-102-Block aci_physical_domain.PHYSDOM_1 aci_ranges.RANGE_100-200 aci_ranges.RANGE_201-300 aci_subnet.subnet-1 aci_tenant.dcloud-tenant-1 aci_tenant.dcloud-tenant-2 aci_vlan_pool.VLAN_POOL_L2 aci_vlan_pool.VLAN_POOL_L3 aci_vrf.vrf1 aci_vrf.vrf2-1 You can see every object/resource we deployed in the previous labs. To see detailed configuration of each object run terraform state show <object name> , for example `terraform state show aci_bridge_domain.bd-network-1` You will get output similar to this: juchowan@JUCHOWAN-M-D2P2 ACI-simulator % terraform state show aci_bridge_domain.bd-network-1 # aci_bridge_domain.bd-network-1: resource \"aci_bridge_domain\" \"bd-network-1\" { annotation = \"orchestrator:terraform\" arp_flood = \"no\" bridge_domain_type = \"regular\" ep_clear = \"no\" ep_move_detect_mode = \"disable\" host_based_routing = \"no\" id = \"uni/tn-dcloud-tenant-1/BD-bd-network-1\" intersite_bum_traffic_allow = \"no\" intersite_l2_stretch = \"no\" ip_learning = \"yes\" ipv6_mcast_allow = \"no\" limit_ip_learn_to_subnets = \"yes\" ll_addr = \"::\" mac = \"00:22:BD:F8:19:FF\" mcast_allow = \"no\" multi_dst_pkt_act = \"bd-flood\" name = \"bd-network-1\" optimize_wan_bandwidth = \"no\" relation_fv_rs_ctx = \"uni/tn-dcloud-tenant-1/ctx-vrf1\" tenant_dn = \"uni/tn-dcloud-tenant-1\" unicast_route = \"yes\" unk_mac_ucast_act = \"proxy\" unk_mcast_act = \"flood\" v6unk_mcast_act = \"flood\" vmac = \"not-applicable\" } You can see the current state of this bridge domain and configuration of many parameters, even the ones we did not specify in the configuration. The reason we have these parameters here, is that they took the default value for bridge domain. The id element is the path of the bridge domain that was returned to us from ACI after we created this bridge domain. You can in the same way check the state of other objects that we deployed. Terraform stores information about our infrastructure in a state file. This state file keeps track of resources created by our configuration and maps them to real-world resources. The state file by default is located in the main directory of our workspace and is called terraform.tfstate . If you list all the contents of our workspace folder you will see that this file exists, and is created after we apply our config: juchowan@JUCHOWAN-M-D2P2 ACI-simulator % ls | grep tfstate terraform.tfstate terraform.tfstate.backup There is also a backup file created of our state. Those files should not be changed manually to avoid drift between our Terraform configuration, state and infrastructure. Open terraform.tfstate file in a file editor. You will see that this file is in the JSON format. The first stanza contains information about our Terraform application { \"version\": 4, \"terraform_version\": \"1.3.2\", \"serial\": 577, \"lineage\": \"8c71daed-6a43-a3c1-3981-9ce1f9789eb6\", \"outputs\": {}, \"resources\": [ Below that is the resources section of the state file, and it contains the schema for any resource that was created in Terraform. Example resource from the state file: { \"mode\": \"managed\", \"type\": \"aci_aaep_to_domain\", \"name\": \"AEP_L2_to_PHYSDOM_1\", \"provider\": \"provider[\\\"registry.terraform.io/ciscodevnet/aci\\\"]\", \"instances\": [ { \"schema_version\": 1, \"attributes\": { \"annotation\": \"orchestrator:terraform\", \"attachable_access_entity_profile_dn\": \"uni/infra/attentp-AEP_L2\", \"description\": null, \"domain_dn\": \"uni/phys-PHYSDOM_1\", \"id\": \"uni/infra/attentp-AEP_L2/rsdomP-[uni/phys-PHYSDOM_1]\" }, \"sensitive_attributes\": [], \"private\": \"eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ==\", \"dependencies\": [ \"aci_attachable_access_entity_profile.AEP_L2\", \"aci_physical_domain.PHYSDOM_1\", \"aci_vlan_pool.VLAN_POOL_L2\" ] } ] }, The first key is mode . Mode refers to the type of resource Terraform creates - either a resource ( managed ) or a data source ( data ). Tye type key refers to the resource type, in this case aci_aaep_to_domain is a resource available in the aci provider. The name of our resrouce is also stated here. Below in the instances section, we have attributes of our resource: annotaion, attachable_access_entity_profile_dn, description, domain_dn and id. All those parameters apart from id can be set in our resource configuration, the ID parameter is returned from APIC after resource creation. If you now run terraform state show aci_aaep_to_domain.AEP_L2_to_PHYSDOM_1 you would see the same parameters displayed in the CLI version. Terraform also marks dependencies between resources in state with the built-in dependency logic. In case of our aci_aaep_to_domain resoucre we have three dependencies to AEP, physical domain and vlan pool. Those dependencies are created based on depends_on attribute or by Terraform automatically. 2. Config drift Terraform usually only updates infrastructure if it does not match the coniguration. Let's see what happens when there is a manual change done on an existing object that was created by Terraform. We will work on the bd-network-1 resource that was created in the dcloud-tenant-1 . This resource was declared following: resource \"aci_bridge_domain\" \"bd-network-1\" { tenant_dn = aci_tenant.dcloud-tenant-1.id relation_fv_rs_ctx = aci_vrf.vrf1.id name = \"bd-network-1\" } Please login to APIC and go to Tenants -> dcloud-tenant-1 -> Networking -> Bridge Domains -> bd-network-1 and open the Policy tab. Change the L2 Unkown Unicast setting from Hardware Proxy to Flood and Submit changes: If you run terraform plan now you will see that no changes to the infrastructure are required. If display state of this object, you will see that it still has \"proxy\" value assigned: juchowan@JUCHOWAN-M-D2P2 ACI % terraform state show aci_bridge_domain.bd-network-1 # aci_bridge_domain.bd-network-1: resource \"aci_bridge_domain\" \"bd-network-1\" { <omitted> unk_mac_ucast_act = \"proxy\" unk_mcast_act = \"flood\" v6unk_mcast_act = \"flood\" vmac = \"not-applicable\" } The reason for that is that the attribute that was changed - unk_mac_ucast_act - was not set in Terraform configuration. Terraform will detect the config drift but it will not revert it. If we now run apply we will see that the parameter has been updated in the Terraform state: juchowan@JUCHOWAN-M-D2P2 ACI % terraform apply <omitted> No changes. Your infrastructure matches the configuration. Terraform has compared your real infrastructure against your configuration and found no differences, so no changes are needed. Apply complete! Resources: 0 added, 0 changed, 0 destroyed. juchowan@JUCHOWAN-M-D2P2 ACI-simulator % terraform state show aci_bridge_domain.bd-network-1 # aci_bridge_domain.bd-network-1: resource \"aci_bridge_domain\" \"bd-network-1\" { <omitted> unk_mac_ucast_act = \"flood\" unk_mcast_act = \"flood\" v6unk_mcast_act = \"flood\" vmac = \"not-applicable\" } If we would like to see those config drifts in the output of our plan and apply command, we would need to use the -refresh-only flag. juchowan@JUCHOWAN-M-D2P2 ACI % terraform plan -refresh-only Note: Objects have changed outside of Terraform Terraform detected the following changes made outside of Terraform since the last \"terraform apply\" which may have affected this plan: # aci_bridge_domain.bd-network-1 has changed ~ resource \"aci_bridge_domain\" \"bd-network-1\" { id = \"uni/tn-dcloud-tenant-1/BD-bd-network-1\" name = \"bd-network-1\" ~ unk_mac_ucast_act = \"proxy\" -> \"flood\" # (22 unchanged attributes hidden) } This is a refresh-only plan, so Terraform will not take any actions to undo these. If you were expecting these changes then you can apply this plan to record the updated values in the Terraform state without changing any remote objects. Let's now see what will happen if we explicitly set this parameter in the Terraform configuration. The current state is that our bridge domain has manually set \"flood\" option, and our state is in line with this. Let's add a new line to our bridge domain configuration. This config was done in the dcloud-tenant-1.tf file. Please add the line: resource \"aci_bridge_domain\" \"bd-network-1\" { tenant _d n = aci_ tenant .dcloud - tenant -1. id rela t io n _ f v_rs_c t x = aci_vr f .vr f 1. id na me = \"bd-network-1\" u n k_mac_ucas t _ac t = \"proxy\" } If you run plan and apply now, you will see the change needs to be done. Please apply that change and verify in APIC that L2 unknown unicast is changed to \"proxy\". At this point our state shows \"proxy\" for this bridge domain settings. Let's change back to flood on the APIC GUI: If we were to run plan now we will start seeing the change required: juchowan@JUCHOWAN-M-D2P2 ACI % terraform plan Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: ~ update in-place Terraform will perform the following actions: # aci_bridge_domain.bd-network-1 will be updated in-place ~ resource \"aci_bridge_domain\" \"bd-network-1\" { id = \"uni/tn-dcloud-tenant-1/BD-bd-network-1\" name = \"bd-network-1\" ~ unk_mac_ucast_act = \"flood\" -> \"proxy\" # (22 unchanged attributes hidden) } Plan: 0 to add, 1 to change, 0 to destroy. This is because attribute has been set in Terraform, so Terraform detects this drift and tries to revert it its expected condition. If we would like now to not revert it, but update Terraform state to reflect this change to \"flood\", we would require to change Terraform config to \"flood\" under resource block, and run terraform apply -refresh-only .","title":"Terraform State"},{"location":"terraform-state/#terraform-state","text":"","title":"Terraform State"},{"location":"terraform-state/#1-view-terraform-state","text":"Let's focus now on the state of our deployments. By now you should have three folders with different Terraform configuration. Please change to ACI folder in terraform terminal and run terraform state list command. The output should list all the objects that were deployed by Terraform modules and will look similar to this: juchowan@JUCHOWAN-M-D2P2 ACI % terraform state list aci_aaep_to_domain.AEP_L2_to_PHYSDOM_1 aci_access_port_block.leaf-101-102-eth-1-block aci_access_port_block.leaf-101-102-eth-2-block aci_access_port_selector.leaf-101-102-eth-1 aci_access_port_selector.leaf-101-102-eth-2 aci_application_epg.epg1 aci_application_profile.ap1 aci_attachable_access_entity_profile.AEP_L2 aci_attachable_access_entity_profile.AEP_L3 aci_bridge_domain.bd-network-1 aci_bridge_domain.bd_192_168_1_0 aci_bridge_domain.bd_192_168_2_0 aci_epg_to_domain.epg1-to-physdom_1 aci_epg_to_static_path.static-binding-1 aci_external_network_instance_profile.extepg aci_fabric_if_pol.LINK_10G aci_fabric_node_member.Leaf-101 aci_fabric_node_member.Leaf-102 aci_fabric_node_member.Spine-103 aci_fabric_node_member.Spine-104 aci_l3_domain_profile.EXTRTDOM_1 aci_l3_ext_subnet.extepg-subnet aci_l3_outside.l3out-1 aci_l3out_path_attachment.l3ip-path aci_lacp_policy.LACP_ACTIVE aci_leaf_access_bundle_policy_group.IntPolGrp_Router1 aci_leaf_access_bundle_policy_group.IntPolGrp_VPC_server1 aci_leaf_interface_profile.IntProf-101-102 aci_leaf_profile.SwProf-101-102 aci_leaf_selector.Sel-101-102 aci_lldp_interface_policy.LLDP_ON aci_logical_interface_profile.ip-101-102 aci_logical_node_profile.np-101-102 aci_logical_node_to_fabric_node.np-101-102-to-node-101 aci_logical_node_to_fabric_node.np-101-102-to-node-102 aci_node_block.Sel-101-102-Block aci_physical_domain.PHYSDOM_1 aci_ranges.RANGE_100-200 aci_ranges.RANGE_201-300 aci_subnet.subnet-1 aci_tenant.dcloud-tenant-1 aci_tenant.dcloud-tenant-2 aci_vlan_pool.VLAN_POOL_L2 aci_vlan_pool.VLAN_POOL_L3 aci_vrf.vrf1 aci_vrf.vrf2-1 You can see every object/resource we deployed in the previous labs. To see detailed configuration of each object run terraform state show <object name> , for example `terraform state show aci_bridge_domain.bd-network-1` You will get output similar to this: juchowan@JUCHOWAN-M-D2P2 ACI-simulator % terraform state show aci_bridge_domain.bd-network-1 # aci_bridge_domain.bd-network-1: resource \"aci_bridge_domain\" \"bd-network-1\" { annotation = \"orchestrator:terraform\" arp_flood = \"no\" bridge_domain_type = \"regular\" ep_clear = \"no\" ep_move_detect_mode = \"disable\" host_based_routing = \"no\" id = \"uni/tn-dcloud-tenant-1/BD-bd-network-1\" intersite_bum_traffic_allow = \"no\" intersite_l2_stretch = \"no\" ip_learning = \"yes\" ipv6_mcast_allow = \"no\" limit_ip_learn_to_subnets = \"yes\" ll_addr = \"::\" mac = \"00:22:BD:F8:19:FF\" mcast_allow = \"no\" multi_dst_pkt_act = \"bd-flood\" name = \"bd-network-1\" optimize_wan_bandwidth = \"no\" relation_fv_rs_ctx = \"uni/tn-dcloud-tenant-1/ctx-vrf1\" tenant_dn = \"uni/tn-dcloud-tenant-1\" unicast_route = \"yes\" unk_mac_ucast_act = \"proxy\" unk_mcast_act = \"flood\" v6unk_mcast_act = \"flood\" vmac = \"not-applicable\" } You can see the current state of this bridge domain and configuration of many parameters, even the ones we did not specify in the configuration. The reason we have these parameters here, is that they took the default value for bridge domain. The id element is the path of the bridge domain that was returned to us from ACI after we created this bridge domain. You can in the same way check the state of other objects that we deployed. Terraform stores information about our infrastructure in a state file. This state file keeps track of resources created by our configuration and maps them to real-world resources. The state file by default is located in the main directory of our workspace and is called terraform.tfstate . If you list all the contents of our workspace folder you will see that this file exists, and is created after we apply our config: juchowan@JUCHOWAN-M-D2P2 ACI-simulator % ls | grep tfstate terraform.tfstate terraform.tfstate.backup There is also a backup file created of our state. Those files should not be changed manually to avoid drift between our Terraform configuration, state and infrastructure. Open terraform.tfstate file in a file editor. You will see that this file is in the JSON format. The first stanza contains information about our Terraform application { \"version\": 4, \"terraform_version\": \"1.3.2\", \"serial\": 577, \"lineage\": \"8c71daed-6a43-a3c1-3981-9ce1f9789eb6\", \"outputs\": {}, \"resources\": [ Below that is the resources section of the state file, and it contains the schema for any resource that was created in Terraform. Example resource from the state file: { \"mode\": \"managed\", \"type\": \"aci_aaep_to_domain\", \"name\": \"AEP_L2_to_PHYSDOM_1\", \"provider\": \"provider[\\\"registry.terraform.io/ciscodevnet/aci\\\"]\", \"instances\": [ { \"schema_version\": 1, \"attributes\": { \"annotation\": \"orchestrator:terraform\", \"attachable_access_entity_profile_dn\": \"uni/infra/attentp-AEP_L2\", \"description\": null, \"domain_dn\": \"uni/phys-PHYSDOM_1\", \"id\": \"uni/infra/attentp-AEP_L2/rsdomP-[uni/phys-PHYSDOM_1]\" }, \"sensitive_attributes\": [], \"private\": \"eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ==\", \"dependencies\": [ \"aci_attachable_access_entity_profile.AEP_L2\", \"aci_physical_domain.PHYSDOM_1\", \"aci_vlan_pool.VLAN_POOL_L2\" ] } ] }, The first key is mode . Mode refers to the type of resource Terraform creates - either a resource ( managed ) or a data source ( data ). Tye type key refers to the resource type, in this case aci_aaep_to_domain is a resource available in the aci provider. The name of our resrouce is also stated here. Below in the instances section, we have attributes of our resource: annotaion, attachable_access_entity_profile_dn, description, domain_dn and id. All those parameters apart from id can be set in our resource configuration, the ID parameter is returned from APIC after resource creation. If you now run terraform state show aci_aaep_to_domain.AEP_L2_to_PHYSDOM_1 you would see the same parameters displayed in the CLI version. Terraform also marks dependencies between resources in state with the built-in dependency logic. In case of our aci_aaep_to_domain resoucre we have three dependencies to AEP, physical domain and vlan pool. Those dependencies are created based on depends_on attribute or by Terraform automatically.","title":"1. View Terraform State"},{"location":"terraform-state/#2-config-drift","text":"Terraform usually only updates infrastructure if it does not match the coniguration. Let's see what happens when there is a manual change done on an existing object that was created by Terraform. We will work on the bd-network-1 resource that was created in the dcloud-tenant-1 . This resource was declared following: resource \"aci_bridge_domain\" \"bd-network-1\" { tenant_dn = aci_tenant.dcloud-tenant-1.id relation_fv_rs_ctx = aci_vrf.vrf1.id name = \"bd-network-1\" } Please login to APIC and go to Tenants -> dcloud-tenant-1 -> Networking -> Bridge Domains -> bd-network-1 and open the Policy tab. Change the L2 Unkown Unicast setting from Hardware Proxy to Flood and Submit changes: If you run terraform plan now you will see that no changes to the infrastructure are required. If display state of this object, you will see that it still has \"proxy\" value assigned: juchowan@JUCHOWAN-M-D2P2 ACI % terraform state show aci_bridge_domain.bd-network-1 # aci_bridge_domain.bd-network-1: resource \"aci_bridge_domain\" \"bd-network-1\" { <omitted> unk_mac_ucast_act = \"proxy\" unk_mcast_act = \"flood\" v6unk_mcast_act = \"flood\" vmac = \"not-applicable\" } The reason for that is that the attribute that was changed - unk_mac_ucast_act - was not set in Terraform configuration. Terraform will detect the config drift but it will not revert it. If we now run apply we will see that the parameter has been updated in the Terraform state: juchowan@JUCHOWAN-M-D2P2 ACI % terraform apply <omitted> No changes. Your infrastructure matches the configuration. Terraform has compared your real infrastructure against your configuration and found no differences, so no changes are needed. Apply complete! Resources: 0 added, 0 changed, 0 destroyed. juchowan@JUCHOWAN-M-D2P2 ACI-simulator % terraform state show aci_bridge_domain.bd-network-1 # aci_bridge_domain.bd-network-1: resource \"aci_bridge_domain\" \"bd-network-1\" { <omitted> unk_mac_ucast_act = \"flood\" unk_mcast_act = \"flood\" v6unk_mcast_act = \"flood\" vmac = \"not-applicable\" } If we would like to see those config drifts in the output of our plan and apply command, we would need to use the -refresh-only flag. juchowan@JUCHOWAN-M-D2P2 ACI % terraform plan -refresh-only Note: Objects have changed outside of Terraform Terraform detected the following changes made outside of Terraform since the last \"terraform apply\" which may have affected this plan: # aci_bridge_domain.bd-network-1 has changed ~ resource \"aci_bridge_domain\" \"bd-network-1\" { id = \"uni/tn-dcloud-tenant-1/BD-bd-network-1\" name = \"bd-network-1\" ~ unk_mac_ucast_act = \"proxy\" -> \"flood\" # (22 unchanged attributes hidden) } This is a refresh-only plan, so Terraform will not take any actions to undo these. If you were expecting these changes then you can apply this plan to record the updated values in the Terraform state without changing any remote objects. Let's now see what will happen if we explicitly set this parameter in the Terraform configuration. The current state is that our bridge domain has manually set \"flood\" option, and our state is in line with this. Let's add a new line to our bridge domain configuration. This config was done in the dcloud-tenant-1.tf file. Please add the line: resource \"aci_bridge_domain\" \"bd-network-1\" { tenant _d n = aci_ tenant .dcloud - tenant -1. id rela t io n _ f v_rs_c t x = aci_vr f .vr f 1. id na me = \"bd-network-1\" u n k_mac_ucas t _ac t = \"proxy\" } If you run plan and apply now, you will see the change needs to be done. Please apply that change and verify in APIC that L2 unknown unicast is changed to \"proxy\". At this point our state shows \"proxy\" for this bridge domain settings. Let's change back to flood on the APIC GUI: If we were to run plan now we will start seeing the change required: juchowan@JUCHOWAN-M-D2P2 ACI % terraform plan Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: ~ update in-place Terraform will perform the following actions: # aci_bridge_domain.bd-network-1 will be updated in-place ~ resource \"aci_bridge_domain\" \"bd-network-1\" { id = \"uni/tn-dcloud-tenant-1/BD-bd-network-1\" name = \"bd-network-1\" ~ unk_mac_ucast_act = \"flood\" -> \"proxy\" # (22 unchanged attributes hidden) } Plan: 0 to add, 1 to change, 0 to destroy. This is because attribute has been set in Terraform, so Terraform detects this drift and tries to revert it its expected condition. If we would like now to not revert it, but update Terraform state to reflect this change to \"flood\", we would require to change Terraform config to \"flood\" under resource block, and run terraform apply -refresh-only .","title":"2. Config drift"},{"location":"terraform-usecases/","text":"Basic use-cases with Terraform In this lab the focus will be on using Terraform as a way to provide operational rigor to your network device configurations. You will learn how to install and use Terraform on Cisco network solutions like ACI. 1. Install Visual Studio Code Terraform uses structured files that require editing. To assist you in this process the best tool is some editor that understands the Terraform semantics. For this lab you will be using a web based integrated development environment that uses the code that is inside Microsofts Visual Studio Code. When Microsoft wrote Visual Studio Code it built the IDE on top of a platform called electron. This allowed for cross platform development and is based on javascript. Visual Studio Code can be installed on variety of operating systems, please download the package suitable for your environment and follow the installation process in the wizard: https://code.visualstudio.com/ Visual Studio Code has three panes that you will be using: - The left pane with the files - The right pane with the file contents - The bottom pane will be leveraging the Terminal to issue commands 1.1 Open a new terminal in the IDE To get started, first you have to open a terminal windows in the IDE to access the underlying operating system and work the lab. On the menu bar click the Terminal tab to open New Terminal Terminal will open at the bottom of the screen. This will be the area that you will execute all the terraform commands after making changes to terraform files. 2 Create the working directory Terraform uses directory structures as a way to organize its automation structure. This is due to the fact that Terraform treats everything in a directory as a unit and reads all the .tf files before execution. The first step is to create a directory called ACI for the terraform files. Using the IDE you can create folders. This directory will live under the ACI folder. There are various ways to create these in visual studio code: using the contextual menu (1a and 1b) or using the icons (2): In that directory create the first terraform file called main.tf using the icon: 3. Configure ACI Provider One of the first things we should do before writing configuration for ACI is to add the Terraform Provider for ACI. The definition will be placed in the main.tf file and we will use the username/password construct. It is also possible and recommended to use certificates based authentication, but for this lab we are using the simpler method. ACI Provider documentation can be found on the official Terraform Registry website: https://registry.terraform.io/providers/CiscoDevNet/aci/latest/docs In also includes instruction on how to use this provider. After clicking on \"Use provider\" small section with code will appear. The code that we will use in this lab contains IP address of APIC in the lab and default admin user/password. It should be copied to the main.tf file: terraform { required_providers { aci = { source = \"CiscoDevNet/aci\" version = \"2.5.2\" } } } provider \"aci\" { # Cisco ACI user name username = \"admin\" password = \"C1sco12345\" url = \"https://198.18.133.200\" insecure = true } 3.1 Run Terraform Init Once the provider is configured and our APIC IP and credentials are present in the file, we can proceed with the first step of Terraform workflow and initialize the directory to include proper terraform provider modules needed for execution. In the terminal window on the bottom pane of Visual Studio make sure you are in the correct directory (your ACI folder) and then execute terraform init . The output should show that the provider plugin hsa been downlaoded: juchowan@JUCHOWAN-M-D2P2 ACI-simulator % terraform init Initializing the backend... Initializing provider plugins... - Finding ciscodevnet/aci versions matching \"2.5.2\"... - Installing ciscodevnet/aci v2.5.2... - Installed ciscodevnet/aci v2.5.2 (signed by a HashiCorp partner, key ID 433649E2C56309DE) Partner and community providers are signed by their developers. If you'd like to know more about provider signing, you can read about it here: https://www.terraform.io/docs/cli/plugins/signing.html Terraform has created a lock file .terraform.lock.hcl to record the provider selections it made above. Include this file in your version control repository so that Terraform can guarantee to make the same selections by default when you run \"terraform init\" in the future. Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. Next you can try running terraform plan , but since our main.tf file has no resource confguration, you will see that there is no change needed: juchowan@JUCHOWAN-M-D2P2 ACI-simulator % terraform plan No changes. Your infrastructure matches the configuration. Terraform has compared your real infrastructure against your configuration and found no differences, so no changes are needed. 3.2 (Optional) Switch registration At this point leaf and spine switches should be registered in the lab using Postman the day before. Registration of switch is possible to be done using resource aci_fabric_node_member . Documentation of this resource is present in the official Terraform Registry documentation . - In the Example Usage section you can find example code to register new node - In the Argument Reference you will see possible arguments/parameters that can be used for switch registration with indication if argument is required or optional and additional information about it - In the Attribute Reference you will see what is the attribute that this resource exports, and in case of ACI resources it will be always id set to the DN of the VLAN Pool. - Importing describes how to import existing object to the terraform resource For reference the following resource configuration in Terraform would register new switch. If the switches are already registered, this task can be skipped resource \"aci_fabric_node_member\" \"Leaf-101\" { name = \"Leaf-101\" serial = \"TEP-1-101\" node_id = \"101\" pod_id = \"1\" role = \"leaf\" } resource \"aci_fabric_node_member\" \"Leaf-102\" { name = \"Leaf-102\" serial = \"TEP-1-102\" node_id = \"102\" pod_id = \"1\" role = \"leaf\" } resource \"aci_fabric_node_member\" \"Spine-103\" { name = \"Spine-103\" serial = \"TEP-1-103\" node_id = \"103\" pod_id = \"1\" role = \"spine\" } resource \"aci_fabric_node_member\" \"Spine-104\" { name = \"Spine-104\" serial = \"TEP-1-104\" node_id = \"104\" pod_id = \"1\" role = \"spine\" } 4 ACI Access Policies Every ACI Fabric configuration starts with Access Polices. You need to define ACI objects which represent single physical port configurations. To remind relationships between objects, please check following figure. Usually Interface Polices are configured once and re-used like LLDP, LACP, CDP, etc. In our LAB some of the policies already exist from the day before and Postman configuration, so different names should be used for Terraform lab to create new objects. Following sections will help you to prepare your API requests. For reference full configuration of access-policies can be found here: Terraform Tenant Ready . 4.1 Interface Policies You will configure LACP Policy, LLDP and speed policies. Let's separate our terraform files and create separate config file for out interface policies. Under your folder in Visual Studio create new file calles interface-policies.tf : Copy each of the Policy resource definition to the new interface-policies.tf file LACP_ACTIVE resource \"aci_lacp_policy\" \"LACP_ACTIVE\" { na me = \"LACP_ACTIVE\" c trl = [ \"susp-individual\" , \"fast-sel-hot-stdby\" , \"graceful-conv\" ] mode = \"active\" } LLDP_ON resource \"aci_lldp_interface_policy\" \"LLDP_ON\" { na me = \"LLDP_ON\" admi n _rx_s t = \"enabled\" admi n _ t x_s t = \"enabled\" } LINK-10G resource \"aci_fabric_if_pol\" \"LINK-10G\" { na me = \"LINK-10G\" au t o_ ne g = \"on\" speed = \"10G\" } The documentation for each of the resources can be found in Terraform Registry: aci_lacp_policy resource aci_lldp_interface_policy resource aci_fabric_if_pol resource 4.2 Run Terraform Plan & Apply At this point we have our first three resources in the configuration so it's time to deploy them on our APIC. To do so we will follow the Terraform workload init -> plan -> apply. In previous step, after configuring ACI provider you should have already done terraform init , so it is not needed to run it again now, as it would not bring any change. Our first step to deploy config is to run terraform plan command in the terminal window. The output will show us what changes Terraform needs to do on the fabric, to bring it to the required configuration. The output should look similar to the following: juchowan@JUCHOWAN-M-D2P2 ACI-simulator % terraform plan aci_fabric_node_member.Leaf-102: Refreshing state... [id=uni/controller/nodeidentpol/nodep-TEP-1-102] aci_fabric_node_member.Leaf-101: Refreshing state... [id=uni/controller/nodeidentpol/nodep-TEP-1-101] aci_fabric_node_member.Spine-103: Refreshing state... [id=uni/controller/nodeidentpol/nodep-TEP-1-103] aci_fabric_node_member.Spine-104: Refreshing state... [id=uni/controller/nodeidentpol/nodep-TEP-1-104] Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # aci_fabric_if_pol.auto-10G will be created + resource \"aci_fabric_if_pol\" \"auto-10G\" { + annotation = \"orchestrator:terraform\" + auto_neg = \"on\" + description = (known after apply) + fec_mode = (known after apply) + id = (known after apply) + link_debounce = (known after apply) + name = \"LINK-10G\" + name_alias = (known after apply) + speed = \"10G\" } # aci_lldp_interface_policy.LLDP_ON will be created + resource \"aci_lldp_interface_policy\" \"LLDP_ON\" { + admin_rx_st = \"enabled\" + admin_tx_st = \"enabled\" + annotation = \"orchestrator:terraform\" + description = (known after apply) + id = (known after apply) + name = \"LLDP_ON\" + name_alias = (known after apply) } # aci_lacp_policy.LACP_ACTIVE will be created + resource \"aci_lacp_policy\" \"LACP_ACTIVE\" { + annotation = \"orchestrator:terraform\" + ctrl = [ + \"susp-individual\", + \"fast-sel-hot-stdby\", + \"graceful-conv\", ] + description = \"done by terraform\" + id = (known after apply) + max_links = (known after apply) + min_links = (known after apply) + mode = \"active\" + name = \"LACP_ACTIVE\" + name_alias = (known after apply) } Plan: 3 to add, 0 to change, 0 to destroy. \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Note: You didn't use the -out option to save this plan, so Terraform can't guarantee to take exactly these actions if you run \"terraform apply\" now. In the plan you can see all the plus signs beside the elements that are going to be added to the fabric. Terraform read the fabric state and compared with required configuration, and then decided which elements need to be added. At this point no configuration has been added yet to the fabric, Terraform only lists the changes it is planning to do. As the Note says at the bottom, this plan was not saved to a file, but only displayed in the console. If you change command to terraform plan -out interface-policies.plan you will see that a file was generated with the contents of the plan. Now you can perform the terraform apply command. This command will run plan again, ask for approval and then it will go into the fabric and perform these actions. <plan omitted> Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes aci_lldp_interface_policy.LLDP_ON: Creating... aci_fabric_if_pol.auto-10G: Creating... aci_lacp_policy.LACP_ACTIVE: Creating... aci_lldp_interface_policy.LLDP_ON: Creation complete after 0s [id=uni/infra/lldpIfP-LLDP_ON] aci_fabric_if_pol.auto-10G: Creation complete after 0s [id=uni/infra/hintfpol-LINK-10G] aci_lacp_policy.LACP_ACTIVE: Creation complete after 0s [id=uni/infra/lacplagp-LACP_ACTIVE] Apply complete! Resources: 3 added, 0 changed, 0 destroyed. If you run plan command again now, you would see the output is different because now it will only show the changes it plans to do over the previous push. Since there are no changes in the config yet, the plan will not show anything new to be added: juchowan@JUCHOWAN-M-D2P2 ACI-simulator % terraform plan aci_fabric_node_member.Spine-103: Refreshing state... [id=uni/controller/nodeidentpol/nodep-TEP-1-103] aci_fabric_node_member.Leaf-102: Refreshing state... [id=uni/controller/nodeidentpol/nodep-TEP-1-102] aci_lacp_policy.LACP_ACTIVE: Refreshing state... [id=uni/infra/lacplagp-LACP_ACTIVE] aci_fabric_if_pol.auto-10G: Refreshing state... [id=uni/infra/hintfpol-LINK-10G] aci_fabric_node_member.Spine-104: Refreshing state... [id=uni/controller/nodeidentpol/nodep-TEP-1-104] aci_lldp_interface_policy.LLDP_ON: Refreshing state... [id=uni/infra/lldpIfP-LLDP_ON] aci_fabric_node_member.Leaf-101: Refreshing state... [id=uni/controller/nodeidentpol/nodep-TEP-1-101] No changes. Your infrastructure matches the configuration. 4.3 VLANs, Domains and AAEPs Create VLAN pool with vlan-range, associate it with Physical Domain and AAEP. Let's put these policies in a separate, new file called access-policies.tf We will create two VLAN pools, one for L2 Physical Domain, and second one for L3 Routed Domain. In the same way we will create two AEP, one for L2 connections and second one for L3 connections. Each AEP will map one domain: - VLAN_POOL_L2 <-- PHYSDOM_1 <-- AEP_L2 - VLAN_POOL_L3 <-- EXTRTDOM_1 <-- AEP_L3 For VLAN pool there are two resources that need to be created: VLAN Pool and VLAN Range. In this case VLAN Range object is a child of VLAN Pool object, which means that the Range resource needs to have a reference to the VLAN Pool resource. This reference can be done in terraform by using the resource name followed by the attribute, in this case id . resource \"aci_vlan_pool\" \"VLAN_POOL_L2\" { name = \"VLAN_POOL_STATIC\" alloc_mode = \"static\" } resource \"aci_ranges\" \"RANGE_100-200\" { vlan_pool_dn = aci_vlan_pool.VLAN_POOL_L2.id from = \"vlan-100\" to = \"vlan-200\" alloc_mode = \"inherit\" } resource \"aci_vlan_pool\" \"VLAN_POOL_L3\" { name = \"VLAN_POOL_STATIC\" alloc_mode = \"static\" } resource \"aci_ranges\" \"RANGE_201-300\" { vlan_pool_dn = aci_vlan_pool.VLAN_POOL_L3.id from = \"vlan-201\" to = \"vlan-300\" alloc_mode = \"inherit\" } Two domains, physical and routed, will be created as two resources, and each of them needs to have a reference to a VLAN Pool. This reference will be done by special attribute relation_infra_rs_vlan_ns , by referencing the resource name and id attribute: resource \"aci_physical_domain\" \"PHYSDOM_1\" { name = \"PHYSDOM_1\" relation_infra_rs_vlan_ns = aci_vlan_pool.VLAN_POOL_L2.id } resource \"aci_l3_domain_profile\" \"EXTRTDOM_1\" { name = \"EXTRTDOM_1\" relation_infra_rs_vlan_ns = aci_vlan_pool.VLAN_POOL_L3.id } Two AAEP will be created, one for L2 connections and another one for L3 connections. Between AAEP and Domains there is also relationship. AEP needs to be related to a domain. This relation can be done in two ways: separate resource aci_aaep_to_domain which is shown in the example below for AEP_L2 and PHYSDOM_1 relation. This resource needs to reference id values of both AEP and Physical Domain argument within aci_attachable_access_entity_profile called relation_infra_rs_dom_p which is shown in the example below for AEP_L3 and EXTRTDOM_1 relation. This argument needs to reference id values of both AEP and Routed Domain, but needs to be embedded in [] as the documentation says it's Type -[Set of String] resource \"aci_attachable_access_entity_profile\" \"AEP_L2\" { description = \"AAEP for L2 connections\" name = \"AEP_L2\" } resource \"aci_aaep_to_domain\" \"AEP_L2_to_PHYSDOM_1\" { attachable_access_entity_profile_dn = aci_attachable_access_entity_profile.AEP_L2.id domain_dn = aci_physical_domain.PHYSDOM_1.id } resource \"aci_attachable_access_entity_profile\" \"AEP_L3\" { description = \"AAEP for L3 connections\" name = \"AEP_L3\" relation_infra_rs_dom_p = [aci_l3_domain_profile.EXTRTDOM_1.id] } Documentation of resources: aci_vlan_pool resource aci_ranges resource aci_physical_domain resource aci_l3_domain_profile resource aci_attachable_access_entity_profile resource aci_aaep_to_domain resource 4.4 Interface Policy group Let's now create two Interface Policy Groups, one for server (L2) and another one for router (L3) and use all created policies: IntPolGrp_VPC_server1 VPC policy with AEP_L2 IntPolGrp_Router1 access port policy with AEP_L3 Those interface policy groups will have relation to all the access policies we created before by referencing the id of those policies: resource \"aci_leaf_access_bundle_policy_group\" \"IntPolGrp_VPC_server1\" { name = \"IntPolGrp_VPC_server1\" lag_t = \"node\" relation_infra_rs_lldp_if_pol = aci_lldp_interface_policy.LLDP_ON.id relation_infra_rs_lacp_pol = aci_lacp_policy.LACP_ACTIVE.id relation_infra_rs_h_if_pol = aci_fabric_if_pol.LINK_10G.id relation_infra_rs_att_ent_p = aci_attachable_access_entity_profile.AEP_L2.id } resource \"aci_leaf_access_bundle_policy_group\" \"IntPolGrp_Router1\" { name = \"IntPolGrp_Router1\" lag_t = \"node\" relation_infra_rs_lldp_if_pol = aci_lldp_interface_policy.LLDP_ON.id relation_infra_rs_lacp_pol = aci_lacp_policy.LACP_ACTIVE.id relation_infra_rs_h_if_pol = aci_fabric_if_pol.LINK_10G.id relation_infra_rs_att_ent_p = aci_attachable_access_entity_profile.AEP_L3.id } Documentation of resources: aci_leaf_access_bundle_policy_group resource aci_leaf_access_port_policy_group resource 4.5 Switch & Interface Profiles Let's assign our created interface policy groups to the newly created switch and interface profiles. We will create: switch profile interface profile IntProf-101-102 with two interface selectors eth1/1 connecting server eth1/2 connecting router First we create interface profile within this profile two interface selectors. Interface selectors need to have reference to the Interface Profile ( IntProf-101-102 ) and relation to the interface policy groups created for our end devices: IntPolGrp_VPC_server1 IntPolGrp_Router1 resource \"aci_leaf_interface_profile\" \"IntProf-101-102\" { name = \"IntProf-101-102\" } resource \"aci_access_port_selector\" \"leaf-101-102-eth-1\" { leaf_interface_profile_dn = aci_leaf_interface_profile.IntProf-101-102.id name = \"leaf-101-102-eth-1\" access_port_selector_type = \"range\" relation_infra_rs_acc_base_grp = aci_leaf_access_bundle_policy_group.IntPolGrp_VPC_server1.id } resource \"aci_access_port_block\" \"leaf-101-102-eth-1-block\" { access_port_selector_dn = aci_access_port_selector.leaf-101-102-eth-1.id name = \"leaf-101-102-eth-1-block\" from_card = \"1\" from_port = \"1\" to_card = \"1\" to_port = \"1\" } resource \"aci_access_port_selector\" \"leaf-101-102-eth-2\" { leaf_interface_profile_dn = aci_leaf_interface_profile.IntProf-101-102.id name = \"leaf-101-102-eth-2\" access_port_selector_type = \"range\" relation_infra_rs_acc_base_grp = aci_leaf_access_bundle_policy_group.IntPolGrp_Router1.id } resource \"aci_access_port_block\" \"leaf-101-102-eth-2-block\" { access_port_selector_dn = aci_access_port_selector.leaf-101-102-eth-2.id name = \"leaf-101-102-eth-2-block\" from_card = \"1\" from_port = \"2\" to_card = \"1\" to_port = \"2\" } Next we have our switch profile configuration which needs to have a reference to the leaf interface profile ( IntProf-101-102 ) as well as mark the nodes which will be used in this switch profile (nodes 101 & 102) resource \"aci_leaf_profile\" \"SwProf-101-102\" { name = \"SwProf-101-102\" relation_infra_rs_acc_port_p = [aci_leaf_interface_profile.IntProf-101-102.id] } resource \"aci_leaf_selector\" \"Sel-101-102\" { leaf_profile_dn = aci_leaf_profile.SwProf-101-102.id name = \"Sel-101-102\" switch_association_type = \"range\" } resource \"aci_node_block\" \"Sel-101-102-Block\" { switch_association_dn = aci_leaf_selector.Sel-101-102.id name = \"Sel-101-102-Block\" from_ = \"101\" to_ = \"102\" } 5 ACI Tenant Upon now you created ACI AccessPolicies for your Tenant. Now is time to create your tenant using Terraform. It will be simple Tenant definition with one VRF, one bridge domains associated with one EPG and one L3out. EPG will be associated with L2 domain and statically binded to VPC created, with VLAN from L2 VLAN pool done by you perviously. The L3out will be associated to L3 domain and router interface, with VLAN from L3 VLAN pool. Figure below shows our Logical Tenant. You need to define Tenant, VRF, Bridge-Domain, Application-Profile, EPG with Domain association, static binding under EPG and L3out. Quite many of resources to do in Terraform. We will define all of them in a single, new config file called dcloud-tenant-1.tf Please, download a terraform file from Terraform Tenant Ready . 5.1 Tenant components This section contains Terraform codes necessary to create Tenant objects 5.1.1 Tenant and VRF Following code can be used to create new Tenant and VRFs. The VRF resource needs to reference its parent object - tenant id. resource \"aci_tenant\" \"dcloud-tenant-1\" { name = \"dcloud-tenant-1\" } resource \"aci_vrf\" \"vrf1\" { tenant_dn = aci_tenant.dcloud-tenant-1.id name = \"vrf1\" } 5.1.2 BD in existing tenant Following code can be used to create new Bridge Domain in existing Tenant. Bridge Domain needs to have a reference to parent objects: tenant ID and VRF ID. There is also a child object of bridge domain which is a Subnet with reference to bridge domain ID. resource \"aci_bridge_domain\" \"bd-network-1\" { tenant_dn = aci_tenant.dcloud-tenant-1.id relation_fv_rs_ctx = aci_vrf.vrf1.id name = \"bd-network-1\" } resource \"aci_subnet\" \"subnet-1\" { parent_dn = aci_bridge_domain.bd-network-1.id ip = \"10.0.0.1/24\" scope = [\"public\"] } 5.1.3 Application Profile with EPG and static bindings Following code can be used to create new Application Profile with EPG. Application Profile needs a reference to parent object tenant ID, while EPG needs reference to AP ID ad Bridge Domain ID. The next two resources reflect the physical domain PHYSDOM_1 attached to that EPG, and one static binding towards our server1 with encapsulation of vlan 100. resource \"aci_application_profile\" \"ap1\" { tenant_dn = aci_tenant.dcloud-tenant-1.id name = \"ap1\" } resource \"aci_application_epg\" \"epg1\" { application_profile_dn = aci_application_profile.ap1.id name = \"epg1\" relation_fv_rs_bd = aci_bridge_domain.bd-network-1.id } resource \"aci_epg_to_domain\" \"epg1-to-physdom_1\" { application_epg_dn = aci_application_epg.epg1.id tdn = aci_physical_domain.PHYSDOM_1.id } resource \"aci_epg_to_static_path\" \"static-binding-1\" { application_epg_dn = aci_application_epg.epg1.id tdn = \"topology/pod-1/protpaths-101-102/pathep-[IntPolGrp_VPC_server1]\" encap = \"vlan-100\" mode = \"regular\" } 5.1.4 L3Out Following code can be used to create new L3out. As you can see there are plenty of resources which reflect node profile and node mapping, interface profile and path used for it, external EPG with subnet. resource \"aci_l3_outside\" \"l3out-1\" { tenant_dn = aci_tenant.dcloud-tenant-1.id name = \"l3out-1\" relation_l3ext_rs_l3_dom_att = aci_l3_domain_profile.EXTRTDOM_1.id relation_l3ext_rs_ectx = aci_vrf.vrf1.id } resource \"aci_logical_node_profile\" \"np-101-102\" { l3_outside_dn = aci_l3_outside.l3out-1.id name = \"np-101-102\" } resource \"aci_logical_node_to_fabric_node\" \"np-101-102-to-node-101\" { logical_node_profile_dn = aci_logical_node_profile.np-101-102.id tdn = \"topology/pod-1/node-101\" rtr_id = \"1.1.1.1\" } resource \"aci_logical_node_to_fabric_node\" \"np-101-102-to-node-102\" { logical_node_profile_dn = aci_logical_node_profile.np-101-102.id tdn = \"topology/pod-1/node-102\" rtr_id = \"1.1.1.2\" } resource \"aci_logical_interface_profile\" \"ip-101-102\" { logical_node_profile_dn = aci_logical_node_profile.np-101-102.id name = \"ip-101-102\" } resource \"aci_l3out_path_attachment\" \"l3ip-path\" { logical_interface_profile_dn = aci_logical_interface_profile.ip-101-102.id target_dn = \"topology/pod-1/protpaths-101-102/pathep-[IntPolGrp_Router1]\" if_inst_t = \"ext-svi\" addr = \"10.1.1.1/24\" encap = \"vlan-201\" encap_scope = \"ctx\" mode = \"regular\" mtu = \"inherit\" } resource \"aci_external_network_instance_profile\" \"extepg\" { l3_outside_dn = aci_l3_outside.l3out-1.id name = \"extepg\" } resource \"aci_l3_ext_subnet\" \"extepg-subnet\" { external_network_instance_profile_dn = aci_external_network_instance_profile.extepg.id ip = \"0.0.0.0/0\" scope = [\"import-security\",\"export-rtctrl\"] } Documentation of resources: aci_tenant resource aci_bridge_domain aci_subnet resource aci_application_profile aci_application_epg aci_epg_to_domain aci_l3_outside","title":"Basic use-cases with Terraform"},{"location":"terraform-usecases/#basic-use-cases-with-terraform","text":"In this lab the focus will be on using Terraform as a way to provide operational rigor to your network device configurations. You will learn how to install and use Terraform on Cisco network solutions like ACI.","title":"Basic use-cases with Terraform"},{"location":"terraform-usecases/#1-install-visual-studio-code","text":"Terraform uses structured files that require editing. To assist you in this process the best tool is some editor that understands the Terraform semantics. For this lab you will be using a web based integrated development environment that uses the code that is inside Microsofts Visual Studio Code. When Microsoft wrote Visual Studio Code it built the IDE on top of a platform called electron. This allowed for cross platform development and is based on javascript. Visual Studio Code can be installed on variety of operating systems, please download the package suitable for your environment and follow the installation process in the wizard: https://code.visualstudio.com/ Visual Studio Code has three panes that you will be using: - The left pane with the files - The right pane with the file contents - The bottom pane will be leveraging the Terminal to issue commands","title":"1. Install Visual Studio Code"},{"location":"terraform-usecases/#11-open-a-new-terminal-in-the-ide","text":"To get started, first you have to open a terminal windows in the IDE to access the underlying operating system and work the lab. On the menu bar click the Terminal tab to open New Terminal Terminal will open at the bottom of the screen. This will be the area that you will execute all the terraform commands after making changes to terraform files.","title":"1.1 Open a new terminal in the IDE"},{"location":"terraform-usecases/#2-create-the-working-directory","text":"Terraform uses directory structures as a way to organize its automation structure. This is due to the fact that Terraform treats everything in a directory as a unit and reads all the .tf files before execution. The first step is to create a directory called ACI for the terraform files. Using the IDE you can create folders. This directory will live under the ACI folder. There are various ways to create these in visual studio code: using the contextual menu (1a and 1b) or using the icons (2): In that directory create the first terraform file called main.tf using the icon:","title":"2 Create the working directory"},{"location":"terraform-usecases/#3-configure-aci-provider","text":"One of the first things we should do before writing configuration for ACI is to add the Terraform Provider for ACI. The definition will be placed in the main.tf file and we will use the username/password construct. It is also possible and recommended to use certificates based authentication, but for this lab we are using the simpler method. ACI Provider documentation can be found on the official Terraform Registry website: https://registry.terraform.io/providers/CiscoDevNet/aci/latest/docs In also includes instruction on how to use this provider. After clicking on \"Use provider\" small section with code will appear. The code that we will use in this lab contains IP address of APIC in the lab and default admin user/password. It should be copied to the main.tf file: terraform { required_providers { aci = { source = \"CiscoDevNet/aci\" version = \"2.5.2\" } } } provider \"aci\" { # Cisco ACI user name username = \"admin\" password = \"C1sco12345\" url = \"https://198.18.133.200\" insecure = true }","title":"3. Configure ACI Provider"},{"location":"terraform-usecases/#31-run-terraform-init","text":"Once the provider is configured and our APIC IP and credentials are present in the file, we can proceed with the first step of Terraform workflow and initialize the directory to include proper terraform provider modules needed for execution. In the terminal window on the bottom pane of Visual Studio make sure you are in the correct directory (your ACI folder) and then execute terraform init . The output should show that the provider plugin hsa been downlaoded: juchowan@JUCHOWAN-M-D2P2 ACI-simulator % terraform init Initializing the backend... Initializing provider plugins... - Finding ciscodevnet/aci versions matching \"2.5.2\"... - Installing ciscodevnet/aci v2.5.2... - Installed ciscodevnet/aci v2.5.2 (signed by a HashiCorp partner, key ID 433649E2C56309DE) Partner and community providers are signed by their developers. If you'd like to know more about provider signing, you can read about it here: https://www.terraform.io/docs/cli/plugins/signing.html Terraform has created a lock file .terraform.lock.hcl to record the provider selections it made above. Include this file in your version control repository so that Terraform can guarantee to make the same selections by default when you run \"terraform init\" in the future. Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. Next you can try running terraform plan , but since our main.tf file has no resource confguration, you will see that there is no change needed: juchowan@JUCHOWAN-M-D2P2 ACI-simulator % terraform plan No changes. Your infrastructure matches the configuration. Terraform has compared your real infrastructure against your configuration and found no differences, so no changes are needed.","title":"3.1 Run Terraform Init"},{"location":"terraform-usecases/#32-optional-switch-registration","text":"At this point leaf and spine switches should be registered in the lab using Postman the day before. Registration of switch is possible to be done using resource aci_fabric_node_member . Documentation of this resource is present in the official Terraform Registry documentation . - In the Example Usage section you can find example code to register new node - In the Argument Reference you will see possible arguments/parameters that can be used for switch registration with indication if argument is required or optional and additional information about it - In the Attribute Reference you will see what is the attribute that this resource exports, and in case of ACI resources it will be always id set to the DN of the VLAN Pool. - Importing describes how to import existing object to the terraform resource For reference the following resource configuration in Terraform would register new switch. If the switches are already registered, this task can be skipped resource \"aci_fabric_node_member\" \"Leaf-101\" { name = \"Leaf-101\" serial = \"TEP-1-101\" node_id = \"101\" pod_id = \"1\" role = \"leaf\" } resource \"aci_fabric_node_member\" \"Leaf-102\" { name = \"Leaf-102\" serial = \"TEP-1-102\" node_id = \"102\" pod_id = \"1\" role = \"leaf\" } resource \"aci_fabric_node_member\" \"Spine-103\" { name = \"Spine-103\" serial = \"TEP-1-103\" node_id = \"103\" pod_id = \"1\" role = \"spine\" } resource \"aci_fabric_node_member\" \"Spine-104\" { name = \"Spine-104\" serial = \"TEP-1-104\" node_id = \"104\" pod_id = \"1\" role = \"spine\" }","title":"3.2 (Optional) Switch registration"},{"location":"terraform-usecases/#4-aci-access-policies","text":"Every ACI Fabric configuration starts with Access Polices. You need to define ACI objects which represent single physical port configurations. To remind relationships between objects, please check following figure. Usually Interface Polices are configured once and re-used like LLDP, LACP, CDP, etc. In our LAB some of the policies already exist from the day before and Postman configuration, so different names should be used for Terraform lab to create new objects. Following sections will help you to prepare your API requests. For reference full configuration of access-policies can be found here: Terraform Tenant Ready .","title":"4 ACI Access Policies"},{"location":"terraform-usecases/#41-interface-policies","text":"You will configure LACP Policy, LLDP and speed policies. Let's separate our terraform files and create separate config file for out interface policies. Under your folder in Visual Studio create new file calles interface-policies.tf : Copy each of the Policy resource definition to the new interface-policies.tf file LACP_ACTIVE resource \"aci_lacp_policy\" \"LACP_ACTIVE\" { na me = \"LACP_ACTIVE\" c trl = [ \"susp-individual\" , \"fast-sel-hot-stdby\" , \"graceful-conv\" ] mode = \"active\" } LLDP_ON resource \"aci_lldp_interface_policy\" \"LLDP_ON\" { na me = \"LLDP_ON\" admi n _rx_s t = \"enabled\" admi n _ t x_s t = \"enabled\" } LINK-10G resource \"aci_fabric_if_pol\" \"LINK-10G\" { na me = \"LINK-10G\" au t o_ ne g = \"on\" speed = \"10G\" } The documentation for each of the resources can be found in Terraform Registry: aci_lacp_policy resource aci_lldp_interface_policy resource aci_fabric_if_pol resource","title":"4.1 Interface Policies"},{"location":"terraform-usecases/#42-run-terraform-plan-apply","text":"At this point we have our first three resources in the configuration so it's time to deploy them on our APIC. To do so we will follow the Terraform workload init -> plan -> apply. In previous step, after configuring ACI provider you should have already done terraform init , so it is not needed to run it again now, as it would not bring any change. Our first step to deploy config is to run terraform plan command in the terminal window. The output will show us what changes Terraform needs to do on the fabric, to bring it to the required configuration. The output should look similar to the following: juchowan@JUCHOWAN-M-D2P2 ACI-simulator % terraform plan aci_fabric_node_member.Leaf-102: Refreshing state... [id=uni/controller/nodeidentpol/nodep-TEP-1-102] aci_fabric_node_member.Leaf-101: Refreshing state... [id=uni/controller/nodeidentpol/nodep-TEP-1-101] aci_fabric_node_member.Spine-103: Refreshing state... [id=uni/controller/nodeidentpol/nodep-TEP-1-103] aci_fabric_node_member.Spine-104: Refreshing state... [id=uni/controller/nodeidentpol/nodep-TEP-1-104] Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # aci_fabric_if_pol.auto-10G will be created + resource \"aci_fabric_if_pol\" \"auto-10G\" { + annotation = \"orchestrator:terraform\" + auto_neg = \"on\" + description = (known after apply) + fec_mode = (known after apply) + id = (known after apply) + link_debounce = (known after apply) + name = \"LINK-10G\" + name_alias = (known after apply) + speed = \"10G\" } # aci_lldp_interface_policy.LLDP_ON will be created + resource \"aci_lldp_interface_policy\" \"LLDP_ON\" { + admin_rx_st = \"enabled\" + admin_tx_st = \"enabled\" + annotation = \"orchestrator:terraform\" + description = (known after apply) + id = (known after apply) + name = \"LLDP_ON\" + name_alias = (known after apply) } # aci_lacp_policy.LACP_ACTIVE will be created + resource \"aci_lacp_policy\" \"LACP_ACTIVE\" { + annotation = \"orchestrator:terraform\" + ctrl = [ + \"susp-individual\", + \"fast-sel-hot-stdby\", + \"graceful-conv\", ] + description = \"done by terraform\" + id = (known after apply) + max_links = (known after apply) + min_links = (known after apply) + mode = \"active\" + name = \"LACP_ACTIVE\" + name_alias = (known after apply) } Plan: 3 to add, 0 to change, 0 to destroy. \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Note: You didn't use the -out option to save this plan, so Terraform can't guarantee to take exactly these actions if you run \"terraform apply\" now. In the plan you can see all the plus signs beside the elements that are going to be added to the fabric. Terraform read the fabric state and compared with required configuration, and then decided which elements need to be added. At this point no configuration has been added yet to the fabric, Terraform only lists the changes it is planning to do. As the Note says at the bottom, this plan was not saved to a file, but only displayed in the console. If you change command to terraform plan -out interface-policies.plan you will see that a file was generated with the contents of the plan. Now you can perform the terraform apply command. This command will run plan again, ask for approval and then it will go into the fabric and perform these actions. <plan omitted> Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes aci_lldp_interface_policy.LLDP_ON: Creating... aci_fabric_if_pol.auto-10G: Creating... aci_lacp_policy.LACP_ACTIVE: Creating... aci_lldp_interface_policy.LLDP_ON: Creation complete after 0s [id=uni/infra/lldpIfP-LLDP_ON] aci_fabric_if_pol.auto-10G: Creation complete after 0s [id=uni/infra/hintfpol-LINK-10G] aci_lacp_policy.LACP_ACTIVE: Creation complete after 0s [id=uni/infra/lacplagp-LACP_ACTIVE] Apply complete! Resources: 3 added, 0 changed, 0 destroyed. If you run plan command again now, you would see the output is different because now it will only show the changes it plans to do over the previous push. Since there are no changes in the config yet, the plan will not show anything new to be added: juchowan@JUCHOWAN-M-D2P2 ACI-simulator % terraform plan aci_fabric_node_member.Spine-103: Refreshing state... [id=uni/controller/nodeidentpol/nodep-TEP-1-103] aci_fabric_node_member.Leaf-102: Refreshing state... [id=uni/controller/nodeidentpol/nodep-TEP-1-102] aci_lacp_policy.LACP_ACTIVE: Refreshing state... [id=uni/infra/lacplagp-LACP_ACTIVE] aci_fabric_if_pol.auto-10G: Refreshing state... [id=uni/infra/hintfpol-LINK-10G] aci_fabric_node_member.Spine-104: Refreshing state... [id=uni/controller/nodeidentpol/nodep-TEP-1-104] aci_lldp_interface_policy.LLDP_ON: Refreshing state... [id=uni/infra/lldpIfP-LLDP_ON] aci_fabric_node_member.Leaf-101: Refreshing state... [id=uni/controller/nodeidentpol/nodep-TEP-1-101] No changes. Your infrastructure matches the configuration.","title":"4.2 Run Terraform Plan &amp; Apply"},{"location":"terraform-usecases/#43-vlans-domains-and-aaeps","text":"Create VLAN pool with vlan-range, associate it with Physical Domain and AAEP. Let's put these policies in a separate, new file called access-policies.tf We will create two VLAN pools, one for L2 Physical Domain, and second one for L3 Routed Domain. In the same way we will create two AEP, one for L2 connections and second one for L3 connections. Each AEP will map one domain: - VLAN_POOL_L2 <-- PHYSDOM_1 <-- AEP_L2 - VLAN_POOL_L3 <-- EXTRTDOM_1 <-- AEP_L3 For VLAN pool there are two resources that need to be created: VLAN Pool and VLAN Range. In this case VLAN Range object is a child of VLAN Pool object, which means that the Range resource needs to have a reference to the VLAN Pool resource. This reference can be done in terraform by using the resource name followed by the attribute, in this case id . resource \"aci_vlan_pool\" \"VLAN_POOL_L2\" { name = \"VLAN_POOL_STATIC\" alloc_mode = \"static\" } resource \"aci_ranges\" \"RANGE_100-200\" { vlan_pool_dn = aci_vlan_pool.VLAN_POOL_L2.id from = \"vlan-100\" to = \"vlan-200\" alloc_mode = \"inherit\" } resource \"aci_vlan_pool\" \"VLAN_POOL_L3\" { name = \"VLAN_POOL_STATIC\" alloc_mode = \"static\" } resource \"aci_ranges\" \"RANGE_201-300\" { vlan_pool_dn = aci_vlan_pool.VLAN_POOL_L3.id from = \"vlan-201\" to = \"vlan-300\" alloc_mode = \"inherit\" } Two domains, physical and routed, will be created as two resources, and each of them needs to have a reference to a VLAN Pool. This reference will be done by special attribute relation_infra_rs_vlan_ns , by referencing the resource name and id attribute: resource \"aci_physical_domain\" \"PHYSDOM_1\" { name = \"PHYSDOM_1\" relation_infra_rs_vlan_ns = aci_vlan_pool.VLAN_POOL_L2.id } resource \"aci_l3_domain_profile\" \"EXTRTDOM_1\" { name = \"EXTRTDOM_1\" relation_infra_rs_vlan_ns = aci_vlan_pool.VLAN_POOL_L3.id } Two AAEP will be created, one for L2 connections and another one for L3 connections. Between AAEP and Domains there is also relationship. AEP needs to be related to a domain. This relation can be done in two ways: separate resource aci_aaep_to_domain which is shown in the example below for AEP_L2 and PHYSDOM_1 relation. This resource needs to reference id values of both AEP and Physical Domain argument within aci_attachable_access_entity_profile called relation_infra_rs_dom_p which is shown in the example below for AEP_L3 and EXTRTDOM_1 relation. This argument needs to reference id values of both AEP and Routed Domain, but needs to be embedded in [] as the documentation says it's Type -[Set of String] resource \"aci_attachable_access_entity_profile\" \"AEP_L2\" { description = \"AAEP for L2 connections\" name = \"AEP_L2\" } resource \"aci_aaep_to_domain\" \"AEP_L2_to_PHYSDOM_1\" { attachable_access_entity_profile_dn = aci_attachable_access_entity_profile.AEP_L2.id domain_dn = aci_physical_domain.PHYSDOM_1.id } resource \"aci_attachable_access_entity_profile\" \"AEP_L3\" { description = \"AAEP for L3 connections\" name = \"AEP_L3\" relation_infra_rs_dom_p = [aci_l3_domain_profile.EXTRTDOM_1.id] } Documentation of resources: aci_vlan_pool resource aci_ranges resource aci_physical_domain resource aci_l3_domain_profile resource aci_attachable_access_entity_profile resource aci_aaep_to_domain resource","title":"4.3 VLANs, Domains and AAEPs"},{"location":"terraform-usecases/#44-interface-policy-group","text":"Let's now create two Interface Policy Groups, one for server (L2) and another one for router (L3) and use all created policies: IntPolGrp_VPC_server1 VPC policy with AEP_L2 IntPolGrp_Router1 access port policy with AEP_L3 Those interface policy groups will have relation to all the access policies we created before by referencing the id of those policies: resource \"aci_leaf_access_bundle_policy_group\" \"IntPolGrp_VPC_server1\" { name = \"IntPolGrp_VPC_server1\" lag_t = \"node\" relation_infra_rs_lldp_if_pol = aci_lldp_interface_policy.LLDP_ON.id relation_infra_rs_lacp_pol = aci_lacp_policy.LACP_ACTIVE.id relation_infra_rs_h_if_pol = aci_fabric_if_pol.LINK_10G.id relation_infra_rs_att_ent_p = aci_attachable_access_entity_profile.AEP_L2.id } resource \"aci_leaf_access_bundle_policy_group\" \"IntPolGrp_Router1\" { name = \"IntPolGrp_Router1\" lag_t = \"node\" relation_infra_rs_lldp_if_pol = aci_lldp_interface_policy.LLDP_ON.id relation_infra_rs_lacp_pol = aci_lacp_policy.LACP_ACTIVE.id relation_infra_rs_h_if_pol = aci_fabric_if_pol.LINK_10G.id relation_infra_rs_att_ent_p = aci_attachable_access_entity_profile.AEP_L3.id } Documentation of resources: aci_leaf_access_bundle_policy_group resource aci_leaf_access_port_policy_group resource","title":"4.4 Interface Policy group"},{"location":"terraform-usecases/#45-switch-interface-profiles","text":"Let's assign our created interface policy groups to the newly created switch and interface profiles. We will create: switch profile interface profile IntProf-101-102 with two interface selectors eth1/1 connecting server eth1/2 connecting router First we create interface profile within this profile two interface selectors. Interface selectors need to have reference to the Interface Profile ( IntProf-101-102 ) and relation to the interface policy groups created for our end devices: IntPolGrp_VPC_server1 IntPolGrp_Router1 resource \"aci_leaf_interface_profile\" \"IntProf-101-102\" { name = \"IntProf-101-102\" } resource \"aci_access_port_selector\" \"leaf-101-102-eth-1\" { leaf_interface_profile_dn = aci_leaf_interface_profile.IntProf-101-102.id name = \"leaf-101-102-eth-1\" access_port_selector_type = \"range\" relation_infra_rs_acc_base_grp = aci_leaf_access_bundle_policy_group.IntPolGrp_VPC_server1.id } resource \"aci_access_port_block\" \"leaf-101-102-eth-1-block\" { access_port_selector_dn = aci_access_port_selector.leaf-101-102-eth-1.id name = \"leaf-101-102-eth-1-block\" from_card = \"1\" from_port = \"1\" to_card = \"1\" to_port = \"1\" } resource \"aci_access_port_selector\" \"leaf-101-102-eth-2\" { leaf_interface_profile_dn = aci_leaf_interface_profile.IntProf-101-102.id name = \"leaf-101-102-eth-2\" access_port_selector_type = \"range\" relation_infra_rs_acc_base_grp = aci_leaf_access_bundle_policy_group.IntPolGrp_Router1.id } resource \"aci_access_port_block\" \"leaf-101-102-eth-2-block\" { access_port_selector_dn = aci_access_port_selector.leaf-101-102-eth-2.id name = \"leaf-101-102-eth-2-block\" from_card = \"1\" from_port = \"2\" to_card = \"1\" to_port = \"2\" } Next we have our switch profile configuration which needs to have a reference to the leaf interface profile ( IntProf-101-102 ) as well as mark the nodes which will be used in this switch profile (nodes 101 & 102) resource \"aci_leaf_profile\" \"SwProf-101-102\" { name = \"SwProf-101-102\" relation_infra_rs_acc_port_p = [aci_leaf_interface_profile.IntProf-101-102.id] } resource \"aci_leaf_selector\" \"Sel-101-102\" { leaf_profile_dn = aci_leaf_profile.SwProf-101-102.id name = \"Sel-101-102\" switch_association_type = \"range\" } resource \"aci_node_block\" \"Sel-101-102-Block\" { switch_association_dn = aci_leaf_selector.Sel-101-102.id name = \"Sel-101-102-Block\" from_ = \"101\" to_ = \"102\" }","title":"4.5 Switch &amp; Interface Profiles"},{"location":"terraform-usecases/#5-aci-tenant","text":"Upon now you created ACI AccessPolicies for your Tenant. Now is time to create your tenant using Terraform. It will be simple Tenant definition with one VRF, one bridge domains associated with one EPG and one L3out. EPG will be associated with L2 domain and statically binded to VPC created, with VLAN from L2 VLAN pool done by you perviously. The L3out will be associated to L3 domain and router interface, with VLAN from L3 VLAN pool. Figure below shows our Logical Tenant. You need to define Tenant, VRF, Bridge-Domain, Application-Profile, EPG with Domain association, static binding under EPG and L3out. Quite many of resources to do in Terraform. We will define all of them in a single, new config file called dcloud-tenant-1.tf Please, download a terraform file from Terraform Tenant Ready .","title":"5 ACI Tenant"},{"location":"terraform-usecases/#51-tenant-components","text":"This section contains Terraform codes necessary to create Tenant objects","title":"5.1 Tenant components"},{"location":"terraform-usecases/#511-tenant-and-vrf","text":"Following code can be used to create new Tenant and VRFs. The VRF resource needs to reference its parent object - tenant id. resource \"aci_tenant\" \"dcloud-tenant-1\" { name = \"dcloud-tenant-1\" } resource \"aci_vrf\" \"vrf1\" { tenant_dn = aci_tenant.dcloud-tenant-1.id name = \"vrf1\" }","title":"5.1.1 Tenant and VRF"},{"location":"terraform-usecases/#512-bd-in-existing-tenant","text":"Following code can be used to create new Bridge Domain in existing Tenant. Bridge Domain needs to have a reference to parent objects: tenant ID and VRF ID. There is also a child object of bridge domain which is a Subnet with reference to bridge domain ID. resource \"aci_bridge_domain\" \"bd-network-1\" { tenant_dn = aci_tenant.dcloud-tenant-1.id relation_fv_rs_ctx = aci_vrf.vrf1.id name = \"bd-network-1\" } resource \"aci_subnet\" \"subnet-1\" { parent_dn = aci_bridge_domain.bd-network-1.id ip = \"10.0.0.1/24\" scope = [\"public\"] }","title":"5.1.2 BD in existing tenant"},{"location":"terraform-usecases/#513-application-profile-with-epg-and-static-bindings","text":"Following code can be used to create new Application Profile with EPG. Application Profile needs a reference to parent object tenant ID, while EPG needs reference to AP ID ad Bridge Domain ID. The next two resources reflect the physical domain PHYSDOM_1 attached to that EPG, and one static binding towards our server1 with encapsulation of vlan 100. resource \"aci_application_profile\" \"ap1\" { tenant_dn = aci_tenant.dcloud-tenant-1.id name = \"ap1\" } resource \"aci_application_epg\" \"epg1\" { application_profile_dn = aci_application_profile.ap1.id name = \"epg1\" relation_fv_rs_bd = aci_bridge_domain.bd-network-1.id } resource \"aci_epg_to_domain\" \"epg1-to-physdom_1\" { application_epg_dn = aci_application_epg.epg1.id tdn = aci_physical_domain.PHYSDOM_1.id } resource \"aci_epg_to_static_path\" \"static-binding-1\" { application_epg_dn = aci_application_epg.epg1.id tdn = \"topology/pod-1/protpaths-101-102/pathep-[IntPolGrp_VPC_server1]\" encap = \"vlan-100\" mode = \"regular\" }","title":"5.1.3 Application Profile with EPG and static bindings"},{"location":"terraform-usecases/#514-l3out","text":"Following code can be used to create new L3out. As you can see there are plenty of resources which reflect node profile and node mapping, interface profile and path used for it, external EPG with subnet. resource \"aci_l3_outside\" \"l3out-1\" { tenant_dn = aci_tenant.dcloud-tenant-1.id name = \"l3out-1\" relation_l3ext_rs_l3_dom_att = aci_l3_domain_profile.EXTRTDOM_1.id relation_l3ext_rs_ectx = aci_vrf.vrf1.id } resource \"aci_logical_node_profile\" \"np-101-102\" { l3_outside_dn = aci_l3_outside.l3out-1.id name = \"np-101-102\" } resource \"aci_logical_node_to_fabric_node\" \"np-101-102-to-node-101\" { logical_node_profile_dn = aci_logical_node_profile.np-101-102.id tdn = \"topology/pod-1/node-101\" rtr_id = \"1.1.1.1\" } resource \"aci_logical_node_to_fabric_node\" \"np-101-102-to-node-102\" { logical_node_profile_dn = aci_logical_node_profile.np-101-102.id tdn = \"topology/pod-1/node-102\" rtr_id = \"1.1.1.2\" } resource \"aci_logical_interface_profile\" \"ip-101-102\" { logical_node_profile_dn = aci_logical_node_profile.np-101-102.id name = \"ip-101-102\" } resource \"aci_l3out_path_attachment\" \"l3ip-path\" { logical_interface_profile_dn = aci_logical_interface_profile.ip-101-102.id target_dn = \"topology/pod-1/protpaths-101-102/pathep-[IntPolGrp_Router1]\" if_inst_t = \"ext-svi\" addr = \"10.1.1.1/24\" encap = \"vlan-201\" encap_scope = \"ctx\" mode = \"regular\" mtu = \"inherit\" } resource \"aci_external_network_instance_profile\" \"extepg\" { l3_outside_dn = aci_l3_outside.l3out-1.id name = \"extepg\" } resource \"aci_l3_ext_subnet\" \"extepg-subnet\" { external_network_instance_profile_dn = aci_external_network_instance_profile.extepg.id ip = \"0.0.0.0/0\" scope = [\"import-security\",\"export-rtctrl\"] } Documentation of resources: aci_tenant resource aci_bridge_domain aci_subnet resource aci_application_profile aci_application_epg aci_epg_to_domain aci_l3_outside","title":"5.1.4 L3Out"},{"location":"terraform-vars/","text":"Terraform Language As you can see in the previous lab, manually specifying each resource and its values is very time consuming. In this lab you will learn how to use variables and functions to simplify the work 1. Variable file Variable declarations can appear anywhere in your configuration files. However, it is recommended putting them into a separate file called variables.tf to make it easier for users to understand how the configuration is meant to be customized. To parameterize an argument with an input variable, you will first define the variable in variables.tf , then replace the hardcoded value with a reference to that variable in your configuration. Variable blocks have three optional arguments. Description : A short description to document the purpose of the variable. Type : The type of data contained in the variable. Default : The default value. If you do not set a default value for a variable, you must assign a value before Terraform can apply the configuration. Terraform does not support unassigned variables. When variables are declared in the root folder of your configuration, they can be set in a number of ways: In a Terraform Cloud workspace. Individually, with the -var command line option. In variable definitions (.tfvars) files, either specified on the command line or automatically loaded. As environment variables. 1.1 Variables definition Let's create in our folder new file called variables.tf . In this file let's define several variables that we will use to deploy new tenant. Following variables will be used as a name for new tenant, VRF and bridge domain. Those variables have only a default value defined which will be used if no other value is specified for execution. variable \"tenant_name\" { default = \"dcloud-tenant-2\" } variable \"vrf_name\" { default = \"vrf2-1\" } variable \"bd_name\" { default = \"bd2-1\" } The next variable is used for specifying the ARP flooding behaviour of a bridge domain. Here you can see that we are using all possible options: description of variable default value to be used if no other is specfied validation mechanism with: condition - variable can be set to either \"yes\" or \"no\" value error message to be displayed if specified value doesn't pass the validation test variable \"bd_arp_flood\" { description = \"Specify whether ARP flooding is enabled. If flooding is disabled, unicast routing will be performed on the target IP address.\" default = \"yes\" validation { condition = (var.bd_arp_flood == \"yes\") || (var.bd_arp_flood == \"no\") error_message = \"Allowed values are \\\"yes\\\" and \\\"no\\\".\" } } The last two variables will be specifying bridge domain behaviour for unicast routing and unknown unicast flooding: variable \"bd_unicast_routing\" { default = \"yes\" } variable \"bd_unk_ucast\" { default = \"proxy\" } 1.2 Variable usage in config Now let's use the variables we defined to create new tenant. For this task please create new config file called dcloud-tenant-2.tf . We will use it to deploy new tenant called dcloud-tenant-2 with one VRF and bridge domain. The way we make references to our variables is by using var. inside our config file. the var. part of this syntax indicates that we are using variables. resource \"aci_tenant\" \"dcloud-tenant-2\" { name = var.tenant_name description = \"This is a new tenant created from Terraform\" } resource \"aci_vrf\" \"vrf2-1\" { tenant_dn = aci_tenant.dcloud-tenant-2.id name = var.vrf_name } resource \"aci_bridge_domain\" \"bd_192_168_1_0\" { tenant_dn = aci_tenant.dcloud-tenant-2.id name = var.bd_name arp_flood = var.bd_arp_flood unicast_route = var.bd_unicast_routing unk_mac_ucast_act = var.bd_unk_ucast unk_mcast_act = \"flood\" relation_fv_rs_ctx = aci_vrf.vrf2-1.id } In our variables.tf all our variables have default value assigned, which means we should be able to apply this configuration and deploy new tenant. Let's run terraform apply and verify on APIC that all the objects have been deployed according to the specified variables. juchowan@JUCHOWAN-M-D2P2 ACI-simulator % terraform apply <omitted> Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # aci_bridge_domain.bd_192_168_1_0 will be created + resource \"aci_bridge_domain\" \"bd_192_168_1_0\" { + annotation = \"orchestrator:terraform\" + arp_flood = \"yes\" + bridge_domain_type = (known after apply) + description = (known after apply) + ep_clear = (known after apply) + ep_move_detect_mode = (known after apply) + host_based_routing = (known after apply) + id = (known after apply) + intersite_bum_traffic_allow = (known after apply) + intersite_l2_stretch = (known after apply) + ip_learning = (known after apply) + ipv6_mcast_allow = (known after apply) + limit_ip_learn_to_subnets = (known after apply) + ll_addr = (known after apply) + mac = (known after apply) + mcast_allow = (known after apply) + multi_dst_pkt_act = (known after apply) + name = \"bd2-1\" + name_alias = (known after apply) + optimize_wan_bandwidth = (known after apply) + relation_fv_rs_bd_to_ep_ret = (known after apply) + relation_fv_rs_bd_to_nd_p = (known after apply) + relation_fv_rs_ctx = (known after apply) + relation_fv_rs_igmpsn = (known after apply) + relation_fv_rs_mldsn = (known after apply) + tenant_dn = (known after apply) + unicast_route = \"yes\" + unk_mac_ucast_act = \"proxy\" + unk_mcast_act = \"flood\" + v6unk_mcast_act = (known after apply) + vmac = (known after apply) } # aci_tenant.dcloud-tenant-2 will be created + resource \"aci_tenant\" \"dcloud-tenant-2\" { + annotation = \"orchestrator:terraform\" + description = \"This is a new tenant created from Terraform\" + id = (known after apply) + name = \"dcloud-tenant-2\" + name_alias = (known after apply) + relation_fv_rs_tenant_mon_pol = (known after apply) } # aci_vrf.vrf2-1 will be created + resource \"aci_vrf\" \"vrf2-1\" { + annotation = \"orchestrator:terraform\" + bd_enforced_enable = (known after apply) + description = (known after apply) + id = (known after apply) + ip_data_plane_learning = (known after apply) + knw_mcast_act = (known after apply) + name = \"vrf2-1\" + name_alias = (known after apply) + pc_enf_dir = (known after apply) + pc_enf_pref = (known after apply) + relation_fv_rs_bgp_ctx_pol = (known after apply) + relation_fv_rs_ctx_to_ep_ret = (known after apply) + relation_fv_rs_ctx_to_ext_route_tag_pol = (known after apply) + relation_fv_rs_ospf_ctx_pol = (known after apply) + relation_fv_rs_vrf_validation_pol = (known after apply) + tenant_dn = (known after apply) } Plan: 3 to add, 0 to change, 0 to destroy. Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes aci_tenant.dcloud-tenant-2: Creating... aci_tenant.dcloud-tenant-2: Creation complete after 1s [id=uni/tn-dcloud-tenant-2] aci_vrf.vrf2-1: Creating... aci_vrf.vrf2-1: Creation complete after 1s [id=uni/tn-dcloud-tenant-2/ctx-vrf2-1] aci_bridge_domain.bd_192_168_1_0: Creating... aci_bridge_domain.bd_192_168_1_0: Creation complete after 2s [id=uni/tn-dcloud-tenant-2/BD-bd2-1] Apply complete! Resources: 3 added, 0 changed, 0 destroyed. 1.3 Provide variable value 1.3.1 Manual variable values Now let's see how else we can provide variable values. In this case first let's modify our variables.tf file and add new variable called bd_name_2 which will have no default value and new variable bd_arp_flood_2 which will also have no default value. This way with no default value, Terraform will be forced to ask user to specify it manually during execution: Note We are not removing default value from existing bd_name or bd_arp_flood variables, as it would force terraform to change it on the existing bridge domain. We are adding new variables to show the difference between them variable \"bd_arp_flood_2\" { description = \"Specify whether ARP flooding is enabled. If flooding is disabled, unicast routing will be performed on the target IP address.\" validation { condition = (var.bd_arp_flood_2 == \"yes\") || (var.bd_arp_flood_2 == \"no\") error_message = \"Allowed values are \\\"yes\\\" and \\\"no\\\".\" } } variable \"bd_name_2\" { } Let's also add another bridge domain resource where we will verify our manual values. Add following block to the dcloud-tenant-2.tf file. Here we are only giving new name to the resource so that Terraform is aware that this is a new resource to be deployed. resource \"aci_bridge_domain\" \"bd_192_168_2_0\" { tenant_dn = aci_tenant.dcloud-tenant-2.id name = var.bd_name_2 arp_flood = var.bd_arp_flood_2 unicast_route = var.bd_unicast_routing unk_mac_ucast_act = var.bd_unk_ucast unk_mcast_act = \"flood\" relation_fv_rs_ctx = aci_vrf.vrf2-1.id } Now if we run terraform plan we will be asked to specify both values that don't have a default value in the variables file. Let's specify them, but at the same time test if our validation rule works. Please specify bd2 as bd_name_2 and enable as bd_arp_flood_2 juchowan@JUCHOWAN-M-D2P2 ACI-simulator % terraform plan var.bd_arp_flood_2 Specify whether ARP flooding is enabled. If flooding is disabled, unicast routing will be performed on the target IP address. Enter a value: enable var.bd_name_2 Enter a value: bd2 <omitted> \u2502 Error: Invalid value for variable \u2502 \u2502 on variables.tf line 31: \u2502 31: variable \"bd_arp_flood_2\" { \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502 \u2502 var.bd_arp_flood_2 is \"enable\" \u2502 \u2502 Allowed values are \"yes\" and \"no\". \u2502 \u2502 This was checked by the validation rule at variables.tf:33,3-13. \u2575 In this case Terraform will stop the plan and report incorrect value provided for bd_arp_flood_2 If we then specify yes to bd_arp_flood and bd2 for bd_name we will see that the plan generated output showing us what would be deployed in the APIC: juchowan@JUCHOWAN-M-D2P2 ACI-simulator % terraform plan var.bd_arp_flood_2 Specify whether ARP flooding is enabled. If flooding is disabled, unicast routing will be performed on the target IP address. Enter a value: yes var.bd_name_2 Enter a value: bd2 Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # aci_bridge_domain.bd_192_168_2_0 will be created + resource \"aci_bridge_domain\" \"bd_192_168_2_0\" { + annotation = \"orchestrator:terraform\" + arp_flood = \"yes\" + bridge_domain_type = (known after apply) + description = (known after apply) + ep_clear = (known after apply) + ep_move_detect_mode = (known after apply) + host_based_routing = (known after apply) + id = (known after apply) + intersite_bum_traffic_allow = (known after apply) + intersite_l2_stretch = (known after apply) + ip_learning = (known after apply) + ipv6_mcast_allow = (known after apply) + limit_ip_learn_to_subnets = (known after apply) + ll_addr = (known after apply) + mac = (known after apply) + mcast_allow = (known after apply) + multi_dst_pkt_act = (known after apply) + name = \"bd2\" + name_alias = (known after apply) + optimize_wan_bandwidth = (known after apply) + relation_fv_rs_bd_to_ep_ret = (known after apply) + relation_fv_rs_bd_to_nd_p = (known after apply) + relation_fv_rs_ctx = \"uni/tn-dcloud-tenant-2/ctx-vrf2-1\" + relation_fv_rs_igmpsn = (known after apply) + relation_fv_rs_mldsn = (known after apply) + tenant_dn = \"uni/tn-dcloud-tenant-2\" + unicast_route = \"yes\" + unk_mac_ucast_act = \"proxy\" + unk_mcast_act = \"flood\" + v6unk_mcast_act = (known after apply) + vmac = (known after apply) } Plan: 1 to add, 0 to change, 0 to destroy. Let's run terraform apply to see that this bridge domain would be deployed. 1.3.2. The .tfvars file Manual method of providing values for our variables can be quite absorbing and prone to humar errors. Another, preferred way would be to include all our variable values into separate file called *.tfvars. Please create new file calle terraform.tfvars . Variable values in such file can be defined in a simple way: tenant_name = \"dcloud-tenant-2\" vrf_name = \"vrf2-1\" bd_name_2 = \"bd2\" bd_arp_flood_2 = \"no\" bd_unicast_routing = \"no\" bd_unk_ucast = \"proxy\" To each variable name we assign our preferred value. If you take a look closely you will see that for bd_arp_flood_2 and bd_unicast_routing we specify different values that we did for our bd2 bridge domain. If we run our terraform apply command now, we will see that terraform wants to do changes on our resource: <omitted> Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: ~ update in-place Terraform will perform the following actions: # aci_bridge_domain.bd_192_168_1_0 will be updated in-place ~ resource \"aci_bridge_domain\" \"bd_192_168_1_0\" { id = \"uni/tn-dcloud-tenant-2/BD-bd2-1\" name = \"bd2-1\" ~ unicast_route = \"yes\" -> \"no\" # (22 unchanged attributes hidden) } # aci_bridge_domain.bd_192_168_2_0 will be updated in-place ~ resource \"aci_bridge_domain\" \"bd_192_168_2_0\" { ~ arp_flood = \"yes\" -> \"no\" id = \"uni/tn-dcloud-tenant-2/BD-bd2\" name = \"bd2\" ~ unicast_route = \"yes\" -> \"no\" # (21 unchanged attributes hidden) } Plan: 0 to add, 2 to change, 0 to destroy. If we execute our command, Terraform will change the configuration on the APIC. Please verify in the GUI that these changes really reflect the configuration. 2. For_each loop This way of specifying variables and creating configuration is good if we want to deploy single object, however in case we have multiple bridge domains to deploy it may not be the most efficient way. By default a resource block configures one real infrastructure object. However, sometimes you want to manage several similar obejcts (like several bridge domains) without writing separate block for each one. Terraform has two ways to do this: count for_each The count argument takes a value of a whole number and Terraform will simply create that many instances of an object. This will not help us much with ACI, as every of these objects would have the same configuration. We want to have option to create multiple bridge domains, which may have different config options. The for_each argument can be included in a resource block, and its value is a map or set of strings. Terraform will create one instance for each member of that map or set. For this exercise we will work with maps that will include our bridge domains configuration. The map has an index value for each object instance. This index is used by terraform to reference resources and to get values we need from it to create objects. In our example let's create new folder for our workspace and call it ACI-2 . Inside this folder create three files that we will use: main.tf - which will hold our provider configuration and resource that will be deployed (tenant, vrf, bridge domains and subents) variables.tf - which will have definition of variables we will use, including their types and default values terraform.tfvars - which we will use to specify our desired values and confiuration of bridge domains Inside variables.tf file paste the following content: variable \"tenant_name\" { default = \"demo_tn\" } variable \"vrf_name\" { default = \"vrf1\" } variable \"bridge_domains\" { type = map(object({ name = string arp_flood = string unicast_routing = string unk_ucast = string subnet = string subnet_scope = list(string) })) default = { default_bd = { name = \"default\" arp_flood = \"yes\" unicast_routing = \"yes\" unk_ucast = \"proxy\" subnet = \"10.10.10.1/24\" subnet_scope = [\"private\"] } } } In this file we can see that we are defining three variables: tenant_name, vrf_name and bridge_domains. The bridge_domains variable is the interesting one, and it has type map of objects. One object will represent single bridge domain and its configuration. As part of this object we have our standard values to configure a BD like name, ARP flooding, unicast routing and unknown unicast flooding behaviour, but also subnet and subnet scope. Inside this variable first we define for those arguments what type they should take (string value for most of them and list of strings for subnet_scope). Then we specify default value that our map would take, in case we don't provide any values ourselves (inside terraform.tfvars file). Now we want to provide the values we want to use in the terraform.tffvars file, please paste following content: tenant_name = \"dcloud-tenant-3\" vrf_name = \"vrf1\" bridge_domains = { bd01 = { name = \"192.168.1.0_24_bd\" arp_flood = \"no\" unicast_routing = \"yes\" unk_ucast = \"proxy\" subnet = \"192.168.1.1/24\" subnet_scope = [\"private\"] }, bd02 = { name = \"192.168.2.0_24_bd\" arp_flood = \"yes\" unicast_routing = \"yes\" unk_ucast = \"proxy\" subnet = \"192.168.2.1/24\" subnet_scope = [\"private\",\"shared\"] } } Inside this file we specify our tenant and vrf name we want to use, and then we specify bridge domains map, which includes two bridge domains: bd01 of name 192.168.1.0_24_bd bd02 of name 192.168.2.0_24_bd That means our map has two objects. Each object is a key and value pair, where: first object key is bd01 and value is all the arguments we specify for it, including name, arp_flood, subnet etc second object key is bd02 and value is all the arguments we specify for it, including name, arp_flood, subnet etc You can see that our bridge domains differ in configuration of parameters. We have different names, arp flooding and subnet with scope. Now, since we are doing this demo in separate folder ACI-2 , we will have to initialize Terraform again and treat it as a separate deployment. For this reason inside the main.tf file we have to paste first our ACI provider configuration: terraform { required_providers { aci = { source = \"CiscoDevNet/aci\" version = \"2.5.2\" } } } provider \"aci\" { # Cisco ACI user name username = \"admin\" password = \"C1sco12345\" url = \"https://198.18.133.200\" insecure = true } Note Please remmeber in the terminal to change path to the right folder with cd command. Below that we will start describing our resources. We want to create four resources: tenant, vrf, bridge domain and subnet. resource \"aci_tenant\" \"dcloud-tenant-3\" { name = \"${var.tenant_name}\" description = \"This is a demo tenant created from Terraform\" } resource \"aci_vrf\" \"main\" { tenant_dn = aci_tenant.dcloud-tenant-3.id name = var.vrf_name } resource \"aci_bridge_domain\" \"bd\" { for_each = var.bridge_domains tenant_dn = aci_tenant.dcloud-tenant-3.id name = each.value.name arp_flood = each.value.arp_flood unicast_route = each.value.unicast_routing unk_mac_ucast_act = each.value.unk_ucast unk_mcast_act = \"flood\" relation_fv_rs_ctx = aci_vrf.main.id } resource \"aci_subnet\" \"subnets\" { for_each = var.bridge_domains parent_dn = aci_bridge_domain.bd[each.key].id ip = each.value.subnet scope = each.value.subnet_scope } The tenant and VRF resources are configured in a similar way as in previous demo, so they don't need description. They use standard references to variables we provided in the file. The interesting part is with the bridge_domain and subnet. In here we can see that each of these resources start with a special line for_each = var.bridge_domains This indicates that we want to create this resource once for every object in our variable called bridge_domains . In our terraform.tfvars we declared this variable to include two different bridge_domains, so we expect that this resource will be created twice. Each resource will be unique by the map key. In resource blocks where for_each is set, an additional each object is available in expressions, so you can modify the configuration of each instance. This object has two attributes: each.key - the map key corresponding to this instance each.value - the map value corresponding to this instance. In our resource block for bridge domain and subnet we can see that we are referring to arguments with each.value.<argumnet> or each.key , as an example: name = each.value.name We can also see that for the relation that needs to happen between bd and subnet we specify parent_dn with following expression using each.key : parent_dn = aci_bridge_domain.bd[each.key].id If we now run terraform plan and apply we will see that two bridge domains have been created according to our settings. Let's expand our tenant configuration by adding several EPGs, which we will create in the same way using for_each expression. First we need to define new variable corresponding to an EPG, it will take values of name and bridge domain : variable \"end_point_groups\" { type = map(object({ name = string bd = string })) } Next we specify desired configuration of our EPGs in terraform.tfvars file: end_point_groups = { epg1 = { name = \"epg1\", bd = \"bd01\" }, epg2 = { name = \"epg2\", bd = \"bd02\" } } Finally we add resource configuration in main.tf file: resource \"aci_application_profile\" \"ap1\" { tenant_dn = aci_tenant.dcloud-tenant-3.id name = \"ap1\" } resource \"aci_application_epg\" \"epg\" { for_each = var.end_point_groups application_profile_dn = aci_application_profile.ap1.id name = each.value.name relation_fv_rs_bd = aci_bridge_domain.bd[each.value.bd].id } You can see that endpoint groups also use the for_each function and create relation to a bridge domain that we specified. If you run apply now you will see additional AP and 2xEPG created. You can try adding new EPGs to the terraform.tfvars map of endpoint groups, to see that now it is easier to add new EPGs. 3. Conditionals and for expression A conditional expression uses the value of a boolean expression to select one of two values. The condition can be any expression that resolves to a boolean value. This will usually be an expression that uses the equality, comparison, or logical operators. In our case we will use it to configure several parameters only under condition that other parameters are configured to a sprecific value. As an example let's take bridge domain configuration. On a high level there are two types of bridge domains we configure in ACI: L2 and L3. For L3 bridge domain we need to set unicast routing option to true, flooding arguments need to be adjusted, and subnet IP should be configured. For L2 bridge domains those parameters are not needed and BD should work in a flooding mode with no IP address. To specify that kind of configuration we can use conditionals in Terraform resource block. Please create new folder ACI-3 that we will use for this test, and under this folder three files: main.tf and paste the ACI provider configuration variables.tf which we will use to define variables type terraform.tfvars which we will use to specify our desired configuration Our variables.tf file will include following variabels: variable \"tenant_name\" { default = \"dcloud-tenant-4\" } variable \"vrf_name\" { default = \"vrf1\" } variable \"bridge_domains\" { type = map(object({ name = string arp_flood = string type = string gateway = string scope = list(string) })) default = { default_bd = { name = \"default\" arp_flood = \"yes\" type = \"L3\" gateway = \"192.168.1.1/24\" scope = [\"private\"] } } } In this example our bridge domains are configured with type (L2 or L3), name, ARP flooding mechanism, gateway (IP address) and its scope. We no longer define unicast routing and unknown unicast flooding mechanism. We want to set them based on the type of bridge domain we specify. Our terraform.tfvar file will have our desired config of bridge domains: bridge_domains = { bd01 = { name = \"192.168.1.0_24_bd\" arp_flood = \"yes\" type = \"L3\" gateway = \"192.168.1.1/24\" scope = [\"private\"] }, bd02 = { name = \"192.168.2.0_24_bd\" arp_flood = \"yes\" type = \"L2\" gateway = null scope = null } } One of the bridge domains is L3 and has IP address, while the second one is L2 and has no IP address. Inside main.tf we specify our resources configuration: resource \"aci_tenant\" \"tenant\" { name = \"${var.tenant_name}\" } resource \"aci_vrf\" \"vrf\" { tenant_dn = aci_tenant.tenant.id name = var.vrf_name } resource \"aci_bridge_domain\" \"bd\" { for_each = var.bridge_domains tenant_dn = aci_tenant.tenant.id name = each.value.name arp_flood = each.value.arp_flood unicast_route = each.value.type == \"L3\" ? \"yes\" : \"no\" unk_mac_ucast_act = each.value.type == \"L3\" ? \"proxy\" : \"flood\" unk_mcast_act = \"flood\" relation_fv_rs_ctx = aci_vrf.vrf.id } We are again using for_each loop to create multiple resources for our bridge domains and we assign values like tenant_dn, name, VRF and ARP_flooding in the same way as before, by refering the each.value.<argument name> . The difference is with assigning values for unicast routing and unknown unicast flooding mechanism. For these values we use conditional expression: unicast_route = each.value.type == \"L3\" ? \"yes\" : \"no\" This expression takes each.value.type of our object and verifies if the content of this type is \"L3\" . \"?\" symbol specified that we are checking this condition: if condition is true the result is that unicast_route argument will be assigned \"yes\" value if condition is false the result is that unicast_route argument will be assigned \"no\" value In the same way we can read the second line for unknown unicast flooding verifying condition that each.value.type is \"L3\" : if condition is true the result is that unk_mac_ucast_act argument will be assigned \"proxy\" value if condition is false the result is that unk_mac_ucast_act argument will be assigned \"flood\" value If we run our terraform init , plan and apply commands now, you will get two bridge domains configured with different settings based on the conditional expression. You can verify configuration in the APIC GUI. Note Remember to change to the correct folder in the Terminal Second thing we want to do is to create subnet resource, that will reflect our bridge domain IP address, but we want to do it only for L3 bridge domains. L2 BD doesn't require subnet to be configured and shouldn't have it at all. There would be no reason to deploy subnet resource for L2 bridge domain. If we simply use the for_each expression, we would deploy subnet for all bridge domains we specify in our map, but we want to verify a condition expression here too. This can be done with a for expression. A for expression creates a complex type value by transforming another complex type value. Each element in the input value can correspond to either one or zero values in the result, and an arbitrary expression can be used to transform each input element into an output element. We want to use the for expression to modify our for_each block: resource \"aci_subnet\" \"net\" { for_each = { for k, v in var.bridge_domains: k => v if v.type == \"L3\" } parent_dn = aci_bridge_domain.bd[each.key].id ip = each.value.gateway scope = each.value.scope } In this example we added the expression to the for_each block. This for loop is supposed to go iterate over every k (key) and v (value) pair in our map called var.bridge_domains and return those key and value pair only if the condition that \"type\" is set to \"L3\" is true. In reality for expression is returning (creating) new map variable that will consist only of key and value pairs of elements that pass our condition. If we wanted to take a look at those values, we would have our bridge_domains map that we declared: bridge_domains = { bd01 = { name = \"192.168.1.0_24_bd\" arp_flood = \"yes\" type = \"L3\" gateway = \"192.168.1.1/24\" scope = [\"private\"] }, bd02 = { name = \"192.168.2.0_24_bd\" arp_flood = \"yes\" type = \"L2\" gateway = null scope = null } } The new map variable created by for expression inside resource block would look like this: { bd01 = { name = \"192.168.1.0_24_bd\" arp_flood = \"yes\" type = \"L3\" gateway = \"192.168.1.1/24\" scope = [\"private\"] } } The second bd02 is not included in this temporary variable as it does not meet the condition. The for_each loop can now loop over this map, which only contains one object (one key, value pair) and inside the resource we refer to this object in the same way as we would normally use, with eack.key and each.value arguments: parent_dn = aci_bridge_domain.bd[each.key].id ip = each.value.gateway scope = each.value.scope If you run terraform plan and apply now, you will see that only one aci_subnet resource is being created for \"bd01\". 4. Data sources Terraform data sources let you dynamically fetch data from APIs or other Terraform state backends. This allows to fetch data from APIC that was already created, and this data source is always read-only. We can use it to read objects in ACI that were already created before either manually or by default like a common tenant in ACI. Those data sources can then be used to create relations with different resources that we want to deploy. Data sources have separate documentation in the Terraform Registry As an example we will try to deploy new bridge domain with VRF that will be placed in the common tenant. In ACI from day-0 there is common tenant deployed with default VRF. We will reference it for our new bridge domain. First we have to create two data sources that will reflect our tenant and VRF: data \"aci_tenant\" \"common-tenant\" { name = \"common\" } data \"aci_vrf\" \"common-vrf\" { tenant_dn = data.aci_tenant.common-tenant.id name = \"default\" } Then we can start using references inside our new bridge domain: resource \"aci_bridge_domain\" \"bd-common\" { tenant_dn = aci_tenant.tenant.id name = \"bd-common\" arp_flood = \"yes\" unicast_route = \"no\" unk_mac_ucast_act = \"flood\" unk_mcast_act = \"flood\" relation_fv_rs_ctx = data.aci_vrf.common-vrf.id } The difference in a way we make references to data sources, instead of normal resource, is by adding data. before the data source name. If we run terraform plan and apply now, we will see that Terraform tries to create new bridge domain: Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # aci_bridge_domain.bd-common will be created + resource \"aci_bridge_domain\" \"bd-common\" { + annotation = \"orchestrator:terraform\" + arp_flood = \"yes\" + bridge_domain_type = (known after apply) + description = (known after apply) + ep_clear = (known after apply) + ep_move_detect_mode = (known after apply) + host_based_routing = (known after apply) + id = (known after apply) + intersite_bum_traffic_allow = (known after apply) + intersite_l2_stretch = (known after apply) + ip_learning = (known after apply) + ipv6_mcast_allow = (known after apply) + limit_ip_learn_to_subnets = (known after apply) + ll_addr = (known after apply) + mac = (known after apply) + mcast_allow = (known after apply) + multi_dst_pkt_act = (known after apply) + name = \"bd-common\" + name_alias = (known after apply) + optimize_wan_bandwidth = (known after apply) + relation_fv_rs_bd_to_ep_ret = (known after apply) + relation_fv_rs_bd_to_nd_p = (known after apply) + relation_fv_rs_ctx = \"uni/tn-common/ctx-default\" + relation_fv_rs_igmpsn = (known after apply) + relation_fv_rs_mldsn = (known after apply) + tenant_dn = \"uni/tn-dcloud-tenant-4\" + unicast_route = \"no\" + unk_mac_ucast_act = \"flood\" + unk_mcast_act = \"flood\" + v6unk_mcast_act = (known after apply) + vmac = (known after apply) } Plan: 1 to add, 0 to change, 0 to destroy. aci_tenant data source aci_vrf data source","title":"Terraform Language"},{"location":"terraform-vars/#terraform-language","text":"As you can see in the previous lab, manually specifying each resource and its values is very time consuming. In this lab you will learn how to use variables and functions to simplify the work","title":"Terraform Language"},{"location":"terraform-vars/#1-variable-file","text":"Variable declarations can appear anywhere in your configuration files. However, it is recommended putting them into a separate file called variables.tf to make it easier for users to understand how the configuration is meant to be customized. To parameterize an argument with an input variable, you will first define the variable in variables.tf , then replace the hardcoded value with a reference to that variable in your configuration. Variable blocks have three optional arguments. Description : A short description to document the purpose of the variable. Type : The type of data contained in the variable. Default : The default value. If you do not set a default value for a variable, you must assign a value before Terraform can apply the configuration. Terraform does not support unassigned variables. When variables are declared in the root folder of your configuration, they can be set in a number of ways: In a Terraform Cloud workspace. Individually, with the -var command line option. In variable definitions (.tfvars) files, either specified on the command line or automatically loaded. As environment variables.","title":"1. Variable file"},{"location":"terraform-vars/#11-variables-definition","text":"Let's create in our folder new file called variables.tf . In this file let's define several variables that we will use to deploy new tenant. Following variables will be used as a name for new tenant, VRF and bridge domain. Those variables have only a default value defined which will be used if no other value is specified for execution. variable \"tenant_name\" { default = \"dcloud-tenant-2\" } variable \"vrf_name\" { default = \"vrf2-1\" } variable \"bd_name\" { default = \"bd2-1\" } The next variable is used for specifying the ARP flooding behaviour of a bridge domain. Here you can see that we are using all possible options: description of variable default value to be used if no other is specfied validation mechanism with: condition - variable can be set to either \"yes\" or \"no\" value error message to be displayed if specified value doesn't pass the validation test variable \"bd_arp_flood\" { description = \"Specify whether ARP flooding is enabled. If flooding is disabled, unicast routing will be performed on the target IP address.\" default = \"yes\" validation { condition = (var.bd_arp_flood == \"yes\") || (var.bd_arp_flood == \"no\") error_message = \"Allowed values are \\\"yes\\\" and \\\"no\\\".\" } } The last two variables will be specifying bridge domain behaviour for unicast routing and unknown unicast flooding: variable \"bd_unicast_routing\" { default = \"yes\" } variable \"bd_unk_ucast\" { default = \"proxy\" }","title":"1.1 Variables definition"},{"location":"terraform-vars/#12-variable-usage-in-config","text":"Now let's use the variables we defined to create new tenant. For this task please create new config file called dcloud-tenant-2.tf . We will use it to deploy new tenant called dcloud-tenant-2 with one VRF and bridge domain. The way we make references to our variables is by using var. inside our config file. the var. part of this syntax indicates that we are using variables. resource \"aci_tenant\" \"dcloud-tenant-2\" { name = var.tenant_name description = \"This is a new tenant created from Terraform\" } resource \"aci_vrf\" \"vrf2-1\" { tenant_dn = aci_tenant.dcloud-tenant-2.id name = var.vrf_name } resource \"aci_bridge_domain\" \"bd_192_168_1_0\" { tenant_dn = aci_tenant.dcloud-tenant-2.id name = var.bd_name arp_flood = var.bd_arp_flood unicast_route = var.bd_unicast_routing unk_mac_ucast_act = var.bd_unk_ucast unk_mcast_act = \"flood\" relation_fv_rs_ctx = aci_vrf.vrf2-1.id } In our variables.tf all our variables have default value assigned, which means we should be able to apply this configuration and deploy new tenant. Let's run terraform apply and verify on APIC that all the objects have been deployed according to the specified variables. juchowan@JUCHOWAN-M-D2P2 ACI-simulator % terraform apply <omitted> Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # aci_bridge_domain.bd_192_168_1_0 will be created + resource \"aci_bridge_domain\" \"bd_192_168_1_0\" { + annotation = \"orchestrator:terraform\" + arp_flood = \"yes\" + bridge_domain_type = (known after apply) + description = (known after apply) + ep_clear = (known after apply) + ep_move_detect_mode = (known after apply) + host_based_routing = (known after apply) + id = (known after apply) + intersite_bum_traffic_allow = (known after apply) + intersite_l2_stretch = (known after apply) + ip_learning = (known after apply) + ipv6_mcast_allow = (known after apply) + limit_ip_learn_to_subnets = (known after apply) + ll_addr = (known after apply) + mac = (known after apply) + mcast_allow = (known after apply) + multi_dst_pkt_act = (known after apply) + name = \"bd2-1\" + name_alias = (known after apply) + optimize_wan_bandwidth = (known after apply) + relation_fv_rs_bd_to_ep_ret = (known after apply) + relation_fv_rs_bd_to_nd_p = (known after apply) + relation_fv_rs_ctx = (known after apply) + relation_fv_rs_igmpsn = (known after apply) + relation_fv_rs_mldsn = (known after apply) + tenant_dn = (known after apply) + unicast_route = \"yes\" + unk_mac_ucast_act = \"proxy\" + unk_mcast_act = \"flood\" + v6unk_mcast_act = (known after apply) + vmac = (known after apply) } # aci_tenant.dcloud-tenant-2 will be created + resource \"aci_tenant\" \"dcloud-tenant-2\" { + annotation = \"orchestrator:terraform\" + description = \"This is a new tenant created from Terraform\" + id = (known after apply) + name = \"dcloud-tenant-2\" + name_alias = (known after apply) + relation_fv_rs_tenant_mon_pol = (known after apply) } # aci_vrf.vrf2-1 will be created + resource \"aci_vrf\" \"vrf2-1\" { + annotation = \"orchestrator:terraform\" + bd_enforced_enable = (known after apply) + description = (known after apply) + id = (known after apply) + ip_data_plane_learning = (known after apply) + knw_mcast_act = (known after apply) + name = \"vrf2-1\" + name_alias = (known after apply) + pc_enf_dir = (known after apply) + pc_enf_pref = (known after apply) + relation_fv_rs_bgp_ctx_pol = (known after apply) + relation_fv_rs_ctx_to_ep_ret = (known after apply) + relation_fv_rs_ctx_to_ext_route_tag_pol = (known after apply) + relation_fv_rs_ospf_ctx_pol = (known after apply) + relation_fv_rs_vrf_validation_pol = (known after apply) + tenant_dn = (known after apply) } Plan: 3 to add, 0 to change, 0 to destroy. Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes aci_tenant.dcloud-tenant-2: Creating... aci_tenant.dcloud-tenant-2: Creation complete after 1s [id=uni/tn-dcloud-tenant-2] aci_vrf.vrf2-1: Creating... aci_vrf.vrf2-1: Creation complete after 1s [id=uni/tn-dcloud-tenant-2/ctx-vrf2-1] aci_bridge_domain.bd_192_168_1_0: Creating... aci_bridge_domain.bd_192_168_1_0: Creation complete after 2s [id=uni/tn-dcloud-tenant-2/BD-bd2-1] Apply complete! Resources: 3 added, 0 changed, 0 destroyed.","title":"1.2 Variable usage in config"},{"location":"terraform-vars/#13-provide-variable-value","text":"","title":"1.3 Provide variable value"},{"location":"terraform-vars/#131-manual-variable-values","text":"Now let's see how else we can provide variable values. In this case first let's modify our variables.tf file and add new variable called bd_name_2 which will have no default value and new variable bd_arp_flood_2 which will also have no default value. This way with no default value, Terraform will be forced to ask user to specify it manually during execution: Note We are not removing default value from existing bd_name or bd_arp_flood variables, as it would force terraform to change it on the existing bridge domain. We are adding new variables to show the difference between them variable \"bd_arp_flood_2\" { description = \"Specify whether ARP flooding is enabled. If flooding is disabled, unicast routing will be performed on the target IP address.\" validation { condition = (var.bd_arp_flood_2 == \"yes\") || (var.bd_arp_flood_2 == \"no\") error_message = \"Allowed values are \\\"yes\\\" and \\\"no\\\".\" } } variable \"bd_name_2\" { } Let's also add another bridge domain resource where we will verify our manual values. Add following block to the dcloud-tenant-2.tf file. Here we are only giving new name to the resource so that Terraform is aware that this is a new resource to be deployed. resource \"aci_bridge_domain\" \"bd_192_168_2_0\" { tenant_dn = aci_tenant.dcloud-tenant-2.id name = var.bd_name_2 arp_flood = var.bd_arp_flood_2 unicast_route = var.bd_unicast_routing unk_mac_ucast_act = var.bd_unk_ucast unk_mcast_act = \"flood\" relation_fv_rs_ctx = aci_vrf.vrf2-1.id } Now if we run terraform plan we will be asked to specify both values that don't have a default value in the variables file. Let's specify them, but at the same time test if our validation rule works. Please specify bd2 as bd_name_2 and enable as bd_arp_flood_2 juchowan@JUCHOWAN-M-D2P2 ACI-simulator % terraform plan var.bd_arp_flood_2 Specify whether ARP flooding is enabled. If flooding is disabled, unicast routing will be performed on the target IP address. Enter a value: enable var.bd_name_2 Enter a value: bd2 <omitted> \u2502 Error: Invalid value for variable \u2502 \u2502 on variables.tf line 31: \u2502 31: variable \"bd_arp_flood_2\" { \u2502 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502 \u2502 var.bd_arp_flood_2 is \"enable\" \u2502 \u2502 Allowed values are \"yes\" and \"no\". \u2502 \u2502 This was checked by the validation rule at variables.tf:33,3-13. \u2575 In this case Terraform will stop the plan and report incorrect value provided for bd_arp_flood_2 If we then specify yes to bd_arp_flood and bd2 for bd_name we will see that the plan generated output showing us what would be deployed in the APIC: juchowan@JUCHOWAN-M-D2P2 ACI-simulator % terraform plan var.bd_arp_flood_2 Specify whether ARP flooding is enabled. If flooding is disabled, unicast routing will be performed on the target IP address. Enter a value: yes var.bd_name_2 Enter a value: bd2 Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # aci_bridge_domain.bd_192_168_2_0 will be created + resource \"aci_bridge_domain\" \"bd_192_168_2_0\" { + annotation = \"orchestrator:terraform\" + arp_flood = \"yes\" + bridge_domain_type = (known after apply) + description = (known after apply) + ep_clear = (known after apply) + ep_move_detect_mode = (known after apply) + host_based_routing = (known after apply) + id = (known after apply) + intersite_bum_traffic_allow = (known after apply) + intersite_l2_stretch = (known after apply) + ip_learning = (known after apply) + ipv6_mcast_allow = (known after apply) + limit_ip_learn_to_subnets = (known after apply) + ll_addr = (known after apply) + mac = (known after apply) + mcast_allow = (known after apply) + multi_dst_pkt_act = (known after apply) + name = \"bd2\" + name_alias = (known after apply) + optimize_wan_bandwidth = (known after apply) + relation_fv_rs_bd_to_ep_ret = (known after apply) + relation_fv_rs_bd_to_nd_p = (known after apply) + relation_fv_rs_ctx = \"uni/tn-dcloud-tenant-2/ctx-vrf2-1\" + relation_fv_rs_igmpsn = (known after apply) + relation_fv_rs_mldsn = (known after apply) + tenant_dn = \"uni/tn-dcloud-tenant-2\" + unicast_route = \"yes\" + unk_mac_ucast_act = \"proxy\" + unk_mcast_act = \"flood\" + v6unk_mcast_act = (known after apply) + vmac = (known after apply) } Plan: 1 to add, 0 to change, 0 to destroy. Let's run terraform apply to see that this bridge domain would be deployed.","title":"1.3.1 Manual variable values"},{"location":"terraform-vars/#132-the-tfvars-file","text":"Manual method of providing values for our variables can be quite absorbing and prone to humar errors. Another, preferred way would be to include all our variable values into separate file called *.tfvars. Please create new file calle terraform.tfvars . Variable values in such file can be defined in a simple way: tenant_name = \"dcloud-tenant-2\" vrf_name = \"vrf2-1\" bd_name_2 = \"bd2\" bd_arp_flood_2 = \"no\" bd_unicast_routing = \"no\" bd_unk_ucast = \"proxy\" To each variable name we assign our preferred value. If you take a look closely you will see that for bd_arp_flood_2 and bd_unicast_routing we specify different values that we did for our bd2 bridge domain. If we run our terraform apply command now, we will see that terraform wants to do changes on our resource: <omitted> Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: ~ update in-place Terraform will perform the following actions: # aci_bridge_domain.bd_192_168_1_0 will be updated in-place ~ resource \"aci_bridge_domain\" \"bd_192_168_1_0\" { id = \"uni/tn-dcloud-tenant-2/BD-bd2-1\" name = \"bd2-1\" ~ unicast_route = \"yes\" -> \"no\" # (22 unchanged attributes hidden) } # aci_bridge_domain.bd_192_168_2_0 will be updated in-place ~ resource \"aci_bridge_domain\" \"bd_192_168_2_0\" { ~ arp_flood = \"yes\" -> \"no\" id = \"uni/tn-dcloud-tenant-2/BD-bd2\" name = \"bd2\" ~ unicast_route = \"yes\" -> \"no\" # (21 unchanged attributes hidden) } Plan: 0 to add, 2 to change, 0 to destroy. If we execute our command, Terraform will change the configuration on the APIC. Please verify in the GUI that these changes really reflect the configuration.","title":"1.3.2. The .tfvars file"},{"location":"terraform-vars/#2-for_each-loop","text":"This way of specifying variables and creating configuration is good if we want to deploy single object, however in case we have multiple bridge domains to deploy it may not be the most efficient way. By default a resource block configures one real infrastructure object. However, sometimes you want to manage several similar obejcts (like several bridge domains) without writing separate block for each one. Terraform has two ways to do this: count for_each The count argument takes a value of a whole number and Terraform will simply create that many instances of an object. This will not help us much with ACI, as every of these objects would have the same configuration. We want to have option to create multiple bridge domains, which may have different config options. The for_each argument can be included in a resource block, and its value is a map or set of strings. Terraform will create one instance for each member of that map or set. For this exercise we will work with maps that will include our bridge domains configuration. The map has an index value for each object instance. This index is used by terraform to reference resources and to get values we need from it to create objects. In our example let's create new folder for our workspace and call it ACI-2 . Inside this folder create three files that we will use: main.tf - which will hold our provider configuration and resource that will be deployed (tenant, vrf, bridge domains and subents) variables.tf - which will have definition of variables we will use, including their types and default values terraform.tfvars - which we will use to specify our desired values and confiuration of bridge domains Inside variables.tf file paste the following content: variable \"tenant_name\" { default = \"demo_tn\" } variable \"vrf_name\" { default = \"vrf1\" } variable \"bridge_domains\" { type = map(object({ name = string arp_flood = string unicast_routing = string unk_ucast = string subnet = string subnet_scope = list(string) })) default = { default_bd = { name = \"default\" arp_flood = \"yes\" unicast_routing = \"yes\" unk_ucast = \"proxy\" subnet = \"10.10.10.1/24\" subnet_scope = [\"private\"] } } } In this file we can see that we are defining three variables: tenant_name, vrf_name and bridge_domains. The bridge_domains variable is the interesting one, and it has type map of objects. One object will represent single bridge domain and its configuration. As part of this object we have our standard values to configure a BD like name, ARP flooding, unicast routing and unknown unicast flooding behaviour, but also subnet and subnet scope. Inside this variable first we define for those arguments what type they should take (string value for most of them and list of strings for subnet_scope). Then we specify default value that our map would take, in case we don't provide any values ourselves (inside terraform.tfvars file). Now we want to provide the values we want to use in the terraform.tffvars file, please paste following content: tenant_name = \"dcloud-tenant-3\" vrf_name = \"vrf1\" bridge_domains = { bd01 = { name = \"192.168.1.0_24_bd\" arp_flood = \"no\" unicast_routing = \"yes\" unk_ucast = \"proxy\" subnet = \"192.168.1.1/24\" subnet_scope = [\"private\"] }, bd02 = { name = \"192.168.2.0_24_bd\" arp_flood = \"yes\" unicast_routing = \"yes\" unk_ucast = \"proxy\" subnet = \"192.168.2.1/24\" subnet_scope = [\"private\",\"shared\"] } } Inside this file we specify our tenant and vrf name we want to use, and then we specify bridge domains map, which includes two bridge domains: bd01 of name 192.168.1.0_24_bd bd02 of name 192.168.2.0_24_bd That means our map has two objects. Each object is a key and value pair, where: first object key is bd01 and value is all the arguments we specify for it, including name, arp_flood, subnet etc second object key is bd02 and value is all the arguments we specify for it, including name, arp_flood, subnet etc You can see that our bridge domains differ in configuration of parameters. We have different names, arp flooding and subnet with scope. Now, since we are doing this demo in separate folder ACI-2 , we will have to initialize Terraform again and treat it as a separate deployment. For this reason inside the main.tf file we have to paste first our ACI provider configuration: terraform { required_providers { aci = { source = \"CiscoDevNet/aci\" version = \"2.5.2\" } } } provider \"aci\" { # Cisco ACI user name username = \"admin\" password = \"C1sco12345\" url = \"https://198.18.133.200\" insecure = true } Note Please remmeber in the terminal to change path to the right folder with cd command. Below that we will start describing our resources. We want to create four resources: tenant, vrf, bridge domain and subnet. resource \"aci_tenant\" \"dcloud-tenant-3\" { name = \"${var.tenant_name}\" description = \"This is a demo tenant created from Terraform\" } resource \"aci_vrf\" \"main\" { tenant_dn = aci_tenant.dcloud-tenant-3.id name = var.vrf_name } resource \"aci_bridge_domain\" \"bd\" { for_each = var.bridge_domains tenant_dn = aci_tenant.dcloud-tenant-3.id name = each.value.name arp_flood = each.value.arp_flood unicast_route = each.value.unicast_routing unk_mac_ucast_act = each.value.unk_ucast unk_mcast_act = \"flood\" relation_fv_rs_ctx = aci_vrf.main.id } resource \"aci_subnet\" \"subnets\" { for_each = var.bridge_domains parent_dn = aci_bridge_domain.bd[each.key].id ip = each.value.subnet scope = each.value.subnet_scope } The tenant and VRF resources are configured in a similar way as in previous demo, so they don't need description. They use standard references to variables we provided in the file. The interesting part is with the bridge_domain and subnet. In here we can see that each of these resources start with a special line for_each = var.bridge_domains This indicates that we want to create this resource once for every object in our variable called bridge_domains . In our terraform.tfvars we declared this variable to include two different bridge_domains, so we expect that this resource will be created twice. Each resource will be unique by the map key. In resource blocks where for_each is set, an additional each object is available in expressions, so you can modify the configuration of each instance. This object has two attributes: each.key - the map key corresponding to this instance each.value - the map value corresponding to this instance. In our resource block for bridge domain and subnet we can see that we are referring to arguments with each.value.<argumnet> or each.key , as an example: name = each.value.name We can also see that for the relation that needs to happen between bd and subnet we specify parent_dn with following expression using each.key : parent_dn = aci_bridge_domain.bd[each.key].id If we now run terraform plan and apply we will see that two bridge domains have been created according to our settings. Let's expand our tenant configuration by adding several EPGs, which we will create in the same way using for_each expression. First we need to define new variable corresponding to an EPG, it will take values of name and bridge domain : variable \"end_point_groups\" { type = map(object({ name = string bd = string })) } Next we specify desired configuration of our EPGs in terraform.tfvars file: end_point_groups = { epg1 = { name = \"epg1\", bd = \"bd01\" }, epg2 = { name = \"epg2\", bd = \"bd02\" } } Finally we add resource configuration in main.tf file: resource \"aci_application_profile\" \"ap1\" { tenant_dn = aci_tenant.dcloud-tenant-3.id name = \"ap1\" } resource \"aci_application_epg\" \"epg\" { for_each = var.end_point_groups application_profile_dn = aci_application_profile.ap1.id name = each.value.name relation_fv_rs_bd = aci_bridge_domain.bd[each.value.bd].id } You can see that endpoint groups also use the for_each function and create relation to a bridge domain that we specified. If you run apply now you will see additional AP and 2xEPG created. You can try adding new EPGs to the terraform.tfvars map of endpoint groups, to see that now it is easier to add new EPGs.","title":"2. For_each loop"},{"location":"terraform-vars/#3-conditionals-and-for-expression","text":"A conditional expression uses the value of a boolean expression to select one of two values. The condition can be any expression that resolves to a boolean value. This will usually be an expression that uses the equality, comparison, or logical operators. In our case we will use it to configure several parameters only under condition that other parameters are configured to a sprecific value. As an example let's take bridge domain configuration. On a high level there are two types of bridge domains we configure in ACI: L2 and L3. For L3 bridge domain we need to set unicast routing option to true, flooding arguments need to be adjusted, and subnet IP should be configured. For L2 bridge domains those parameters are not needed and BD should work in a flooding mode with no IP address. To specify that kind of configuration we can use conditionals in Terraform resource block. Please create new folder ACI-3 that we will use for this test, and under this folder three files: main.tf and paste the ACI provider configuration variables.tf which we will use to define variables type terraform.tfvars which we will use to specify our desired configuration Our variables.tf file will include following variabels: variable \"tenant_name\" { default = \"dcloud-tenant-4\" } variable \"vrf_name\" { default = \"vrf1\" } variable \"bridge_domains\" { type = map(object({ name = string arp_flood = string type = string gateway = string scope = list(string) })) default = { default_bd = { name = \"default\" arp_flood = \"yes\" type = \"L3\" gateway = \"192.168.1.1/24\" scope = [\"private\"] } } } In this example our bridge domains are configured with type (L2 or L3), name, ARP flooding mechanism, gateway (IP address) and its scope. We no longer define unicast routing and unknown unicast flooding mechanism. We want to set them based on the type of bridge domain we specify. Our terraform.tfvar file will have our desired config of bridge domains: bridge_domains = { bd01 = { name = \"192.168.1.0_24_bd\" arp_flood = \"yes\" type = \"L3\" gateway = \"192.168.1.1/24\" scope = [\"private\"] }, bd02 = { name = \"192.168.2.0_24_bd\" arp_flood = \"yes\" type = \"L2\" gateway = null scope = null } } One of the bridge domains is L3 and has IP address, while the second one is L2 and has no IP address. Inside main.tf we specify our resources configuration: resource \"aci_tenant\" \"tenant\" { name = \"${var.tenant_name}\" } resource \"aci_vrf\" \"vrf\" { tenant_dn = aci_tenant.tenant.id name = var.vrf_name } resource \"aci_bridge_domain\" \"bd\" { for_each = var.bridge_domains tenant_dn = aci_tenant.tenant.id name = each.value.name arp_flood = each.value.arp_flood unicast_route = each.value.type == \"L3\" ? \"yes\" : \"no\" unk_mac_ucast_act = each.value.type == \"L3\" ? \"proxy\" : \"flood\" unk_mcast_act = \"flood\" relation_fv_rs_ctx = aci_vrf.vrf.id } We are again using for_each loop to create multiple resources for our bridge domains and we assign values like tenant_dn, name, VRF and ARP_flooding in the same way as before, by refering the each.value.<argument name> . The difference is with assigning values for unicast routing and unknown unicast flooding mechanism. For these values we use conditional expression: unicast_route = each.value.type == \"L3\" ? \"yes\" : \"no\" This expression takes each.value.type of our object and verifies if the content of this type is \"L3\" . \"?\" symbol specified that we are checking this condition: if condition is true the result is that unicast_route argument will be assigned \"yes\" value if condition is false the result is that unicast_route argument will be assigned \"no\" value In the same way we can read the second line for unknown unicast flooding verifying condition that each.value.type is \"L3\" : if condition is true the result is that unk_mac_ucast_act argument will be assigned \"proxy\" value if condition is false the result is that unk_mac_ucast_act argument will be assigned \"flood\" value If we run our terraform init , plan and apply commands now, you will get two bridge domains configured with different settings based on the conditional expression. You can verify configuration in the APIC GUI. Note Remember to change to the correct folder in the Terminal Second thing we want to do is to create subnet resource, that will reflect our bridge domain IP address, but we want to do it only for L3 bridge domains. L2 BD doesn't require subnet to be configured and shouldn't have it at all. There would be no reason to deploy subnet resource for L2 bridge domain. If we simply use the for_each expression, we would deploy subnet for all bridge domains we specify in our map, but we want to verify a condition expression here too. This can be done with a for expression. A for expression creates a complex type value by transforming another complex type value. Each element in the input value can correspond to either one or zero values in the result, and an arbitrary expression can be used to transform each input element into an output element. We want to use the for expression to modify our for_each block: resource \"aci_subnet\" \"net\" { for_each = { for k, v in var.bridge_domains: k => v if v.type == \"L3\" } parent_dn = aci_bridge_domain.bd[each.key].id ip = each.value.gateway scope = each.value.scope } In this example we added the expression to the for_each block. This for loop is supposed to go iterate over every k (key) and v (value) pair in our map called var.bridge_domains and return those key and value pair only if the condition that \"type\" is set to \"L3\" is true. In reality for expression is returning (creating) new map variable that will consist only of key and value pairs of elements that pass our condition. If we wanted to take a look at those values, we would have our bridge_domains map that we declared: bridge_domains = { bd01 = { name = \"192.168.1.0_24_bd\" arp_flood = \"yes\" type = \"L3\" gateway = \"192.168.1.1/24\" scope = [\"private\"] }, bd02 = { name = \"192.168.2.0_24_bd\" arp_flood = \"yes\" type = \"L2\" gateway = null scope = null } } The new map variable created by for expression inside resource block would look like this: { bd01 = { name = \"192.168.1.0_24_bd\" arp_flood = \"yes\" type = \"L3\" gateway = \"192.168.1.1/24\" scope = [\"private\"] } } The second bd02 is not included in this temporary variable as it does not meet the condition. The for_each loop can now loop over this map, which only contains one object (one key, value pair) and inside the resource we refer to this object in the same way as we would normally use, with eack.key and each.value arguments: parent_dn = aci_bridge_domain.bd[each.key].id ip = each.value.gateway scope = each.value.scope If you run terraform plan and apply now, you will see that only one aci_subnet resource is being created for \"bd01\".","title":"3. Conditionals and for expression"},{"location":"terraform-vars/#4-data-sources","text":"Terraform data sources let you dynamically fetch data from APIs or other Terraform state backends. This allows to fetch data from APIC that was already created, and this data source is always read-only. We can use it to read objects in ACI that were already created before either manually or by default like a common tenant in ACI. Those data sources can then be used to create relations with different resources that we want to deploy. Data sources have separate documentation in the Terraform Registry As an example we will try to deploy new bridge domain with VRF that will be placed in the common tenant. In ACI from day-0 there is common tenant deployed with default VRF. We will reference it for our new bridge domain. First we have to create two data sources that will reflect our tenant and VRF: data \"aci_tenant\" \"common-tenant\" { name = \"common\" } data \"aci_vrf\" \"common-vrf\" { tenant_dn = data.aci_tenant.common-tenant.id name = \"default\" } Then we can start using references inside our new bridge domain: resource \"aci_bridge_domain\" \"bd-common\" { tenant_dn = aci_tenant.tenant.id name = \"bd-common\" arp_flood = \"yes\" unicast_route = \"no\" unk_mac_ucast_act = \"flood\" unk_mcast_act = \"flood\" relation_fv_rs_ctx = data.aci_vrf.common-vrf.id } The difference in a way we make references to data sources, instead of normal resource, is by adding data. before the data source name. If we run terraform plan and apply now, we will see that Terraform tries to create new bridge domain: Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # aci_bridge_domain.bd-common will be created + resource \"aci_bridge_domain\" \"bd-common\" { + annotation = \"orchestrator:terraform\" + arp_flood = \"yes\" + bridge_domain_type = (known after apply) + description = (known after apply) + ep_clear = (known after apply) + ep_move_detect_mode = (known after apply) + host_based_routing = (known after apply) + id = (known after apply) + intersite_bum_traffic_allow = (known after apply) + intersite_l2_stretch = (known after apply) + ip_learning = (known after apply) + ipv6_mcast_allow = (known after apply) + limit_ip_learn_to_subnets = (known after apply) + ll_addr = (known after apply) + mac = (known after apply) + mcast_allow = (known after apply) + multi_dst_pkt_act = (known after apply) + name = \"bd-common\" + name_alias = (known after apply) + optimize_wan_bandwidth = (known after apply) + relation_fv_rs_bd_to_ep_ret = (known after apply) + relation_fv_rs_bd_to_nd_p = (known after apply) + relation_fv_rs_ctx = \"uni/tn-common/ctx-default\" + relation_fv_rs_igmpsn = (known after apply) + relation_fv_rs_mldsn = (known after apply) + tenant_dn = \"uni/tn-dcloud-tenant-4\" + unicast_route = \"no\" + unk_mac_ucast_act = \"flood\" + unk_mcast_act = \"flood\" + v6unk_mcast_act = (known after apply) + vmac = (known after apply) } Plan: 1 to add, 0 to change, 0 to destroy. aci_tenant data source aci_vrf data source","title":"4. Data sources"},{"location":"use-cases-postman/","text":"Create sample use-cases with Postman When you think about automation, you should start with standardization. Definition of Use-Cases, Naming Convention, and AccessPolicies strategy is a minimum to start with. Once those three points are confirmed, you easily define your automation scripts. For use of our training we will work at three use-cases which very often appears in Customer's designs. Prerequisites - Tenant Common Configuration in Common Tenant is done once at initial config and then reused across EPGs. For that reason, your use-case should contain only Custom Tenant configuration. Configuration of Common Tenant can be deployed using already done Postman Requests. Prerequisite, you need to deploy, two VRFs, two Bridge-Domains and EPG_Shared. Use prepared CSV file for you - download from here . Run same collection which in previous excercise: Run ACI dCloud - results should be 200 OK . Use-Case no.1 Customer place network components in ACI shared tenant common . Custom tenants contain only EPGs, Domain associations and static-bindings for particular department. Below you can find code for Tenant Custom in use case 1. https://{{apic}}/api/node/mo/uni.json Tenant Custom Use-Case 1 { \"fvTenant\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}\" , \"name\" : \"{{tenantname}}\" }, \"children\" : [ { \"fvAp\" : { \"attributes\" : { \"name\" : \"{{ap}}\" }, \"children\" : [ { \"fvAEPg\" : { \"attributes\" : { \"name\" : \"{{epgname}}\" }, \"children\" : [ { \"fvRsDomAtt\" : { \"attributes\" : { \"tDn\" : \"uni/phys-{{domain}}\" } } }, { \"fvRsBd\" : { \"attributes\" : { \"annotation\" : \"\" , \"tnFvBDName\" : \"{{bdname}}\" } } } ] } } ] } } ] } } Run the code with new CSV file which you will use for custom-tenant definition, together with static biding associated to EPGs download from here . Tenant Custom Use-Case 1 static biding { \"fvTenant\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}\" , \"name\" : \"{{tenantname}}\" }, \"children\" : [ { \"fvAp\" : { \"attributes\" : { \"name\" : \"{{ap}}\" }, \"children\" : [ { \"fvAEPg\" : { \"attributes\" : { \"name\" : \"{{epgname}}\" }, \"children\" : [ { \"fvRsPathAtt\" : { \"attributes\" : { \"encap\" : \"vlan-{{vlan}}\" , \"instrImedcy\" : \"lazy\" , \"mode\" : \"{{mode}}\" , \"primaryEncap\" : \"unknown\" , \"tDn\" : \"topology/pod-{{pod}}/paths-{{leaf}}/pathep-[eth{{interface}}]\" , \"userdom\" : \":all:\" } } } ] } } ] } } ] } } Tip Assumption that Access-Policies, meaning VLAN pool and Domain Association is done beforehand. Otherwise use other Postman Requests to create proper vlan mapping Use-Case no.2 Customer place network components in ACI shared tenant common as well as Custom Tenant. Moreover custom tenants contain EPGs, Domain associations and static-bindings for particular department. How would you start preparation of such JSON Use-case definition? Try to build the JSON code yourself, prepare CSV file for input data. You can reuse existing JSON files you did so far. How to start the process: Identify components you want to include in your use-case Understand dependencies to not forget some objects (cross-tenant object model or security-policy, etc) Define where and how to connect external components (servers, routers, etc) Reuse done so far API calls, which from them suite most? Note If it's difficult or you would like to compare it, Solution for the Use-Case2 can be found under the link -- DoWnLoAd JSON -- DoWnLoAd CSV Use-Case no.3 Customer place SHARED network component in ACI shared tenant common . Custom tenants contain dedicated VRF, BDs and EPGs, Domain associations and static-bindings for particular department. L3out configuration for routing with external networks. Based on experience gained, build yourself another use-case in JSON. New thing in the Use-case, comparing to previous is L3out located in Custom Tenant. Please use API Inspector or Save manually created L3out object to understand MIT of it.","title":"Create sample use-cases with Postman"},{"location":"use-cases-postman/#create-sample-use-cases-with-postman","text":"When you think about automation, you should start with standardization. Definition of Use-Cases, Naming Convention, and AccessPolicies strategy is a minimum to start with. Once those three points are confirmed, you easily define your automation scripts. For use of our training we will work at three use-cases which very often appears in Customer's designs.","title":"Create sample use-cases with Postman"},{"location":"use-cases-postman/#prerequisites-tenant-common","text":"Configuration in Common Tenant is done once at initial config and then reused across EPGs. For that reason, your use-case should contain only Custom Tenant configuration. Configuration of Common Tenant can be deployed using already done Postman Requests. Prerequisite, you need to deploy, two VRFs, two Bridge-Domains and EPG_Shared. Use prepared CSV file for you - download from here . Run same collection which in previous excercise: Run ACI dCloud - results should be 200 OK .","title":"Prerequisites - Tenant Common"},{"location":"use-cases-postman/#use-case-no1","text":"Customer place network components in ACI shared tenant common . Custom tenants contain only EPGs, Domain associations and static-bindings for particular department. Below you can find code for Tenant Custom in use case 1. https://{{apic}}/api/node/mo/uni.json Tenant Custom Use-Case 1 { \"fvTenant\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}\" , \"name\" : \"{{tenantname}}\" }, \"children\" : [ { \"fvAp\" : { \"attributes\" : { \"name\" : \"{{ap}}\" }, \"children\" : [ { \"fvAEPg\" : { \"attributes\" : { \"name\" : \"{{epgname}}\" }, \"children\" : [ { \"fvRsDomAtt\" : { \"attributes\" : { \"tDn\" : \"uni/phys-{{domain}}\" } } }, { \"fvRsBd\" : { \"attributes\" : { \"annotation\" : \"\" , \"tnFvBDName\" : \"{{bdname}}\" } } } ] } } ] } } ] } } Run the code with new CSV file which you will use for custom-tenant definition, together with static biding associated to EPGs download from here . Tenant Custom Use-Case 1 static biding { \"fvTenant\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}\" , \"name\" : \"{{tenantname}}\" }, \"children\" : [ { \"fvAp\" : { \"attributes\" : { \"name\" : \"{{ap}}\" }, \"children\" : [ { \"fvAEPg\" : { \"attributes\" : { \"name\" : \"{{epgname}}\" }, \"children\" : [ { \"fvRsPathAtt\" : { \"attributes\" : { \"encap\" : \"vlan-{{vlan}}\" , \"instrImedcy\" : \"lazy\" , \"mode\" : \"{{mode}}\" , \"primaryEncap\" : \"unknown\" , \"tDn\" : \"topology/pod-{{pod}}/paths-{{leaf}}/pathep-[eth{{interface}}]\" , \"userdom\" : \":all:\" } } } ] } } ] } } ] } } Tip Assumption that Access-Policies, meaning VLAN pool and Domain Association is done beforehand. Otherwise use other Postman Requests to create proper vlan mapping","title":"Use-Case no.1"},{"location":"use-cases-postman/#use-case-no2","text":"Customer place network components in ACI shared tenant common as well as Custom Tenant. Moreover custom tenants contain EPGs, Domain associations and static-bindings for particular department. How would you start preparation of such JSON Use-case definition? Try to build the JSON code yourself, prepare CSV file for input data. You can reuse existing JSON files you did so far. How to start the process: Identify components you want to include in your use-case Understand dependencies to not forget some objects (cross-tenant object model or security-policy, etc) Define where and how to connect external components (servers, routers, etc) Reuse done so far API calls, which from them suite most? Note If it's difficult or you would like to compare it, Solution for the Use-Case2 can be found under the link -- DoWnLoAd JSON -- DoWnLoAd CSV","title":"Use-Case no.2"},{"location":"use-cases-postman/#use-case-no3","text":"Customer place SHARED network component in ACI shared tenant common . Custom tenants contain dedicated VRF, BDs and EPGs, Domain associations and static-bindings for particular department. L3out configuration for routing with external networks. Based on experience gained, build yourself another use-case in JSON. New thing in the Use-case, comparing to previous is L3out located in Custom Tenant. Please use API Inspector or Save manually created L3out object to understand MIT of it.","title":"Use-Case no.3"}]}