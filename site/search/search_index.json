{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to CiscoLive 2023 - Instructor Lab about ACI Multicloud Speakers: Karol Okraska , CX Delivery Architect, Cisco Systems, Inc. Marcin Duma , CX Delivery Architect, Cisco Systems, Inc. Cisco ACI support of Public Cloud infrastructure Cisco Cloud Network Controller (formerly Cloud APIC) provides enterprises with networking tools necessary to accelerate their hybrid-cloud and/or multicloud journey. Utilizing cloud-native constructs, the solution enables automation that accelerates infrastructure deployment and governance and simplifies management to easily connect workloads across multicloud environments. The Cisco Cloud Network Controller vision is to support enhanced observability, operations, and troubleshooting across the entire environment. Cisco Cloud Network Controller enables: \u25cf Seamless connectivity for any workload at scale across any location \u25cf Operational simplicity and visibility across a vast multisite, multicloud \u25cf data-center network \u25cf Easy L4-7 services integration \u25cf Consistent security and segmentation \u25cf Business continuity and disaster recovery Cloud Network Controller components: Cisco Cloud Network Controller is the main architectural component of this multicloud solution. It is the unified point of automation and management for the solution fabric including network and security policy, health monitoring, and optimizes performance and agility. The complete solution includes: \u25cf Cisco Cloud Network Controller(deployed in each Public Cloud which is to be managed) \u25cf Cisco Nexus Dashboard (in our lab deployed in AWS Cloud) - Multicloud networking orchestration and policy management, disaster recovery, and high availability, as well as provisioning and health monitoring. \u25cf Cisco Catalyst\u00ae 8000V - deployed in Public Clouds, allowing for communication with other Clouds or on-premises datacenter. Responsible for traffic secuirty and end to end policy enforcement. Topology In this lab we will be using two(2) Public Cloud Provders: \u25cf Microsoft Azure \u25cf Amazon AWS However Cloud Network Controller (CNC) is also supported on Google Cloud. In both Public Enviroments CNC will be deployed. Nexus Dashboard orchestrator will be run on Nexus Dashboard appliance deployed in AWS cloud. Due to time concern of this lab, Cloud Network Controller and Nexus Dasboard along with Nexus Dashboard Orchestrator Application will be pre-installed for you and ready for configuration. Intial lab diagram: Lab agenda 1. Infrastructure veryfication - login and access 2. Site onboarding in Nexus Dashboard 3. Multisite infrastructure configuration 4. Multisite configuration check 5. Tenant creation and Public Cloud Trust configuration 6. Three(3) common use-cases configuration and veryfcation - Stretched VRF across Public Clouds with Cloud-local EPGs - Internet Gateway configuration in AWS - Inter-Tenant routing Step by step configuration will guide you towards final topology as indicated in the picture below: Enjoy!","title":"Home"},{"location":"#welcome-to-ciscolive-2023-instructor-lab-about-aci-multicloud","text":"Speakers: Karol Okraska , CX Delivery Architect, Cisco Systems, Inc. Marcin Duma , CX Delivery Architect, Cisco Systems, Inc.","title":"Welcome to CiscoLive 2023 - Instructor Lab about ACI Multicloud"},{"location":"#cisco-aci-support-of-public-cloud-infrastructure","text":"Cisco Cloud Network Controller (formerly Cloud APIC) provides enterprises with networking tools necessary to accelerate their hybrid-cloud and/or multicloud journey. Utilizing cloud-native constructs, the solution enables automation that accelerates infrastructure deployment and governance and simplifies management to easily connect workloads across multicloud environments. The Cisco Cloud Network Controller vision is to support enhanced observability, operations, and troubleshooting across the entire environment. Cisco Cloud Network Controller enables: \u25cf Seamless connectivity for any workload at scale across any location \u25cf Operational simplicity and visibility across a vast multisite, multicloud \u25cf data-center network \u25cf Easy L4-7 services integration \u25cf Consistent security and segmentation \u25cf Business continuity and disaster recovery","title":"Cisco ACI support of Public Cloud infrastructure"},{"location":"#cloud-network-controller-components","text":"Cisco Cloud Network Controller is the main architectural component of this multicloud solution. It is the unified point of automation and management for the solution fabric including network and security policy, health monitoring, and optimizes performance and agility. The complete solution includes: \u25cf Cisco Cloud Network Controller(deployed in each Public Cloud which is to be managed) \u25cf Cisco Nexus Dashboard (in our lab deployed in AWS Cloud) - Multicloud networking orchestration and policy management, disaster recovery, and high availability, as well as provisioning and health monitoring. \u25cf Cisco Catalyst\u00ae 8000V - deployed in Public Clouds, allowing for communication with other Clouds or on-premises datacenter. Responsible for traffic secuirty and end to end policy enforcement.","title":"Cloud Network Controller components:"},{"location":"#topology","text":"In this lab we will be using two(2) Public Cloud Provders: \u25cf Microsoft Azure \u25cf Amazon AWS However Cloud Network Controller (CNC) is also supported on Google Cloud. In both Public Enviroments CNC will be deployed. Nexus Dashboard orchestrator will be run on Nexus Dashboard appliance deployed in AWS cloud. Due to time concern of this lab, Cloud Network Controller and Nexus Dasboard along with Nexus Dashboard Orchestrator Application will be pre-installed for you and ready for configuration. Intial lab diagram:","title":"Topology"},{"location":"#lab-agenda","text":"","title":"Lab agenda"},{"location":"#1-infrastructure-veryfication-login-and-access","text":"","title":"1. Infrastructure veryfication - login and access"},{"location":"#2-site-onboarding-in-nexus-dashboard","text":"","title":"2. Site onboarding in Nexus Dashboard"},{"location":"#3-multisite-infrastructure-configuration","text":"","title":"3. Multisite infrastructure configuration"},{"location":"#4-multisite-configuration-check","text":"","title":"4. Multisite configuration check"},{"location":"#5-tenant-creation-and-public-cloud-trust-configuration","text":"","title":"5. Tenant creation and Public Cloud Trust configuration"},{"location":"#6-three3-common-use-cases-configuration-and-veryfcation","text":"","title":"6. Three(3) common use-cases configuration and veryfcation"},{"location":"#-stretched-vrf-across-public-clouds-with-cloud-local-epgs","text":"","title":"- Stretched VRF across Public Clouds with Cloud-local EPGs"},{"location":"#-internet-gateway-configuration-in-aws","text":"","title":"- Internet Gateway configuration in AWS"},{"location":"#-inter-tenant-routing","text":"Step by step configuration will guide you towards final topology as indicated in the picture below: Enjoy!","title":"- Inter-Tenant routing"},{"location":"LAB_access/","text":"Connectivity Check 1. Lab access general description This lab is fully hosted in Public Cloud infrastrucure, both AWS and Azure, no local resources are used. You will have access to Public Cloud Console Interface, as well as you will be able to login to necessary resources and appliances. 2. AWS and Azure console access details: Login link for AWS console: https://console.aws.amazon.com Login link for Azure portal: https://portal.azure.com 3. Accessing Cloud Network Controller and Nexus Dashboard. All appliances are addressed with Public IP address, hence it can be accessed from local browser without any additional settings or VPN connections. List of IP addreses:","title":"Accessing the Lab Environment"},{"location":"LAB_access/#connectivity-check","text":"","title":"Connectivity Check"},{"location":"LAB_access/#1-lab-access-general-description","text":"This lab is fully hosted in Public Cloud infrastrucure, both AWS and Azure, no local resources are used. You will have access to Public Cloud Console Interface, as well as you will be able to login to necessary resources and appliances.","title":"1. Lab access general description"},{"location":"LAB_access/#2-aws-and-azure-console-access-details","text":"Login link for AWS console: https://console.aws.amazon.com Login link for Azure portal: https://portal.azure.com","title":"2. AWS and Azure console access details:"},{"location":"LAB_access/#3-accessing-cloud-network-controller-and-nexus-dashboard","text":"All appliances are addressed with Public IP address, hence it can be accessed from local browser without any additional settings or VPN connections. List of IP addreses:","title":"3. Accessing Cloud Network Controller and Nexus Dashboard."},{"location":"aws-trust/","text":"Nexus Dasboard Orchestrator Tenant AWS Trust Configuration In previus step we select Tenant configuration Mode as Trusted, hence trust need to be made to allow for configuration. AWS User Account Login Each User POD has two(2) AWS Accounts. 1st for Infrastrucre Configuration 2nd for Tenant Configuration Open AWS console via browser Note As you are already logged into AWS for Infrastructure Account, you can logout or use Incognito Mode (preferably), or different browser. Select IAM user, provide Account ID and hit \"Next\" Note For this login please use AWS User Account ID Provide Username and password and hit \"Sign In\" You will land on the welcome page of AWS cloud console. AWS Cloud Network Controller Login Using the IP found during site onboarding (can be also found in POD guide), connect via browser to CNC GUI for AWS instance. Make sure to use the same incognito mode as for AWS User tenant. Note The trust configuration CloudFormation script needs to be executed on AWS User Account! Provide Credentials and hit Login Username: admin Password: CiscoLive2023! Hit \"Get started\" to view Cloud Network Controller Dashboard. Look on the Dashboard -> Application Management -> Tenants We can see newly create tenant in the list. Double-click on the tenant name \"Tenant-01\" to open it. Under the AWS Account section there is warning related to our Trust configuration, along with the \"Run the CloudFormation Template\" hyperlink. Click on the hyperlink to run CloudFormation Script. You will be then redirected to AWS console CloudFormation Stack configuration. All the parameters are already filled in, hit \"Next\" to continue. In Specify stack details step - fill in stack name and hit \"Next\" Stack Name: CNC-AWS-Trust Parameters: none In Configure stack options step - leave all options as default and hit \"Next\" In Review CNC-AWS-Trust step - leave all options as default, scroll down to \"Capabilities\" section and check the checkbox for resource creation and hit \"Submit\" After couple of seconds(use refresh button) - stack will be completed and we are good to go.","title":"AWS Trust"},{"location":"aws-trust/#nexus-dasboard-orchestrator-tenant-aws-trust-configuration","text":"In previus step we select Tenant configuration Mode as Trusted, hence trust need to be made to allow for configuration.","title":"Nexus Dasboard Orchestrator Tenant AWS Trust Configuration"},{"location":"aws-trust/#aws-user-account-login","text":"Each User POD has two(2) AWS Accounts. 1st for Infrastrucre Configuration 2nd for Tenant Configuration Open AWS console via browser Note As you are already logged into AWS for Infrastructure Account, you can logout or use Incognito Mode (preferably), or different browser. Select IAM user, provide Account ID and hit \"Next\" Note For this login please use AWS User Account ID Provide Username and password and hit \"Sign In\" You will land on the welcome page of AWS cloud console.","title":"AWS User Account Login"},{"location":"aws-trust/#aws-cloud-network-controller-login","text":"Using the IP found during site onboarding (can be also found in POD guide), connect via browser to CNC GUI for AWS instance. Make sure to use the same incognito mode as for AWS User tenant. Note The trust configuration CloudFormation script needs to be executed on AWS User Account! Provide Credentials and hit Login Username: admin Password: CiscoLive2023! Hit \"Get started\" to view Cloud Network Controller Dashboard. Look on the Dashboard -> Application Management -> Tenants We can see newly create tenant in the list. Double-click on the tenant name \"Tenant-01\" to open it. Under the AWS Account section there is warning related to our Trust configuration, along with the \"Run the CloudFormation Template\" hyperlink. Click on the hyperlink to run CloudFormation Script. You will be then redirected to AWS console CloudFormation Stack configuration. All the parameters are already filled in, hit \"Next\" to continue. In Specify stack details step - fill in stack name and hit \"Next\" Stack Name: CNC-AWS-Trust Parameters: none In Configure stack options step - leave all options as default and hit \"Next\" In Review CNC-AWS-Trust step - leave all options as default, scroll down to \"Capabilities\" section and check the checkbox for resource creation and hit \"Submit\" After couple of seconds(use refresh button) - stack will be completed and we are good to go.","title":"AWS Cloud Network Controller Login"},{"location":"guacamole/","text":"Copy and Paste in and out Guacamole Apache Guacamole serves as a proxy to provide clientless access to other servers. The copy paste function is achieved through browser. Guacamole Menu The guacamole menu is a sidebar which is hidden. On a Windows device with an external keyboard, the guacamole menu is displayed by pressing Ctrl+Alt+Shift On a Mac device with an external keyboard, the guacamole menu is displayed by pressing Ctrl+Command+Shift 3. On a device that doesn\u2019t have a keyboard, e.g. mobile or touchscreen device, the guacamole menu is displayed by swiping right from the left edge of the screen. To Hide the guacamole menu, press Ctrl+Alt+Shift/ Ctrl+Command+Shift or swipe left across the screen again. Guacamole Use The Clipboard text area functions as an interface between the remote clipboard and local clipboard. Text from the local clipboard can be pasted into the text area, and the text is sent to the remote clipboard. 2. After the text is copied to the remote clipboard, the Guacamole menu can be closed. The text from the clipboard can be pasted in the desired location. 3. Similarly, copying text from the remote desktop sends the text to the Clipboard text area on the Guacamole Menu, which can then be copied to the local clipboard.","title":"Copy and Paste in and out Guacamole"},{"location":"guacamole/#copy-and-paste-in-and-out-guacamole","text":"Apache Guacamole serves as a proxy to provide clientless access to other servers. The copy paste function is achieved through browser.","title":"Copy and Paste in and out Guacamole"},{"location":"guacamole/#guacamole-menu","text":"The guacamole menu is a sidebar which is hidden. On a Windows device with an external keyboard, the guacamole menu is displayed by pressing Ctrl+Alt+Shift On a Mac device with an external keyboard, the guacamole menu is displayed by pressing Ctrl+Command+Shift 3. On a device that doesn\u2019t have a keyboard, e.g. mobile or touchscreen device, the guacamole menu is displayed by swiping right from the left edge of the screen. To Hide the guacamole menu, press Ctrl+Alt+Shift/ Ctrl+Command+Shift or swipe left across the screen again.","title":"Guacamole Menu"},{"location":"guacamole/#guacamole-use","text":"The Clipboard text area functions as an interface between the remote clipboard and local clipboard. Text from the local clipboard can be pasted into the text area, and the text is sent to the remote clipboard. 2. After the text is copied to the remote clipboard, the Guacamole menu can be closed. The text from the clipboard can be pasted in the desired location. 3. Similarly, copying text from the remote desktop sends the text to the Clipboard text area on the Guacamole Menu, which can then be copied to the local clipboard.","title":"Guacamole Use"},{"location":"infra-config-check/","text":"","title":"Infra config check"},{"location":"infra-config-msite/","text":"Infrastructure configuration - Multi-Site configuration In this lab section we will use Nexus Dashboard orchestrator to connect 2 ACI Fabrics together. Nexus Dashboard Orchestrator(NDO) 1. NDO Introduction On the Site list hit \"Continue\" and then \"Done\" button to finish Initial Setup from previous chapter. Click \"Go To Dashboard\" in bottom right corner - you should be moved to ND Dashboard with Site Map In the navigation menu on the left go to \"Admin Console\" You will be moved to Admin Console page, where you can see Health of your Nexus Dashboard and status of sites and services. Navigate to \"Sites\" tab and verify if \"Connectivity Status\" of you sites is \"Up\" Navigate to \"Sevices\" tab to see what applications are installed on this Nexus Dashboard Nexus Dashboard Orchestrator is already installed and enabled under \"Service Catalog\" inside \"Installed Service\" Hit \"Open\" button in order to login You will be now redirected to Nexus Dashboard Orchestrator Dashboard. 2. NDO Sites Onboarding In order to configure AWS and Azure sites from NDO, added previously on Nexus Dashboard , sites have to be Maneged and have Site ID assigned. Navigate to Sites : Click on \"Unmanaged\" box under State Column for each site and assign Site ID , confirming with \"Add\" button. For AWS Site - set ID **10** For Azure Site - set ID **20** After this operation - both sites should be visable as Managed 3. Site Connectivity Configuration In Next Step we would configure Infrastructure to connect 2 Cloud ACI Sites togehter. Navigate to Infrastructure -> Site Connectivity -> Configure button On the General Settings Page, scroll down to OSPF Configuration and fill in OSPF Area ID to value 0.0.0.0 , leave other setting as default. Go to tab \"IPSec Tunnel Subnet Pools\" , click Add IP address button and add subnet Subnet: 192.168.255.0/24 Confirm with checkbox button. In the left navigation bar, under the Sites bar, click on first site CNC-AWS-01 Enable the site for MultiSite by checking the checkbox \"ACI Multisite\" also enable \"Contract Based Routing\" Settings: - ACI Multisite - checked - Contract Based Routing - checked Click the \"Add Site\" button to cross-connect 2 sites. Under Connected to Site select Select a Site hyperlink. Select Azure fabric and hit Select For the Connection Type select Public Internet and hit Ok . Leave rest of the setting as default, you can review them. Move to CNC-Azure-01 Site and also enable the site for MultiSite by checking the checkbox \"ACI Multisite\" - simiar as in previous point. Settings: - ACI Multisite - checked - Contract Based Routing - checked Note that second site is already selected for Inter-Site Connectivity Once done, locate the Deploy button on top the screen, click it and Select \"Deploy Only\" , hit Yes for confirmation. As you hit Deploy button, NDO will now configure CNC and Cloud Routes. - IPSec tunnels, full mesh between all 4 routers (2 per Cloud) - OSPF routing over IPSec - BGP EVPN peering for prefixes exchange Inter-site connectivity veryfication It may take 5-10 minutes for configuration to be pushed and Tunnels to be established. At this point we configured this part of our topology diagram: 1. Nexush Dashboard view Nexus Dashboard allows for monitoring of Inter-Site connectivity. On the Left navigation page click \"Dashboard\" to go back to main Connectivity View. Take a look into green line between AWS and Azure site(you can use use magnifying tool for better view). Green line indicated that all is fine with connectivity. On the Left navigation page click \"Infrastructure\" -> \"Site Connectivity\" and scroll down on a page. Under the site list, locate \"Show Connectivity Status\" and click on it. Check the connectivity status for both BGP EVPN as well as Tunnel Status. There should be 4 UP BGP sessions, as well as 4 Tunnels which are UP between Sites. Inter-site connectivity veryfication (Cloud Routers) 1. IPSec tunnel veryfication Open Putty client from desktop and put IP address of Cloud router. Note Cloud Routers IP address are avaibale in POD details IP address schema. Login with username \"csradmin\" and password \"CiscoLive2023!\" Execute command \"show ip int brief | include Tunnel\" show ip int brief | include Tunnel Expected output: ct_routerp_eu-central-1_0_0#show ip int brief | include Tunnel Tunnel0 10.10.0.52 YES unset up up Tunnel1 169.254.112.1 YES NVRAM up up Tunnel6 192.168.255.4 YES NVRAM up up Tunnel7 192.168.255.2 YES NVRAM up up ct_routerp_eu-central-1_0_0# Verify that all 4 tunnels are up/up. Note also that Tunnel6 and Tunnel7 are addressed from subnet which was specified in the beginning of Multiste Configuration. Those 2 tunnel are configured towards two (2) Catalyst 8000V Routers in another site! 2. OSPF adjacency veryfication Execute command \"show ip ospf neighbor\" show ip ospf neighbor Expected output: ct_routerp_eu-central-1_0_0#show ip ospf neighbor Neighbor ID Pri State Dead Time Address Interface 10.20.0.20 0 FULL/ - 00:00:33 192.168.255.3 Tunnel7 10.20.0.68 0 FULL/ - 00:00:38 192.168.255.5 Tunnel6 ct_routerp_eu-central-1_0_0# Verify that both sessions are in FULL State. 2. BGP EVPN veryfication Execute command \"show bgp l2vpn evpn summary\" show bgp l2vpn evpn summary Expected output: ct_routerp_eu-central-1_0_0#show bgp l2vpn evpn summary BGP router identifier 10.10.0.20, local AS number 65110 BGP table version is 1, main routing table version 1 Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd 10.20.0.52 4 65200 96 94 1 0 0 01:22:47 0 10.20.0.116 4 65200 95 94 1 0 0 01:22:52 0 ct_routerp_eu-central-1_0_0# Verify that both sessions are Up (O in \"State/PfxRcd\" column)","title":"Multisite configuration"},{"location":"infra-config-msite/#infrastructure-configuration-multi-site-configuration","text":"In this lab section we will use Nexus Dashboard orchestrator to connect 2 ACI Fabrics together.","title":"Infrastructure configuration - Multi-Site configuration"},{"location":"infra-config-msite/#nexus-dashboard-orchestratorndo","text":"","title":"Nexus Dashboard Orchestrator(NDO)"},{"location":"infra-config-msite/#1-ndo-introduction","text":"On the Site list hit \"Continue\" and then \"Done\" button to finish Initial Setup from previous chapter. Click \"Go To Dashboard\" in bottom right corner - you should be moved to ND Dashboard with Site Map In the navigation menu on the left go to \"Admin Console\" You will be moved to Admin Console page, where you can see Health of your Nexus Dashboard and status of sites and services. Navigate to \"Sites\" tab and verify if \"Connectivity Status\" of you sites is \"Up\" Navigate to \"Sevices\" tab to see what applications are installed on this Nexus Dashboard Nexus Dashboard Orchestrator is already installed and enabled under \"Service Catalog\" inside \"Installed Service\" Hit \"Open\" button in order to login You will be now redirected to Nexus Dashboard Orchestrator Dashboard.","title":"1. NDO Introduction"},{"location":"infra-config-msite/#2-ndo-sites-onboarding","text":"In order to configure AWS and Azure sites from NDO, added previously on Nexus Dashboard , sites have to be Maneged and have Site ID assigned. Navigate to Sites : Click on \"Unmanaged\" box under State Column for each site and assign Site ID , confirming with \"Add\" button. For AWS Site - set ID **10** For Azure Site - set ID **20** After this operation - both sites should be visable as Managed","title":"2. NDO Sites Onboarding"},{"location":"infra-config-msite/#3-site-connectivity-configuration","text":"In Next Step we would configure Infrastructure to connect 2 Cloud ACI Sites togehter. Navigate to Infrastructure -> Site Connectivity -> Configure button On the General Settings Page, scroll down to OSPF Configuration and fill in OSPF Area ID to value 0.0.0.0 , leave other setting as default. Go to tab \"IPSec Tunnel Subnet Pools\" , click Add IP address button and add subnet Subnet: 192.168.255.0/24 Confirm with checkbox button. In the left navigation bar, under the Sites bar, click on first site CNC-AWS-01 Enable the site for MultiSite by checking the checkbox \"ACI Multisite\" also enable \"Contract Based Routing\" Settings: - ACI Multisite - checked - Contract Based Routing - checked Click the \"Add Site\" button to cross-connect 2 sites. Under Connected to Site select Select a Site hyperlink. Select Azure fabric and hit Select For the Connection Type select Public Internet and hit Ok . Leave rest of the setting as default, you can review them. Move to CNC-Azure-01 Site and also enable the site for MultiSite by checking the checkbox \"ACI Multisite\" - simiar as in previous point. Settings: - ACI Multisite - checked - Contract Based Routing - checked Note that second site is already selected for Inter-Site Connectivity Once done, locate the Deploy button on top the screen, click it and Select \"Deploy Only\" , hit Yes for confirmation. As you hit Deploy button, NDO will now configure CNC and Cloud Routes. - IPSec tunnels, full mesh between all 4 routers (2 per Cloud) - OSPF routing over IPSec - BGP EVPN peering for prefixes exchange","title":"3. Site Connectivity Configuration"},{"location":"infra-config-msite/#inter-site-connectivity-veryfication","text":"It may take 5-10 minutes for configuration to be pushed and Tunnels to be established. At this point we configured this part of our topology diagram:","title":"Inter-site connectivity veryfication"},{"location":"infra-config-msite/#1-nexush-dashboard-view","text":"Nexus Dashboard allows for monitoring of Inter-Site connectivity. On the Left navigation page click \"Dashboard\" to go back to main Connectivity View. Take a look into green line between AWS and Azure site(you can use use magnifying tool for better view). Green line indicated that all is fine with connectivity. On the Left navigation page click \"Infrastructure\" -> \"Site Connectivity\" and scroll down on a page. Under the site list, locate \"Show Connectivity Status\" and click on it. Check the connectivity status for both BGP EVPN as well as Tunnel Status. There should be 4 UP BGP sessions, as well as 4 Tunnels which are UP between Sites.","title":"1. Nexush Dashboard view"},{"location":"infra-config-msite/#inter-site-connectivity-veryfication-cloud-routers","text":"","title":"Inter-site connectivity veryfication  (Cloud Routers)"},{"location":"infra-config-msite/#1-ipsec-tunnel-veryfication","text":"Open Putty client from desktop and put IP address of Cloud router. Note Cloud Routers IP address are avaibale in POD details IP address schema. Login with username \"csradmin\" and password \"CiscoLive2023!\" Execute command \"show ip int brief | include Tunnel\" show ip int brief | include Tunnel Expected output: ct_routerp_eu-central-1_0_0#show ip int brief | include Tunnel Tunnel0 10.10.0.52 YES unset up up Tunnel1 169.254.112.1 YES NVRAM up up Tunnel6 192.168.255.4 YES NVRAM up up Tunnel7 192.168.255.2 YES NVRAM up up ct_routerp_eu-central-1_0_0# Verify that all 4 tunnels are up/up. Note also that Tunnel6 and Tunnel7 are addressed from subnet which was specified in the beginning of Multiste Configuration. Those 2 tunnel are configured towards two (2) Catalyst 8000V Routers in another site!","title":"1. IPSec tunnel veryfication"},{"location":"infra-config-msite/#2-ospf-adjacency-veryfication","text":"Execute command \"show ip ospf neighbor\" show ip ospf neighbor Expected output: ct_routerp_eu-central-1_0_0#show ip ospf neighbor Neighbor ID Pri State Dead Time Address Interface 10.20.0.20 0 FULL/ - 00:00:33 192.168.255.3 Tunnel7 10.20.0.68 0 FULL/ - 00:00:38 192.168.255.5 Tunnel6 ct_routerp_eu-central-1_0_0# Verify that both sessions are in FULL State.","title":"2. OSPF adjacency veryfication"},{"location":"infra-config-msite/#2-bgp-evpn-veryfication","text":"Execute command \"show bgp l2vpn evpn summary\" show bgp l2vpn evpn summary Expected output: ct_routerp_eu-central-1_0_0#show bgp l2vpn evpn summary BGP router identifier 10.10.0.20, local AS number 65110 BGP table version is 1, main routing table version 1 Neighbor V AS MsgRcvd MsgSent TblVer InQ OutQ Up/Down State/PfxRcd 10.20.0.52 4 65200 96 94 1 0 0 01:22:47 0 10.20.0.116 4 65200 95 94 1 0 0 01:22:52 0 ct_routerp_eu-central-1_0_0# Verify that both sessions are Up (O in \"State/PfxRcd\" column)","title":"2. BGP EVPN veryfication"},{"location":"infra-config/","text":"Infrastructure configuration - Site onboarding Find Public IP Address of Nexus Dashboard and Cisco Cloud Network Controller in AWS 1. Open AWS console via browser 2. Select IAM user, provide Account ID and hit \"Next\" 3. Provide Username and password and hit \"Sign In\" 4. In AWS Search Bar type \"EC2\" and Select \"EC2\" Services Tab 5. From \"Resources\" select \"Instances (Running)\" 6. On the Instances List scroll to the right and note down \"Public IPv4 address\" Find Public IP Address of Cisco Cloud Network Controller in Azure 1. Open Azure portal via browser 2. Enter your username along with domain and hit \"Next\" 3. Provide password and hit \"Sign In\" 4. Use \"Skip for now\" option for Account Protection 5. In Search bar look for \"virtual machines\" and select from services 6. On the Virtual Machines list scroll to the right and note down \"Public IP address\" Nexus Dashboard site onboarding 1. Login to Nexus Dasboard using IP collected above and login with provided credentials 2. Hit \"Get started\" and Setup screen will pop-up 3. On the \"Add Sites\" section hit \"Begin\" 4. Hit \"Add Site\" button 5. Add AWS Cloud Network Controller Site Type: \"Cloud Network Controller\" Name: \"CNC-AWS-01\" Hostname/IP Address: \"Public IP of Cloud Network Controller for AWS collected above\" Username: admin Password: CiscoLive2023! Login Domain: empty 6. Hit \"Save\" \"AWS site should be added now, stay on the same page! \" 7. Hit \"Add site button one more time to add Azure site 8. Add Azure site details Site Type: \"Cloud Network Controller\" Name: \"CNC-Azure-01\" Hostname/IP Address: \"Public IP of Cloud Network Controller from Azure collected above\" Username: admin Password: CiscoLive2023! Login Domain: empty 6. Hit \"Save\" Check the site list You should see both sites added under the site list.","title":"Sites onboarding"},{"location":"infra-config/#infrastructure-configuration-site-onboarding","text":"","title":"Infrastructure configuration - Site onboarding"},{"location":"infra-config/#find-public-ip-address-of-nexus-dashboard-and-cisco-cloud-network-controller-in-aws","text":"","title":"Find Public IP Address of Nexus Dashboard and Cisco Cloud Network Controller in AWS"},{"location":"infra-config/#1-open-aws-console-via-browser","text":"","title":"1. Open AWS console via browser"},{"location":"infra-config/#2-select-iam-user-provide-account-id-and-hit-next","text":"","title":"2. Select IAM user,  provide Account ID and hit \"Next\""},{"location":"infra-config/#3-provide-username-and-password-and-hit-sign-in","text":"","title":"3. Provide Username and password and hit \"Sign In\""},{"location":"infra-config/#4-in-aws-search-bar-type-ec2-and-select-ec2-services-tab","text":"","title":"4. In AWS Search Bar type \"EC2\" and Select \"EC2\" Services Tab"},{"location":"infra-config/#5-from-resources-select-instances-running","text":"","title":"5. From \"Resources\" select \"Instances (Running)\""},{"location":"infra-config/#6-on-the-instances-list-scroll-to-the-right-and-note-down-public-ipv4-address","text":"","title":"6. On the Instances List scroll to the right and note down \"Public IPv4 address\""},{"location":"infra-config/#find-public-ip-address-of-cisco-cloud-network-controller-in-azure","text":"","title":"Find Public IP Address of Cisco Cloud Network Controller in Azure"},{"location":"infra-config/#1-open-azure-portal-via-browser","text":"","title":"1. Open Azure portal via browser"},{"location":"infra-config/#2-enter-your-username-along-with-domain-and-hit-next","text":"","title":"2. Enter your username along with domain and hit \"Next\""},{"location":"infra-config/#3-provide-password-and-hit-sign-in","text":"","title":"3. Provide password and hit \"Sign In\""},{"location":"infra-config/#4-use-skip-for-now-option-for-account-protection","text":"","title":"4. Use \"Skip for now\" option for Account Protection"},{"location":"infra-config/#5-in-search-bar-look-for-virtual-machines-and-select-from-services","text":"","title":"5. In Search bar look for \"virtual machines\" and select from services"},{"location":"infra-config/#6-on-the-virtual-machines-list-scroll-to-the-right-and-note-down-public-ip-address","text":"","title":"6. On the Virtual Machines list scroll to the right and note down \"Public IP address\""},{"location":"infra-config/#nexus-dashboard-site-onboarding","text":"","title":"Nexus Dashboard site onboarding"},{"location":"infra-config/#1-login-to-nexus-dasboard-using-ip-collected-above-and-login-with-provided-credentials","text":"","title":"1. Login to Nexus Dasboard using IP collected above and login with provided credentials"},{"location":"infra-config/#2-hit-get-started-and-setup-screen-will-pop-up","text":"","title":"2. Hit \"Get started\" and Setup screen will pop-up"},{"location":"infra-config/#3-on-the-add-sites-section-hit-begin","text":"","title":"3. On the \"Add Sites\" section hit \"Begin\""},{"location":"infra-config/#4-hit-add-site-button","text":"","title":"4. Hit \"Add Site\" button"},{"location":"infra-config/#5-add-aws-cloud-network-controller","text":"Site Type: \"Cloud Network Controller\" Name: \"CNC-AWS-01\" Hostname/IP Address: \"Public IP of Cloud Network Controller for AWS collected above\" Username: admin Password: CiscoLive2023! Login Domain: empty","title":"5. Add AWS Cloud Network Controller"},{"location":"infra-config/#6-hit-save","text":"\"AWS site should be added now, stay on the same page! \"","title":"6. Hit \"Save\""},{"location":"infra-config/#7-hit-add-site-button-one-more-time-to-add-azure-site","text":"","title":"7. Hit \"Add site button one more time to add Azure site"},{"location":"infra-config/#8-add-azure-site-details","text":"Site Type: \"Cloud Network Controller\" Name: \"CNC-Azure-01\" Hostname/IP Address: \"Public IP of Cloud Network Controller from Azure collected above\" Username: admin Password: CiscoLive2023! Login Domain: empty","title":"8. Add Azure site details"},{"location":"infra-config/#6-hit-save_1","text":"","title":"6. Hit \"Save\""},{"location":"infra-config/#check-the-site-list","text":"You should see both sites added under the site list.","title":"Check the site list"},{"location":"intersight/","text":"Explore Cisco Intersight Dashboard 1. Accessing Cisco Intersight Platform Cisco Intersight Platform manages Kuberenetes clusters in the private infrasturcture. You will have access to dedicated instance of Cisco Intersight, from which you will manage your own Kuberenetes Clusters used later on to deploy application. Please find login credentials and URL to your IKS instance below: URL: https://intersight.com/ User name: holcld2611+pod'X'@gmail.com Where X will be provided by proctor User password: CiscoLive!2022 Warning You can explore Cisco Intersight through the GUI, but please do not delete content already created. Access Intersight using following steps: Open WebBrowser (Chrome) and connect to https://intersight.com Select Sign In with Cisco ID option on the first screen - as shown on figure below. Enter e-mail provided by proctor to your POD: Click next and fill password. Tip Credentials for the session can be found in WIL Assistant portal. In case of issues, ask proctor for help. 2. Intersigh Dashboard Target While you are logged to Cisco Intersight, first page you see is Monitor section. Navigate to Admin section in the bottom of Navigation Pane on Left. Click on Target Target section shows already Claimed services - in our case it is Cisco Intersight Assist installed \"on-prem\" together with vCenter. You can see status of claim, date and who executed it. Click on our Intersight Assist - iva.dcloud.cisco.com to see details of the Target. 3. Intersight Dashboard Operate Most information from claimed targets are visible under Operate section. The section is divided to two sub categories: Virtualization and Kuberenetes. Virtualization When exloring first of them, you will see information about all VMs managed by vCenter claimed in Target section. By using three dots button on right column, you are able to manage each of VM: Option when open the Menu : On the other Tabs, you can verify details of Hosts, Storage, etc. Please explore yourself rest of the tab when interested. Kuberenetes Second section contain informations about deployed Intersight Kuberenetes Clusters (IKS). You will see one IKS cluster, which is deployed on-prem. On main screen you see general information about its status - numbers of control plane nodes, workers, associated profile. Using three dots button on most right column you open menu where you are able to download kubeconfig, undeploy Cluster or Open TAC case. Explore deployed IKS cluster Please, click at name of deployed IKS cluster. Now you are inside the selected cluster, where you find more details about deployment. Dashboard shows information about cluster condition, version of the K8s installed, CNI and more. By selecting Operate Tab, you find more information about profile and policy usage. Please explore rest of the Sections to see information under them. 4. Intersight Dashboard Configure Another section in Navigation pane is \"Configure\". Under \"Configure > Profiles\" you find definition of IKS profile used for CLUS-IKS-1 K8s deployment you just explored. IKS Profile wizzard When you open existing profile, you will be able to see progress wizzard. Tip In new deployment you need to create all Pools and Polices for IKS beforehand. In the wizzard below, you will see already selected polices. It is explained in next section. Warning PLEASE, DO NOT EDIT ANY OF THE POLICES UNDER THE PROFILE - IT MAY CAUSE DESTROY OF YOUR IKS CLUSTER. Explore it only and CLOSE in the END - DON'T DEPLOY CHANGES. First page of the wizzard is asking Name of the Cluster Second page contains information about cluster in general. You find there IP Pool, LoadBalancer IP counts, DNS/NTP policy, etc. Next one, its K8s Control Plane node definition. It is possible to configure desired state of CP nodes, minumum/maximum, K8s version, IP pools. It is first place where user must specify VM infra policy which contain vCenter information (basically definition where to deploy CP nodes). VM Machine policy contain information about VM size (RAM, CPU, disk size). Fourth page its definition of worker nodes. Similar to control-plane node definition, its possible to configure size of cluster, VM polices, IP pools, K8s version. Next section contain ADD-Ons definition. Cisco Intersight give you an option to install K8s add-ons automatically during IKS cluster deployments. One of the ADD-on installed is Kuberenetes Dashboard. Final page is summary of the entire profile, where is \"DEPLOY\" button. DO NOT DEPLOY PROFILE, BUT CLOSE IT. IKS Configure Polices Important section when working with Cisco Intersight is Polices . Here user needs to configure parameters of deployments before start doing profiles. In the LAB we use seven polices as shown on the figure below. Table gives general information about Names and Policy Types, time of creation/update. By clicking on the policy name, its possible to check details and edit it.","title":"Explore Cisco Intersight Dashboard"},{"location":"intersight/#explore-cisco-intersight-dashboard","text":"","title":"Explore Cisco Intersight Dashboard"},{"location":"intersight/#1-accessing-cisco-intersight-platform","text":"Cisco Intersight Platform manages Kuberenetes clusters in the private infrasturcture. You will have access to dedicated instance of Cisco Intersight, from which you will manage your own Kuberenetes Clusters used later on to deploy application. Please find login credentials and URL to your IKS instance below: URL: https://intersight.com/ User name: holcld2611+pod'X'@gmail.com Where X will be provided by proctor User password: CiscoLive!2022 Warning You can explore Cisco Intersight through the GUI, but please do not delete content already created.","title":"1. Accessing Cisco Intersight Platform"},{"location":"intersight/#access-intersight-using-following-steps","text":"Open WebBrowser (Chrome) and connect to https://intersight.com Select Sign In with Cisco ID option on the first screen - as shown on figure below. Enter e-mail provided by proctor to your POD: Click next and fill password. Tip Credentials for the session can be found in WIL Assistant portal. In case of issues, ask proctor for help.","title":"Access Intersight using following steps:"},{"location":"intersight/#2-intersigh-dashboard-target","text":"While you are logged to Cisco Intersight, first page you see is Monitor section. Navigate to Admin section in the bottom of Navigation Pane on Left. Click on Target Target section shows already Claimed services - in our case it is Cisco Intersight Assist installed \"on-prem\" together with vCenter. You can see status of claim, date and who executed it. Click on our Intersight Assist - iva.dcloud.cisco.com to see details of the Target.","title":"2. Intersigh Dashboard Target"},{"location":"intersight/#3-intersight-dashboard-operate","text":"Most information from claimed targets are visible under Operate section. The section is divided to two sub categories: Virtualization and Kuberenetes.","title":"3. Intersight Dashboard Operate"},{"location":"intersight/#virtualization","text":"When exloring first of them, you will see information about all VMs managed by vCenter claimed in Target section. By using three dots button on right column, you are able to manage each of VM: Option when open the Menu : On the other Tabs, you can verify details of Hosts, Storage, etc. Please explore yourself rest of the tab when interested.","title":"Virtualization"},{"location":"intersight/#kuberenetes","text":"Second section contain informations about deployed Intersight Kuberenetes Clusters (IKS). You will see one IKS cluster, which is deployed on-prem. On main screen you see general information about its status - numbers of control plane nodes, workers, associated profile. Using three dots button on most right column you open menu where you are able to download kubeconfig, undeploy Cluster or Open TAC case.","title":"Kuberenetes"},{"location":"intersight/#explore-deployed-iks-cluster","text":"Please, click at name of deployed IKS cluster. Now you are inside the selected cluster, where you find more details about deployment. Dashboard shows information about cluster condition, version of the K8s installed, CNI and more. By selecting Operate Tab, you find more information about profile and policy usage. Please explore rest of the Sections to see information under them.","title":"Explore deployed IKS cluster"},{"location":"intersight/#4-intersight-dashboard-configure","text":"Another section in Navigation pane is \"Configure\". Under \"Configure > Profiles\" you find definition of IKS profile used for CLUS-IKS-1 K8s deployment you just explored.","title":"4. Intersight Dashboard Configure"},{"location":"intersight/#iks-profile-wizzard","text":"When you open existing profile, you will be able to see progress wizzard. Tip In new deployment you need to create all Pools and Polices for IKS beforehand. In the wizzard below, you will see already selected polices. It is explained in next section. Warning PLEASE, DO NOT EDIT ANY OF THE POLICES UNDER THE PROFILE - IT MAY CAUSE DESTROY OF YOUR IKS CLUSTER. Explore it only and CLOSE in the END - DON'T DEPLOY CHANGES. First page of the wizzard is asking Name of the Cluster Second page contains information about cluster in general. You find there IP Pool, LoadBalancer IP counts, DNS/NTP policy, etc. Next one, its K8s Control Plane node definition. It is possible to configure desired state of CP nodes, minumum/maximum, K8s version, IP pools. It is first place where user must specify VM infra policy which contain vCenter information (basically definition where to deploy CP nodes). VM Machine policy contain information about VM size (RAM, CPU, disk size). Fourth page its definition of worker nodes. Similar to control-plane node definition, its possible to configure size of cluster, VM polices, IP pools, K8s version. Next section contain ADD-Ons definition. Cisco Intersight give you an option to install K8s add-ons automatically during IKS cluster deployments. One of the ADD-on installed is Kuberenetes Dashboard. Final page is summary of the entire profile, where is \"DEPLOY\" button. DO NOT DEPLOY PROFILE, BUT CLOSE IT.","title":"IKS Profile wizzard"},{"location":"intersight/#iks-configure-polices","text":"Important section when working with Cisco Intersight is Polices . Here user needs to configure parameters of deployments before start doing profiles. In the LAB we use seven polices as shown on the figure below. Table gives general information about Names and Policy Types, time of creation/update. By clicking on the policy name, its possible to check details and edit it.","title":"IKS Configure Polices"},{"location":"lab-check/","text":"","title":"Lab check"},{"location":"ndo-tenant/","text":"Nexus Dasboard Orchestrator Tenant configuration In this section we will configure our first Tenant, streteched between Azure and AWS. As Cloud Network Controller will be used for configuration, we need to make sure that CNC has privillages to do so. Will learn how to do it. Tenant Creation on NDO On the Left navigation page click \"Application Management\" -> \"Tenant\" and then \"Add Tenant\" Fill in Tenant details for name and description Display Name: Tenant-01 Descrption: Cisco Live 2023 AMS Tenant Associate Tenant to both Site by checking the checkbox next to it. Note For now you are not able to Save this configuration with red marking on both Sites. Click the Pencil button at the end of each site line to finish configuration. Additional setting are needed for CNC, so it knows which subscribtion to use on Azure and which Tenant ID on AWS. CNC-Azure-01 site configuration For Azure site, we will be using the same Subscription as the one used for CNC deployment - select Mode as \"Select Shared\" and use existing subscription from drop-down. Leave security domains empty. Hit Save . CNC-AWS-01 site configuration For AWS site we have 2 options - Untrusted with Cloud Access key and Secret or Trusted . In our case we would be using Trusted configuration. Each Tenant create on NDO with AWS Site association required sepearete Account ID on AWS site. You can find your in POD Details. Fill in with your POD AWS User-Account ID and select Access Type as Trusted , hit Save . Now configuration can be saved, leave assocaited user list empty as there are no additional users and hit Save . On the Tenant list you should see Tenant-01 created an assigned to 2 Sites.","title":"NDO Tenant configuration"},{"location":"ndo-tenant/#nexus-dasboard-orchestrator-tenant-configuration","text":"In this section we will configure our first Tenant, streteched between Azure and AWS. As Cloud Network Controller will be used for configuration, we need to make sure that CNC has privillages to do so. Will learn how to do it.","title":"Nexus Dasboard Orchestrator Tenant configuration"},{"location":"ndo-tenant/#tenant-creation-on-ndo","text":"On the Left navigation page click \"Application Management\" -> \"Tenant\" and then \"Add Tenant\" Fill in Tenant details for name and description Display Name: Tenant-01 Descrption: Cisco Live 2023 AMS Tenant Associate Tenant to both Site by checking the checkbox next to it. Note For now you are not able to Save this configuration with red marking on both Sites. Click the Pencil button at the end of each site line to finish configuration. Additional setting are needed for CNC, so it knows which subscribtion to use on Azure and which Tenant ID on AWS. CNC-Azure-01 site configuration For Azure site, we will be using the same Subscription as the one used for CNC deployment - select Mode as \"Select Shared\" and use existing subscription from drop-down. Leave security domains empty. Hit Save . CNC-AWS-01 site configuration For AWS site we have 2 options - Untrusted with Cloud Access key and Secret or Trusted . In our case we would be using Trusted configuration. Each Tenant create on NDO with AWS Site association required sepearete Account ID on AWS site. You can find your in POD Details. Fill in with your POD AWS User-Account ID and select Access Type as Trusted , hit Save . Now configuration can be saved, leave assocaited user list empty as there are no additional users and hit Save . On the Tenant list you should see Tenant-01 created an assigned to 2 Sites.","title":"Tenant Creation on NDO"},{"location":"object-map/","text":"ACI to Public Cloud object mapping Cloud Netowrk Controller is using cloud native objects to map ACI constructins into various of public clouds. ACI to AWS object mapping Below you can find how ACI policy model is mapped to AWS cloud native objects ACI to AWS object mapping Below you can find how ACI policy model is mapped to AWS cloud native objects","title":"ACI to Public Cloud Object mapping"},{"location":"object-map/#aci-to-public-cloud-object-mapping","text":"Cloud Netowrk Controller is using cloud native objects to map ACI constructins into various of public clouds.","title":"ACI to Public Cloud object mapping"},{"location":"object-map/#aci-to-aws-object-mapping","text":"Below you can find how ACI policy model is mapped to AWS cloud native objects","title":"ACI to AWS object mapping"},{"location":"object-map/#aci-to-aws-object-mapping_1","text":"Below you can find how ACI policy model is mapped to AWS cloud native objects","title":"ACI to AWS object mapping"},{"location":"resources/","text":"Resources from the LAB In this section you can find necessary files used during the Lab to download. Cisco Cloud Network Controller Solution Cisco Cloud Network Controller Main Page Cisco Cloud Network Controller Solution Overview White Papers Cisco Cloud ACI on AWS White Paper Cisco Cloud ACI on Microsoft Azure White Paper Case Studies Cisco ACI Case Studies Installation Guides AWS Installation Guide Azure Installation Guide Google Cloud Installation Guide","title":"Resources"},{"location":"resources/#resources-from-the-lab","text":"In this section you can find necessary files used during the Lab to download.","title":"Resources from the LAB"},{"location":"resources/#cisco-cloud-network-controller-solution","text":"Cisco Cloud Network Controller Main Page Cisco Cloud Network Controller Solution Overview","title":"Cisco Cloud Network Controller Solution"},{"location":"resources/#white-papers","text":"Cisco Cloud ACI on AWS White Paper Cisco Cloud ACI on Microsoft Azure White Paper","title":"White Papers"},{"location":"resources/#case-studies","text":"Cisco ACI Case Studies","title":"Case Studies"},{"location":"resources/#installation-guides","text":"AWS Installation Guide Azure Installation Guide Google Cloud Installation Guide","title":"Installation Guides"},{"location":"restAPI/","text":"Cisco ACI rest API In this lab section you will create and execute list of API calls to configure ACI managed objects as well as read data from existing polices and devices. After this section you should feel comfortable with running simple automation tasks and build your own tasks for further customizations. You will be familiar with Postman dashboard and general idea of running request individually or as a collection defined. 1 Define restAPI calls under Collection Now is a time to work with our requests to GET or POST restAPI. You will define couple of them during the LAB. Figure below shows where to navigate to create new request in your collection. Every API command have predefined structure. Administrator needs to specify URI for correct object are you going to work with. Everything is described in details under the following link: Cisco ACI API Configuration guide . The figure below shows structure of every API call: 1.1 Create ACI login request First request when working with ACI must be authentication. Without having your session authenticated, no API calls can be successfully executed. Information about Authentication request structure can be found here: Cisco ACI Authentication API . Define it in Postman: In New Request section specify: 1) Name of Request ACI Login 2) Select Method to be POST 3) Enter request URL https://{{apic}}/api/aaaLogin.json Note Please notice that it's first place you use your Environment Variable = apic. In JSON definition every variable is closed by {{ }} . 4) Move to Body section for further work with your request Once you open Body section, you need to select type of data to be raw and coding to JSON . Now you can copy code defined below to body section of your request. aaaLogin { \"aaaUser\" : { \"attributes\" : { \"name\" : \"{{user}}\" , \"pwd\" : \"{{password}}\" } } } Yellow highlighted text in the json code above will use variables you defined in Environment at the beginning of the Lab. Now your Request should looks like on the figure below. It's time to TEST it! . Click on Send button. Make sure your environment ACI-dcloud is selected - above the Send button. When works as expected you will be authenticated to APIC. Responce from the APIC is visible in the section at your screen. Let's go together across results you see on the screen. First is a Status 200 OK - means APIC server accepted your request, execute it and responce with data. In the REPLY Body you see JSON structure data. You can see token which is your authentication token. RefreshTimeoutSeconds set to 600 seconds = 10 minutes. Token is valid for 10 minutes and after that period will expire. Please scroll down across the response body yourself. Look for such information: userName sessionId aaaWriteRoles Write them down to notepad. Register your ACI Fabric Switches in APIC Lab is clear and leaf/spines requires registration. You can automate it using Postman. Configure new Postman POST Request with code downloaded from here use URI: https://{{apic}}/api/node/class/fabricNode.json Contact Instructor in case of issues. 1.2 Get Information About a Node POST is not only one request type. You can configure your ACI fabric, but you can use restAPI to pull data you are interest to analyse. This excercise is about writing a GET Request under already created ACI collection. As an example you will pull information about Leaf node-101 system in json . Name your new Request Node-101-topsystem check Copy the URI to GET request: https://{{apic}}/api/mo/topology/pod-1/node-101/sys.json Save new Request and Click Send . Note It may happen that your API Token expired - remember its valid only 10 minutes. If you experience it, go to ACI Login Request and Send it again to re-authenticate. In the Body responce you can verify topsystem information about Leaf in your Fabric. You can pull data about AccessPolices, Interfaces as well as Logical construct - Tenant, EPGs, VRFs etc. The idea is always same - specify correct URL. Another test will be quering information about APIC node and current firmware version running on it. Please Create another Request under your Collection with following setup: Name of Request: Get Running Firmware GET request URI: https://{{apic}}/api/mo/topology/pod-1/node-1/sys/ctrlrfwstatuscont.json?query-target=subtree&target-subtree-class=firmwareCtrlrRunning Please observe that current query is composed with parameters. Before we pulled data directly from top-system of our ACI Leaf. Now you are narrowing the output to only sub-class you are intrested about. Try to delete part of URI ?query-target=subtree&target-subtree-class=firmwareCtrlrRunning and do Send again. 2 ACI Access Polices Every ACI Fabric configuration starts with Access Polices. You need to define ACI objects which represent single physical port configurations. To remind relationships between objects, please check following figure. Usually Interface Polices are configured once and re-used like LLDP, LACP, CDP, etc. However our LAB is empty and you need to start from zero. Following sections will help you to prepare your API requests. 2.1 Interface Policies You will configure LACP Policy, LLDP_ON, LINK-10G. Copy each of the Policy json definition to individual POST requests under your ACI Collection in Postman. Use URI to POST: https://{{apic}}/api/node/mo/uni.json LACP_ACTIVE { \"lacpLagPol\" : { \"attributes\" : { \"dn\" : \"uni/infra/lacplagp-LACP_ACTIVE\" , \"ctrl\" : \"fast-sel-hot-stdby,graceful-conv,susp-individual\" , \"name\" : \"LACP_ACTIVE\" , \"mode\" : \"active\" , \"rn\" : \"lacplagp-LACP_ACTIVE\" , \"status\" : \"created\" }, \"children\" : [] } } LLDP_ON { \"lldpIfPol\" : { \"attributes\" : { \"dn\" : \"uni/infra/lldpIfP-LLDP_ON\" , \"name\" : \"LLDP_ON\" , \"rn\" : \"lldpIfP-LLDP_ON\" , \"status\" : \"created\" }, \"children\" : [] } } LINK-10G { \"fabricHIfPol\" : { \"attributes\" : { \"dn\" : \"uni/infra/hintfpol-LINK-10G\" , \"name\" : \"LINK-10G\" , \"speed\" : \"10G\" , \"rn\" : \"hintfpol-LINK-10G\" , \"status\" : \"created\" }, \"children\" : [] } } 2.2 VLANs, Domains and AAEPs Create VLAN pool with vlan-range, associate it with Physical Domain and AAEP. Now you will change an approach. Your task is to create VLAN Pool with APIC GUI and capture json code from API Inspector . Connect to your APIC using bookmark in dCloud workstation Chrome. Navigate to Access Polices -> Pools -> VLAN. When you are there, Click on Help and Tools menu in APIC Dashboard and Click on Show API Inspector . In new window you will open API Inspector, where you can capture every API call send to the APIC server. Now you can start ADD VLAN-pool in APIC GUI. VLAN POOL NAME: vlan-pool Allocation Mode: Static Encap Blocks : 100-200, Inherit Allocation mode from parent, Role: External or On the wire encapsulations SUBMIT and open API Inspector in the another Chrome Window. Search for POST method. COPY now ALL POST URI from API Inspector to Postman request Body. You should capture data as shown in code below. h tt ps : //apic1.dcloud.cisco.com/api/node/mo/uni/infra/vlanns-[vlan-pool]-static.json payload { \"fvnsVlanInstP\" : { \"attributes\" : { \"dn\" : \"uni/infra/vlanns-[vlan-pool]-static\" , \"name\" : \"vlan-pool\" , \"allocMode\" : \"static\" , \"rn\" : \"vlanns-[vlan-pool]-static\" , \"status\" : \"created\" }, \"children\" : [ { \"fvnsEncapBlk\" : { \"attributes\" : { \"dn\" : \"uni/infra/vlanns-[vlan-pool]-static/from-[vlan-100]-to-[vlan-200]\" , \"from\" : \"vlan-100\" , \"to\" : \"vlan-200\" , \"rn\" : \"from-[vlan-100]-to-[vlan-200]\" , \"status\" : \"created\" }, \"children\" : [] } } ] } } Now it's time to build a request from data captured. You will use that request in the future for adding more vlan pools. First, copy URL and place it in Postman URL. Replace apic1.dcloud.cisco.com with our variable {{apic}}. Next replace vlanpool name in [ ] brakets. New URI looks like that: https://{{apic}}/api/node/mo/uni/infra/vlanns-[{{vlanpoolname}}]-static.json VLAN POOL { \"fvnsVlanInstP\" : { \"attributes\" : { \"dn\" : \"uni/infra/vlanns-[{{vlanpoolname}}]-static\" , \"name\" : \"{{vlanpoolname}}\" , \"allocMode\" : \"static\" , \"rn\" : \"vlanns-[{{vlanpoolname}}]-static\" , \"status\" : \"created\" }, \"children\" : [ { \"fvnsEncapBlk\" : { \"attributes\" : { \"dn\" : \"uni/infra/vlanns-[{{vlanpoolname}}]-static/from-[vlan-{{vlanstart}}]-to-[vlan-{{vlanend}}]\" , \"from\" : \"vlan-{{vlanstart}}\" , \"to\" : \"vlan-{{vlanend}}\" , \"rn\" : \"from-[vlan-{{vlanstart}}]-to-[vlan-{{vlanend}]\" , \"status\" : \"created\" }, \"children\" : [] } } ] } } Variables {{vlanpoolname}}, {{vlanstart}}, {{vlanend}} you can replace by hardcoded values in your code, or define values in Environment. Later today you will see the powerfull of variables using csv file for input data. Same approach please use in Domain and AAEP creation - Go to GUI, do Domain - PHY-DOM, associate with vlan-pool and AAEP - dcloud-AAEP. Observe API Inspector, capture the code and build those two requests in Postman. URI for AAEP Function: https://{{apic}}/api/node/mo/uni/infra.json AAEP { \"infraInfra\" : { \"attributes\" : { \"dn\" : \"uni/infra\" , \"status\" : \"modified\" }, \"children\" : [ { \"infraAttEntityP\" : { \"attributes\" : { \"dn\" : \"uni/infra/attentp-{{aaep}}\" , \"name\" : \"{{aaep}}\" , \"rn\" : \"attentp-{{aaep}}\" , \"status\" : \"created\" }, \"children\" : [] } }, { \"infraFuncP\" : { \"attributes\" : { \"dn\" : \"uni/infra/funcprof\" , \"status\" : \"modified\" }, \"children\" : [] } } ] } } Last one is Domain. Create Physical domain PHY-DOM with vlan pool associated vlan-pool and AAEP dcloud-AAEP . https://{{apic}}/api/node/mo/uni/phys-{{domain}}.json Domain { \"physDomP\" : { \"attributes\" : { \"dn\" : \"uni/phys-{{domain}}\" , \"name\" : \"{{domain}}\" , \"rn\" : \"phys-{{domain}}\" , \"status\" : \"created\" }, \"children\" : [ { \"infraRsVlanNs\" : { \"attributes\" : { \"tDn\" : \"uni/infra/vlanns-[{{vlanpoolname}}]-static\" , \"status\" : \"created\" }, \"children\" : [] } } ] } } Interesting fact about domain to AAEP association. Domain is sub-object to AAEP, you will not see AAEP under Domain definition, even you specify it in APIC GUI. In API you need to add it under AAEP. https://{{apic}}/api/node/mo/uni/infra.json Domain to AAEP association { \"infraAttEntityP\" : { \"attributes\" : { \"annotation\" : \"\" , \"descr\" : \"\" , \"dn\" : \"uni/infra/attentp-{{aaep}}\" , \"name\" : \"{{aaep}}\" , \"nameAlias\" : \"\" , \"ownerKey\" : \"\" , \"ownerTag\" : \"\" , \"userdom\" : \":all:\" }, \"children\" : [ { \"infraRsDomP\" : { \"attributes\" : { \"annotation\" : \"\" , \"tDn\" : \"uni/phys-{{domain}}\" , \"userdom\" : \":all:\" } } } ] } } 2.3 Interface Policy Group Lets now create one of the Interface policy group and use all created policy. Interface Policy Group VPC { \"infraAccBndlGrp\" : { \"attributes\" : { \"dn\" : \"uni/infra/funcprof/accbundle-intpolgrp-vpc-server1\" , \"lagT\" : \"node\" , \"name\" : \"intpolgrp-vpc-server1\" , \"rn\" : \"accbundle-intpolgrp-vpc-server1\" , \"status\" : \"created\" }, \"children\" : [ { \"infraRsAttEntP\" : { \"attributes\" : { \"tDn\" : \"uni/infra/attentp-dcloud-AAEP\" , \"status\" : \"created,modified\" }, \"children\" : [] } }, { \"infraRsHIfPol\" : { \"attributes\" : { \"tnFabricHIfPolName\" : \"LINK-10G\" , \"status\" : \"created,modified\" }, \"children\" : [] } }, { \"infraRsLldpIfPol\" : { \"attributes\" : { \"tnLldpIfPolName\" : \"LLDP_ON\" , \"status\" : \"created,modified\" }, \"children\" : [] } }, { \"infraRsLacpPol\" : { \"attributes\" : { \"tnLacpLagPolName\" : \"LACP_ACTIVE\" , \"status\" : \"created,modified\" }, \"children\" : [] } } ] } } Replace intpolgrp-vpc-server1 with intpolgrp-vpc-server2 to observe how simple you can add new VPC policy group using Postman. Warning Download three files to your dcloud workstation and using post in Postman or in APIC GUI upload it to access-polices. Without them, your VPC won't instanciate and cannot be used in later stage of the lab. JSON VPC Policy . JSON interface leaf profile . JSON switch leaf profile . Ask instructor for assistance if not clear. 3 ACI Tenant Upon now you created ACI AccessPolicies for your Tenant. Having JSON code for every object you will be able to unified variables across Postman Collection and then run all together using Variables from Environment or much better from CSV file. Now is time to create your tenant using JSON. It will be simple Tenant definition with one VRF and two bridge domains associated with two EPGs. Every EPG will be associated with created domain and statically binded to both VPC created, with VLAN from VLAN pool done by you perviously. Figure below shows our Logical Tenant. You need to define Tenant, VRF, Bridge-Domains, Application-Profile, EPGs with Domain association and static binding under EPG. Quite many of Requests to do in Postman. You can define all of them under one Request. Please, download a JSON file from JSON Tenant Ready . Once you have it on dcloud workstation, please create new Request under ACI Collection: NAME: CREATE TENANT-1 Method: POST URL: https://{{apic}}/api/node/mo/uni.json COPY now content from downloaded file to Body of the new request, Save and Send it. Note Please check your authentication token if still valid. Tenant-1 will be created with all structure shown in figure above. 3.1 Tenant components This section contain JSON codes necessary to create Tenant objects. 3.1.1 Tenant and VRF Code below can be used to create new Tenant and VRF. https://{{apic}}/api/node/mo/uni.json Tenant and VRF from CSV { \"fvTenant\" : { \"attributes\" : { \"annotation\" : \"\" , \"descr\" : \"\" , \"dn\" : \"uni/tn-{{tenantname}}\" , \"name\" : \"{{tenantname}}\" }, \"children\" : [ { \"fvCtx\" : { \"attributes\" : { \"name\" : \"{{vrfname}}\" }, \"children\" : [] } } ] } } 3.1.2 BD in existing tenant Code below can be used to create new Bridge-Domain in existing Tenant. https://{{apic}}/api/node/mo/uni.json Bridge-domain L3 from CSV { \"fvBD\" : { \"attributes\" : { \"name\" : \"{{bdname}}\" , \"dn\" : \"uni/tn-{{tenantname}}/BD-{{bdname}}\" , \"rn\" : \"BD-{{bdname}}\" , \"type\" : \"regular\" }, \"children\" : [ { \"fvSubnet\" : { \"attributes\" : { \"ip\" : \"{{ipaddress}}\" , \"ipDPLearning\" : \"enabled\" , \"scope\" : \"private\" } } }, { \"fvRsCtx\" : { \"attributes\" : { \"tnFvCtxName\" : \"{{vrfname}}\" } } } ] } } In case you need to add BD without IP address and unicast routing disabled, use code below. https://{{apic}}/api/node/mo/uni.json Bridge-domain L2 from CSV { \"fvTenant\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}\" , \"status\" : \"modified\" }, \"children\" : [ { \"fvBD\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}/BD-{{bdname}}\" , \"mac\" : \"00:22:BD:F8:19:FF\" , \"name\" : \"{{bdname}}\" , \"rn\" : \"BD-{{bdname}}\" , \"arpFlood\" : \"yes\" , \"ipLearning\" : \"no\" , \"unicastRoute\" : \"no\" , \"unkMacUcastAct\" : \"flood\" , \"type\" : \"regular\" , \"unkMcastAct\" : \"flood\" , \"status\" : \"created\" }, \"children\" : [ { \"fvRsCtx\" : { \"attributes\" : { \"tnFvCtxName\" : \"{{vrfname}}\" , \"status\" : \"created,modified\" }, \"children\" : [] } } ] } } ] } } Highlighted rows are specific for L2 Bridge-Domain. 3.1.3 Application Profile Component which contain all EPGs in the Tenant. https://{{apic}}/api/node/mo/uni.json Application Profile from CSV { \"fvAp\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}/ap-{{ap}}\" , \"name\" : \"{{ap}}\" , \"rn\" : \"ap-{{ap}}\" , \"status\" : \"created\" } } } 3.1.4 EPGs in existing tenant/appprofiles and associated with domain https://{{apic}}/api/node/mo/uni.json Create EPG under existing application profile from CSV { \"fvAEPg\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}/ap-{{ap}}/epg-{{epgname}}\" , \"name\" : \"{{epgname}}\" , \"rn\" : \"epg-{{epgname}}\" , \"status\" : \"created\" }, \"children\" : [ { \"fvRsBd\" : { \"attributes\" : { \"tnFvBDname\" : \"{{bdname}}\" , \"status\" : \"created,modified\" }, \"children\" : [] } }, { \"fvRsDomAtt\" : { \"attributes\" : { \"tDn\" : \"uni/phys-{{domain}}\" , \"status\" : \"created\" }, \"children\" : [] } } ] } } 4 Use CSV file for input data You are about to re-use work you did during all excercises. Untill now you run every request for static data, only once. Adding 100 Bridge-Domains, by changing \"Variables\" within the code would be problematic. Postman is giving an option to use external variable file to execute created requests in the collection. By running five requests from your Collection and using CSV file from location FILE CSV TO DOWNLOAD you will add new Tenant with one VRF, Applicatioin Profile, 22 Bridge-Domains and 22 EPGs associated to Physical Domain created before. All of this will be done in one Postman collection Run. 4.1 Run Collection requests Follow the instruction from the figure below: 1) Click on dots icon at your ACI dCloud collection folder 2) Select Run Collection You will be moved to new dashboard, where you will select Requests and input file. Deselect All, and then Select only needed requests: When you select Requests, go to Data \"Select File\" and choice downloaded CSV file - tenant-create.csv . Confirm that your file is selected as on the figure above. You can confirm amount of iterations in the top field Iterations . Note Do not forget to authenticate your Postman session using ACI Login request. I recommend to do it out of collection Run -- beforehand. Click Run ACI dCloud Blue button. 4.2 Verification after runing the collection When you successfully execute your collection, in Summary at the screen you will see results of every Iteration. When Request was accepted by APIC, result will be 200 OK . Every Iteration contain all five seleceted requests. You probably noticed that one of them is getting 400 Bad Request - \"Bridge Domain L3 from CSV\". Reason for that, is missing children attribute fvSubnet for all L2 bridge-domain in our CSV file. Now connect to APIC GUI and verify if tenant exists, check dashboard status. You successfully Created New Tenant with 22 EPGs and 22 BDs. Please, edit CSV file in notepad++ editor at your dCloud workstation, replace Tenantname with new and run your Postman Collection once again. Do you understand now power of simple automation?","title":"Cisco ACI rest API"},{"location":"restAPI/#cisco-aci-rest-api","text":"In this lab section you will create and execute list of API calls to configure ACI managed objects as well as read data from existing polices and devices. After this section you should feel comfortable with running simple automation tasks and build your own tasks for further customizations. You will be familiar with Postman dashboard and general idea of running request individually or as a collection defined.","title":"Cisco ACI rest API"},{"location":"restAPI/#1-define-restapi-calls-under-collection","text":"Now is a time to work with our requests to GET or POST restAPI. You will define couple of them during the LAB. Figure below shows where to navigate to create new request in your collection. Every API command have predefined structure. Administrator needs to specify URI for correct object are you going to work with. Everything is described in details under the following link: Cisco ACI API Configuration guide . The figure below shows structure of every API call:","title":"1 Define restAPI calls under Collection"},{"location":"restAPI/#11-create-aci-login-request","text":"First request when working with ACI must be authentication. Without having your session authenticated, no API calls can be successfully executed. Information about Authentication request structure can be found here: Cisco ACI Authentication API . Define it in Postman: In New Request section specify: 1) Name of Request ACI Login 2) Select Method to be POST 3) Enter request URL https://{{apic}}/api/aaaLogin.json Note Please notice that it's first place you use your Environment Variable = apic. In JSON definition every variable is closed by {{ }} . 4) Move to Body section for further work with your request Once you open Body section, you need to select type of data to be raw and coding to JSON . Now you can copy code defined below to body section of your request. aaaLogin { \"aaaUser\" : { \"attributes\" : { \"name\" : \"{{user}}\" , \"pwd\" : \"{{password}}\" } } } Yellow highlighted text in the json code above will use variables you defined in Environment at the beginning of the Lab. Now your Request should looks like on the figure below. It's time to TEST it! . Click on Send button. Make sure your environment ACI-dcloud is selected - above the Send button. When works as expected you will be authenticated to APIC. Responce from the APIC is visible in the section at your screen. Let's go together across results you see on the screen. First is a Status 200 OK - means APIC server accepted your request, execute it and responce with data. In the REPLY Body you see JSON structure data. You can see token which is your authentication token. RefreshTimeoutSeconds set to 600 seconds = 10 minutes. Token is valid for 10 minutes and after that period will expire. Please scroll down across the response body yourself. Look for such information: userName sessionId aaaWriteRoles Write them down to notepad.","title":"1.1 Create ACI login request"},{"location":"restAPI/#register-your-aci-fabric-switches-in-apic","text":"Lab is clear and leaf/spines requires registration. You can automate it using Postman. Configure new Postman POST Request with code downloaded from here use URI: https://{{apic}}/api/node/class/fabricNode.json Contact Instructor in case of issues.","title":"Register your ACI Fabric Switches in APIC"},{"location":"restAPI/#12-get-information-about-a-node","text":"POST is not only one request type. You can configure your ACI fabric, but you can use restAPI to pull data you are interest to analyse. This excercise is about writing a GET Request under already created ACI collection. As an example you will pull information about Leaf node-101 system in json . Name your new Request Node-101-topsystem check Copy the URI to GET request: https://{{apic}}/api/mo/topology/pod-1/node-101/sys.json Save new Request and Click Send . Note It may happen that your API Token expired - remember its valid only 10 minutes. If you experience it, go to ACI Login Request and Send it again to re-authenticate. In the Body responce you can verify topsystem information about Leaf in your Fabric. You can pull data about AccessPolices, Interfaces as well as Logical construct - Tenant, EPGs, VRFs etc. The idea is always same - specify correct URL. Another test will be quering information about APIC node and current firmware version running on it. Please Create another Request under your Collection with following setup: Name of Request: Get Running Firmware GET request URI: https://{{apic}}/api/mo/topology/pod-1/node-1/sys/ctrlrfwstatuscont.json?query-target=subtree&target-subtree-class=firmwareCtrlrRunning Please observe that current query is composed with parameters. Before we pulled data directly from top-system of our ACI Leaf. Now you are narrowing the output to only sub-class you are intrested about. Try to delete part of URI ?query-target=subtree&target-subtree-class=firmwareCtrlrRunning and do Send again.","title":"1.2 Get Information About a Node"},{"location":"restAPI/#2-aci-access-polices","text":"Every ACI Fabric configuration starts with Access Polices. You need to define ACI objects which represent single physical port configurations. To remind relationships between objects, please check following figure. Usually Interface Polices are configured once and re-used like LLDP, LACP, CDP, etc. However our LAB is empty and you need to start from zero. Following sections will help you to prepare your API requests.","title":"2  ACI Access Polices"},{"location":"restAPI/#21-interface-policies","text":"You will configure LACP Policy, LLDP_ON, LINK-10G. Copy each of the Policy json definition to individual POST requests under your ACI Collection in Postman. Use URI to POST: https://{{apic}}/api/node/mo/uni.json LACP_ACTIVE { \"lacpLagPol\" : { \"attributes\" : { \"dn\" : \"uni/infra/lacplagp-LACP_ACTIVE\" , \"ctrl\" : \"fast-sel-hot-stdby,graceful-conv,susp-individual\" , \"name\" : \"LACP_ACTIVE\" , \"mode\" : \"active\" , \"rn\" : \"lacplagp-LACP_ACTIVE\" , \"status\" : \"created\" }, \"children\" : [] } } LLDP_ON { \"lldpIfPol\" : { \"attributes\" : { \"dn\" : \"uni/infra/lldpIfP-LLDP_ON\" , \"name\" : \"LLDP_ON\" , \"rn\" : \"lldpIfP-LLDP_ON\" , \"status\" : \"created\" }, \"children\" : [] } } LINK-10G { \"fabricHIfPol\" : { \"attributes\" : { \"dn\" : \"uni/infra/hintfpol-LINK-10G\" , \"name\" : \"LINK-10G\" , \"speed\" : \"10G\" , \"rn\" : \"hintfpol-LINK-10G\" , \"status\" : \"created\" }, \"children\" : [] } }","title":"2.1 Interface Policies"},{"location":"restAPI/#22-vlans-domains-and-aaeps","text":"Create VLAN pool with vlan-range, associate it with Physical Domain and AAEP. Now you will change an approach. Your task is to create VLAN Pool with APIC GUI and capture json code from API Inspector . Connect to your APIC using bookmark in dCloud workstation Chrome. Navigate to Access Polices -> Pools -> VLAN. When you are there, Click on Help and Tools menu in APIC Dashboard and Click on Show API Inspector . In new window you will open API Inspector, where you can capture every API call send to the APIC server. Now you can start ADD VLAN-pool in APIC GUI. VLAN POOL NAME: vlan-pool Allocation Mode: Static Encap Blocks : 100-200, Inherit Allocation mode from parent, Role: External or On the wire encapsulations SUBMIT and open API Inspector in the another Chrome Window. Search for POST method. COPY now ALL POST URI from API Inspector to Postman request Body. You should capture data as shown in code below. h tt ps : //apic1.dcloud.cisco.com/api/node/mo/uni/infra/vlanns-[vlan-pool]-static.json payload { \"fvnsVlanInstP\" : { \"attributes\" : { \"dn\" : \"uni/infra/vlanns-[vlan-pool]-static\" , \"name\" : \"vlan-pool\" , \"allocMode\" : \"static\" , \"rn\" : \"vlanns-[vlan-pool]-static\" , \"status\" : \"created\" }, \"children\" : [ { \"fvnsEncapBlk\" : { \"attributes\" : { \"dn\" : \"uni/infra/vlanns-[vlan-pool]-static/from-[vlan-100]-to-[vlan-200]\" , \"from\" : \"vlan-100\" , \"to\" : \"vlan-200\" , \"rn\" : \"from-[vlan-100]-to-[vlan-200]\" , \"status\" : \"created\" }, \"children\" : [] } } ] } } Now it's time to build a request from data captured. You will use that request in the future for adding more vlan pools. First, copy URL and place it in Postman URL. Replace apic1.dcloud.cisco.com with our variable {{apic}}. Next replace vlanpool name in [ ] brakets. New URI looks like that: https://{{apic}}/api/node/mo/uni/infra/vlanns-[{{vlanpoolname}}]-static.json VLAN POOL { \"fvnsVlanInstP\" : { \"attributes\" : { \"dn\" : \"uni/infra/vlanns-[{{vlanpoolname}}]-static\" , \"name\" : \"{{vlanpoolname}}\" , \"allocMode\" : \"static\" , \"rn\" : \"vlanns-[{{vlanpoolname}}]-static\" , \"status\" : \"created\" }, \"children\" : [ { \"fvnsEncapBlk\" : { \"attributes\" : { \"dn\" : \"uni/infra/vlanns-[{{vlanpoolname}}]-static/from-[vlan-{{vlanstart}}]-to-[vlan-{{vlanend}}]\" , \"from\" : \"vlan-{{vlanstart}}\" , \"to\" : \"vlan-{{vlanend}}\" , \"rn\" : \"from-[vlan-{{vlanstart}}]-to-[vlan-{{vlanend}]\" , \"status\" : \"created\" }, \"children\" : [] } } ] } } Variables {{vlanpoolname}}, {{vlanstart}}, {{vlanend}} you can replace by hardcoded values in your code, or define values in Environment. Later today you will see the powerfull of variables using csv file for input data. Same approach please use in Domain and AAEP creation - Go to GUI, do Domain - PHY-DOM, associate with vlan-pool and AAEP - dcloud-AAEP. Observe API Inspector, capture the code and build those two requests in Postman. URI for AAEP Function: https://{{apic}}/api/node/mo/uni/infra.json AAEP { \"infraInfra\" : { \"attributes\" : { \"dn\" : \"uni/infra\" , \"status\" : \"modified\" }, \"children\" : [ { \"infraAttEntityP\" : { \"attributes\" : { \"dn\" : \"uni/infra/attentp-{{aaep}}\" , \"name\" : \"{{aaep}}\" , \"rn\" : \"attentp-{{aaep}}\" , \"status\" : \"created\" }, \"children\" : [] } }, { \"infraFuncP\" : { \"attributes\" : { \"dn\" : \"uni/infra/funcprof\" , \"status\" : \"modified\" }, \"children\" : [] } } ] } } Last one is Domain. Create Physical domain PHY-DOM with vlan pool associated vlan-pool and AAEP dcloud-AAEP . https://{{apic}}/api/node/mo/uni/phys-{{domain}}.json Domain { \"physDomP\" : { \"attributes\" : { \"dn\" : \"uni/phys-{{domain}}\" , \"name\" : \"{{domain}}\" , \"rn\" : \"phys-{{domain}}\" , \"status\" : \"created\" }, \"children\" : [ { \"infraRsVlanNs\" : { \"attributes\" : { \"tDn\" : \"uni/infra/vlanns-[{{vlanpoolname}}]-static\" , \"status\" : \"created\" }, \"children\" : [] } } ] } } Interesting fact about domain to AAEP association. Domain is sub-object to AAEP, you will not see AAEP under Domain definition, even you specify it in APIC GUI. In API you need to add it under AAEP. https://{{apic}}/api/node/mo/uni/infra.json Domain to AAEP association { \"infraAttEntityP\" : { \"attributes\" : { \"annotation\" : \"\" , \"descr\" : \"\" , \"dn\" : \"uni/infra/attentp-{{aaep}}\" , \"name\" : \"{{aaep}}\" , \"nameAlias\" : \"\" , \"ownerKey\" : \"\" , \"ownerTag\" : \"\" , \"userdom\" : \":all:\" }, \"children\" : [ { \"infraRsDomP\" : { \"attributes\" : { \"annotation\" : \"\" , \"tDn\" : \"uni/phys-{{domain}}\" , \"userdom\" : \":all:\" } } } ] } }","title":"2.2 VLANs, Domains and AAEPs"},{"location":"restAPI/#23-interface-policy-group","text":"Lets now create one of the Interface policy group and use all created policy. Interface Policy Group VPC { \"infraAccBndlGrp\" : { \"attributes\" : { \"dn\" : \"uni/infra/funcprof/accbundle-intpolgrp-vpc-server1\" , \"lagT\" : \"node\" , \"name\" : \"intpolgrp-vpc-server1\" , \"rn\" : \"accbundle-intpolgrp-vpc-server1\" , \"status\" : \"created\" }, \"children\" : [ { \"infraRsAttEntP\" : { \"attributes\" : { \"tDn\" : \"uni/infra/attentp-dcloud-AAEP\" , \"status\" : \"created,modified\" }, \"children\" : [] } }, { \"infraRsHIfPol\" : { \"attributes\" : { \"tnFabricHIfPolName\" : \"LINK-10G\" , \"status\" : \"created,modified\" }, \"children\" : [] } }, { \"infraRsLldpIfPol\" : { \"attributes\" : { \"tnLldpIfPolName\" : \"LLDP_ON\" , \"status\" : \"created,modified\" }, \"children\" : [] } }, { \"infraRsLacpPol\" : { \"attributes\" : { \"tnLacpLagPolName\" : \"LACP_ACTIVE\" , \"status\" : \"created,modified\" }, \"children\" : [] } } ] } } Replace intpolgrp-vpc-server1 with intpolgrp-vpc-server2 to observe how simple you can add new VPC policy group using Postman. Warning Download three files to your dcloud workstation and using post in Postman or in APIC GUI upload it to access-polices. Without them, your VPC won't instanciate and cannot be used in later stage of the lab. JSON VPC Policy . JSON interface leaf profile . JSON switch leaf profile . Ask instructor for assistance if not clear.","title":"2.3 Interface Policy Group"},{"location":"restAPI/#3-aci-tenant","text":"Upon now you created ACI AccessPolicies for your Tenant. Having JSON code for every object you will be able to unified variables across Postman Collection and then run all together using Variables from Environment or much better from CSV file. Now is time to create your tenant using JSON. It will be simple Tenant definition with one VRF and two bridge domains associated with two EPGs. Every EPG will be associated with created domain and statically binded to both VPC created, with VLAN from VLAN pool done by you perviously. Figure below shows our Logical Tenant. You need to define Tenant, VRF, Bridge-Domains, Application-Profile, EPGs with Domain association and static binding under EPG. Quite many of Requests to do in Postman. You can define all of them under one Request. Please, download a JSON file from JSON Tenant Ready . Once you have it on dcloud workstation, please create new Request under ACI Collection: NAME: CREATE TENANT-1 Method: POST URL: https://{{apic}}/api/node/mo/uni.json COPY now content from downloaded file to Body of the new request, Save and Send it. Note Please check your authentication token if still valid. Tenant-1 will be created with all structure shown in figure above.","title":"3  ACI Tenant"},{"location":"restAPI/#31-tenant-components","text":"This section contain JSON codes necessary to create Tenant objects.","title":"3.1 Tenant components"},{"location":"restAPI/#311-tenant-and-vrf","text":"Code below can be used to create new Tenant and VRF. https://{{apic}}/api/node/mo/uni.json Tenant and VRF from CSV { \"fvTenant\" : { \"attributes\" : { \"annotation\" : \"\" , \"descr\" : \"\" , \"dn\" : \"uni/tn-{{tenantname}}\" , \"name\" : \"{{tenantname}}\" }, \"children\" : [ { \"fvCtx\" : { \"attributes\" : { \"name\" : \"{{vrfname}}\" }, \"children\" : [] } } ] } }","title":"3.1.1 Tenant and VRF"},{"location":"restAPI/#312-bd-in-existing-tenant","text":"Code below can be used to create new Bridge-Domain in existing Tenant. https://{{apic}}/api/node/mo/uni.json Bridge-domain L3 from CSV { \"fvBD\" : { \"attributes\" : { \"name\" : \"{{bdname}}\" , \"dn\" : \"uni/tn-{{tenantname}}/BD-{{bdname}}\" , \"rn\" : \"BD-{{bdname}}\" , \"type\" : \"regular\" }, \"children\" : [ { \"fvSubnet\" : { \"attributes\" : { \"ip\" : \"{{ipaddress}}\" , \"ipDPLearning\" : \"enabled\" , \"scope\" : \"private\" } } }, { \"fvRsCtx\" : { \"attributes\" : { \"tnFvCtxName\" : \"{{vrfname}}\" } } } ] } } In case you need to add BD without IP address and unicast routing disabled, use code below. https://{{apic}}/api/node/mo/uni.json Bridge-domain L2 from CSV { \"fvTenant\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}\" , \"status\" : \"modified\" }, \"children\" : [ { \"fvBD\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}/BD-{{bdname}}\" , \"mac\" : \"00:22:BD:F8:19:FF\" , \"name\" : \"{{bdname}}\" , \"rn\" : \"BD-{{bdname}}\" , \"arpFlood\" : \"yes\" , \"ipLearning\" : \"no\" , \"unicastRoute\" : \"no\" , \"unkMacUcastAct\" : \"flood\" , \"type\" : \"regular\" , \"unkMcastAct\" : \"flood\" , \"status\" : \"created\" }, \"children\" : [ { \"fvRsCtx\" : { \"attributes\" : { \"tnFvCtxName\" : \"{{vrfname}}\" , \"status\" : \"created,modified\" }, \"children\" : [] } } ] } } ] } } Highlighted rows are specific for L2 Bridge-Domain.","title":"3.1.2 BD in existing tenant"},{"location":"restAPI/#313-application-profile","text":"Component which contain all EPGs in the Tenant. https://{{apic}}/api/node/mo/uni.json Application Profile from CSV { \"fvAp\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}/ap-{{ap}}\" , \"name\" : \"{{ap}}\" , \"rn\" : \"ap-{{ap}}\" , \"status\" : \"created\" } } }","title":"3.1.3 Application Profile"},{"location":"restAPI/#314-epgs-in-existing-tenantappprofiles-and-associated-with-domain","text":"https://{{apic}}/api/node/mo/uni.json Create EPG under existing application profile from CSV { \"fvAEPg\" : { \"attributes\" : { \"dn\" : \"uni/tn-{{tenantname}}/ap-{{ap}}/epg-{{epgname}}\" , \"name\" : \"{{epgname}}\" , \"rn\" : \"epg-{{epgname}}\" , \"status\" : \"created\" }, \"children\" : [ { \"fvRsBd\" : { \"attributes\" : { \"tnFvBDname\" : \"{{bdname}}\" , \"status\" : \"created,modified\" }, \"children\" : [] } }, { \"fvRsDomAtt\" : { \"attributes\" : { \"tDn\" : \"uni/phys-{{domain}}\" , \"status\" : \"created\" }, \"children\" : [] } } ] } }","title":"3.1.4 EPGs in existing tenant/appprofiles and associated with domain"},{"location":"restAPI/#4-use-csv-file-for-input-data","text":"You are about to re-use work you did during all excercises. Untill now you run every request for static data, only once. Adding 100 Bridge-Domains, by changing \"Variables\" within the code would be problematic. Postman is giving an option to use external variable file to execute created requests in the collection. By running five requests from your Collection and using CSV file from location FILE CSV TO DOWNLOAD you will add new Tenant with one VRF, Applicatioin Profile, 22 Bridge-Domains and 22 EPGs associated to Physical Domain created before. All of this will be done in one Postman collection Run.","title":"4 Use CSV file for input data"},{"location":"restAPI/#41-run-collection-requests","text":"Follow the instruction from the figure below: 1) Click on dots icon at your ACI dCloud collection folder 2) Select Run Collection You will be moved to new dashboard, where you will select Requests and input file. Deselect All, and then Select only needed requests: When you select Requests, go to Data \"Select File\" and choice downloaded CSV file - tenant-create.csv . Confirm that your file is selected as on the figure above. You can confirm amount of iterations in the top field Iterations . Note Do not forget to authenticate your Postman session using ACI Login request. I recommend to do it out of collection Run -- beforehand. Click Run ACI dCloud Blue button.","title":"4.1 Run Collection requests"},{"location":"restAPI/#42-verification-after-runing-the-collection","text":"When you successfully execute your collection, in Summary at the screen you will see results of every Iteration. When Request was accepted by APIC, result will be 200 OK . Every Iteration contain all five seleceted requests. You probably noticed that one of them is getting 400 Bad Request - \"Bridge Domain L3 from CSV\". Reason for that, is missing children attribute fvSubnet for all L2 bridge-domain in our CSV file. Now connect to APIC GUI and verify if tenant exists, check dashboard status. You successfully Created New Tenant with 22 EPGs and 22 BDs. Please, edit CSV file in notepad++ editor at your dCloud workstation, replace Tenantname with new and run your Postman Collection once again. Do you understand now power of simple automation?","title":"4.2 Verification after runing the collection"},{"location":"use-case1/","text":"Use-case 01 - Stretched VRF Previous tasks were focusing on infrastructure configuration, from now on infrastructure is ready and will be working on logical topology. First use-case task will focus on: Configuration of stretch VRF inside Tenant-01 VPC CIDR, VNET configuration in respective clouds EPG configuration in both clouds EC2/Virutal Machine creation for communication testing Contract configuration to allow traffic flow Communication testing Schema, Template configuration All logical polices and configuration is done in Nexus Dashboard via Schemas and Templates. Each Schema can have multiple Templates, but Template can belong to only one Schema. Each Template is also assocaited to one and only one Tenant. Template to site assoction will also define to which fabric (on prem or cloud) configration will be pushed, therefore is the smallest logical unit we can decide where changes are deployed. 1. Schema creation Navigate to Dashboard -> Application Management -> Schemas , then hit \"Add Schema\" Add Schema name and Description and hit \"Add\" button Name: Schema-T01 Description: Schema for Tenant-01 2. Template creation Under the Schema-T01, let's create first Template with \"Add New Template\" button For a Template type select \"ACI Multi-Cloud\" and hit \"Add\" On the right side of the template screen, we can customize the Tempalte Display Name, and also select Tenant. Add Display Name and Select a Tenant. Display Name: temp-stretch-01 Tenant setting: Tenant-01 3. Template to site association For configuration in template to be deployed, appropriate sites need to be added. Sites added will decide to which fabric configuration will be pushed. For temp-stretch-01 , we want to add both Azure and AWS sites. To do it under the Template Properties locate the Actions button and hit Sites Associations Select both site CNC-AWS-01 and CNC-Azure-01 and hit Ok VRF, EPG, selectors deployment 1. VRF Configuration Inside Schema-T01 , inside temp-stretch-01 add the VRF with \"Add VRF\" button under VRFs box Add VRF Common Properties Display Name: VRF-01 Description: CL2023 Stretch VRF Leave other as default. As Tempalte was assocaited to both sites, we need to update details for each cloud, to do so under temp-stretch-01 expand \"Template Properties\" and go to \"CNC-AWS-01\" Click on the \"VRF-01\" which opens Site specific properties for this CNC-AWS-01 Site Under the \"Template Properties\" hit \"Add Region\" button and select Region from the list and then hit \"Add CIDRs\" button Region: eu-central-1 Now we need to specify what subnet we would like to use in AWS cloud for that VPC. This subnet will be configure as VPC CIDR on AWS cloud and will be used for VM and endpoint addressing. CIDR: 10.0.0.0/23 Our CIDR also needs to be divided into subnets for respective Availability Zones (AZ) - hit \"Add Subnet\" to configure it. We need to add subnet for a,b,c (number of AZ depends on the region), each subet needs to be confirmed with click on checkbox button. Create subnet for all 3 AZ in our Region, use non-virutal zone. Subnet: 10.0.0.0/25 Name: az-1a-subnet AZ: eu-central-1a Subnet: 10.0.0.128/25 Name: az-1b-subnet AZ: eu-central-1b Subnet: 10.0.1.0/25 Name: az-1c-subnet AZ: eu-central-1c When done, hit \"Save\" button to save subnet configuration. We also want to connect our subnets to Catalys8000V routers, this is done via Hub Network (Transit Gateway in AWS) - check the checkbox for \"Hub Network\" , select Hub Network and also all Subnets added. Hub Network: TGW-HUB Subnets: 10.0.0.0/2, 10.0.0.128/25, 10.0.1.0/25 Hit ok to finish configuration for CNC-AWS-01 fabric. Similar configuration has to be done for CNC-Azure-01 . Under temp-stretch-01 expand \"Tempalate Properties\" and go to \"CNC-Azure-01\" Click on the \"VRF-01\" which opens Site specific properties for this CNC-Azure-01 Site Under the \"Tempalate Properties\" hit \"Add Region\" button and select Region from the list and then hit \"Add CIDRs\" button Region: francecentral Now we need to specify what subnet we would like to use in AWS cloud for that VPC. CIDR: 10.0.0.0/23 Similar like for AWS we also needs to add subnet, but in case of Azure we don't need to select Availability Zones. We can add whole CIDR as subnet. When done, hit \"Save\" button to save subnet configuration. In case of Azure cloud, connection to CNC VNET is done via \"VNET Peering\" feature - enable it and select \"Default\" Hub Network Hit ok to finish configuration for CNC-Azure-01 fabric. Under temp-stretch-01 expand \"Template Properties\" and go to \"Template Properties\" main settings. When done \"Deploy to sites\" button will become active (blue) - click to deploy VRF to respective sites. Nexus Dashboard will also show you what changes are to be made. Review those and hit Deploy button to push configuration. If all goes well, confirmation pop-up will get displayed on the screen. At this point in both Azure and AWS cloud, there will be VNET/VPC (respectively) created with defined addressing. 2. VRF verification In the browser, open AWS console page. From the Region list, make sure that you have Frankfurt (eu-central-1) region selected. If not, switch to it. Search for \"VPC\" resource in Search tool and open it. You should have 2 VPCs created. context-[VRF-01]-addr-[10.0.0.0/23] - coresponding to NDO created VRF Default, without name, created automaticaly by AWS Let's check the same in Azure Cloud. Open the Azure Portal and login if needed. Search for Virtual Networks\" in search bar As we are using shared subscribtion in this lab, there will be 2 Virtual Networks VRF-01 - corresponding to our VRF overlay-1 - infrastructure VRF in which CNC and Cloud Routers are connected 3. Additional Template creation As per our topology diagram, EPGs in each Cloud are separate. As current temp-stretch-01 is associated with both sites, we need to create 2 new templates in the Schema-T01 schema, to Azure and AWS cloud sites respectively. Use the \"Add new Template\" and create 2 new templates with following names. Similar to first template form Actions mennu associate to respective sites. AWS Template: Template name: temp-AWS-01 Template type: ACI Multi-Cloud Template Tenant: Tenant-01 Site associated: CNC-AWS-01 Hit Save button to save the template. Azure Template: Template name: temp-Azure-01 Template type: ACI Multi-Cloud Template Tenant: Tenant-01 Site associated: CNC-Azure-01 Hit Save button to save the template. 4. Application Profile and EPG Configuration for AWS ACI in Cloud doesn't use the concept of Bridge Domain , they don't have any representation in Cloud, that's why IP ranges were defined as part of VRF configuration. EPGs can be now created and attached to VRF directly. Navigate to temp-AWS-01 template using View dropdown menu. Add Application Profile using Add Application Profile button. Display Name: AppProf-AWS-01 Description: Application Profile CL 2023 AWS Hit Save Under created Application Profile add EPG Display Name: EPG-AWS-01 Description: EPG AWS CL 2023 EPG Type: Application Skip the \"On-Premises Properties\" and click on \"Cloud Properties\" and select \"Virtual Routing & Forwarding\" as \"VRF-01\" . We also need to add Selector . Selectors are used in Public Cloud to assign Virtual Machines and Endpoints to correct EPG represented by a Security Group. Selector should be added Under Site Specific configuration for each EPG. Thare can be different type of selectores: IP based TAG based Region Based Custom In our case we will use IP based selectors. Under Template Properties swtich to CNC-AWS-01 Site, click on EPG-AWS-01 and hit \"Add Selector\" button under the EPG Cloud Properties. Selector details: Endpoint Selector Name: AWS-sel Expression: Type: IP address Operator: Equals Value: 10.0.0.0/23 Hit checkbox sign to save expression and then Ok to finish. Under Template Properties swtich back to Template Properties and hit \"Deploy to sites\" and \"Deploy\" . Once done confirmation will pop up on the screen. 5. Application Profile and EPG Configuration for Azure Navigate to temp-Azure-01 template using View dropdown menu. Add Application Profile using Add Application Profile button. Display Name: AppProf-Azure-01 Description: Application Profile CL 2023 AWS Hit Save Under created Application Profile add EPG Display Name: EPG-AWS-01 Description: EPG AWS CL 2023 EPG Type: Application Skip the \"On-Premises Properties\" and click on \"Cloud Properties\" and select \"Virtual Routing & Forwarding\" as \"VRF-01\" . We also need to add Selector . Selectors are used in Public Cloud to assign Virtual Machines and Endpoints to correct EPG represented by a Security Group. Selector should be added Under Site Specific configuration for each EPG. Under Template Properties swtich to CNC-Azure-01 Site, click on EPG-Azure-01 and hit \"Add Selector\" button under the EPG Cloud Properties. Selector details: Endpoint Selector Name: Azure-sel Expression: Type: IP address Operator: Equals Value: 10.100.0.0/23 Hit checkbox sign to save expression and then Ok to finish. Under Template Properties swtich back to Template Properties and hit \"Deploy to sites\" and \"Deploy\" . Once done confirmation will pop up on the screen. EC2/VM creation and verification Next step is creation of Vritual Machine/EC2 Instance in respective clouds for traffic veryfication. 1. AWS EC2 creation Login to AWS user tenant via https://console.aws.amazon.com and make sure that you have Frankfurt/eu-central-1 region selected To be able to launch and login to VM we need to have SSH key-pair created. In seach bar type \"key pairs\" and select Key Pairs from Features list. Hit \"Create key pair\" in top right corner and provide following settings: Name: AWS-key-pair Key pair type: RSA Private key file format: .pem Hit \"Create key pair\" to finish. Once you do it key will be downloaded to your desktop, so you can use for connection to VMs. Make sure to store it in know place on your desktop. In seach bar type \"EC2\" and select EC2 from Service list. On the EC2 Dashboard locate \"Launch instance\" and hit Launch instance option. Instance details: Name: VM-AWS-01 Application and OS Images: Amazon Linux Architecture: 64-bit Instance type: t2.micro Key pair(login): AWS-key-pair Network settings (hit Edit to change): VPC: context-[VRF-01]-addr[10.0.0.0/23] subnet: subnet-[10.0.0.0/25] Auto-assign public IP: Enable Firewall (security groups): leave default Other setting leave default Review summary and hit \"Launch instance\" to create Virtual Machine It may take 3-5 minutes for instance to be ready - you may jump to next step and deploy instance in Azure, then come back here for veryfication. 2. AWS EC2 veryfication Go back to EC2 -> Instances and locate your EC2 machine. Click on Instance-ID of you machine to open it. Verify that VPC ID is correct Verify that subnet is correct Verify that instance has correct Private IPv4 addressess\" Note down IP address of EC2 instance, we will need it for veryfication. Go to Security settings of VM and check that Security groups for you instance is the one related to EPG-AWS-01 This security group was pushed when EPG was deployed towards AWS site. Instance interfaces was assigned to this EPG/Security Group based on the IP based Selector configured for this EPG. Cloud Network Controller is monitoring resources created in managed Tenants and it's automatically assigning Security groupes based on selectors. 3. Azure Virutal Machine creation Login to Azure portal via https://portal.azure.com with your account Search for Virtual machine in search bar and open from Services list Under Create button select \"Azure virtual machine\" Virutal Machine details: !!!Note If setting is not listed, leave default. Subscription: leave selected Resource Group: create new \"RG-CL23\" Virtual Machine name: VM-AZ-01 Region: (Europe) France Central Authentication type: Password username: student password: CiscoLive2023! Hit \"Next: Disks >\" and accept all default values, hit \"Next: Networking >\" and configure: Virtual Network: VRF-01 Subnet: az-subnet (10.100.0.0/23) Public IP: leave default Hit directly \"Review + Create\" and all other setting will stat default. Hit directly \"Create\" to deploy Virutal Machine. It may take 3-5 minutes for instance to be ready. Portal will notify you when done. 4. Azure Virutal Machine veryfication Go back to Services -> Virutal machines and locate your Virutal Machine. Click on the name to open it. Under the \"Settings\" , go to \"Networking\" and then \"Application Security groups (ASG)\" and notice that our Virutal Machine is assigned to \"EPG-Azure-01_cloudapp-AppProf-Azure-01\" securty group. Similarly like for AWS, this ASG was assigned due to Selector configuration for EPG. Cross Cloud traffic verification Let's check if our Virutal Machines are able to communicate. For now we have configured that part of our infrastructure. On the Azure Virtual Machine, scroll down to \"Help\" Section and select \"Serial Console\" Hit enter and provide VM login credentials: username: student password: CiscoLive2023! Inside the console try to reach via ping to AWS EC2 IP address we noted earlier. student@VM-AZ-01:~$ ping 10.0.0.106 PING 10.0.0.106 (10.0.0.106) 56(84) bytes of data. --- 10.0.0.106 ping statistics --- 7 packets transmitted, 0 received, 100% packet loss, time 6134ms student@VM-AZ-01:~$ It doesnt work, what is expected as we didn't connect our two EPGs via contract. So even they are part of the same VRF, they are not able to communicate, as exactly same principles as on ACI applies. Contract configuration 1. Filter creation In order to create Contract, we need to have a filter which defines what type of traffis is allowed or denied under specyfic contract. Open Nexus Dashboard Orchestrator GUI then go to Application Management -> Schemas -> \"Schema-T01\" -> open Under View select \"temp-stretch-01\" and \"Add filter\" under Filter section. Display Name: permit-any Then click \"Add entry\" to define protocols and ports. Name: permit-any Leave rest setting as default - this will allow for all protocols and ports. Hit Ok to save it. 2. Contract configuration As contracts will be used to connect AWS EPG to Azure EPG, contract should be created in stretched templates so it's deployed in both sites. Under View select \"temp-stretch-01\" and \"Add contract\" under Contract section. Create two(2) contracts with following details: Contract con-AWS-01-to-Azure-01 Display Name: con-AWS-01-to-Azure-01 Scope: VRF Apply both direction: yes Add Filter: permit-any Hit Save to finish contract configuration. Contract con-Azure-01-to-AWS-01 Display Name: con-Azure-01-to-AWS-01 Scope: VRF Apply both direction: yes Add Filter: permit-any Hit Save to finish contract configuration. Hit Deploy to sites for contracts to be pushed. 2. Contract assigment to EPGs Just creation of contract doesn't have any impact on the traffic. Contract needs to have at least on Provider EPG and one Consumer EPG to allow communication between them. Under View select \"temp-AWS-01\" click on the EPG-AWS-01 and under EPG specific setting locate Contract section Hit \"Add Conctract\" button and add the following Contract 1: EPG: EPG-AWS-01 Contract: 152 Type: provider Contract 2: EPG: EPG-AWS-01 Contract: con-Azure-01-to-AWS-01 Type: consumer Hit Deploy to sites for contracts to be applied to EPGs. Under View select \"temp-Azure-01\" click on the EPG-Azure-01 and under EPG specific setting locate Contract section Hit \"Add Conctract\" button and add the following Contract 1: EPG: EPG-Azure-01 Contract: con-Azure-01-to-AWS-01 Type: provider Contract 2: EPG: EPG-Azure-01 Contract: con-AWS-01-to-Azure-01 Type: consumer Hit Deploy to sites for contracts to be applied to EPGs. Cross Cloud traffic verification attempt 2 Let's check now if our Virutal Machine are able to communicate. For now we have configured that part of our infrastructure. On the Azure Virtual Machine, scroll down to \"Help\" Section and select \"Serial Console\" Hit enter and provide VM login credentials: username: student password: CiscoLive2023! Once in the console try to reach via ping to AWS EC2 IP address we noted earlier. student@VM-AZ-01:~$ ping 10.0.0.106 PING 10.0.0.106 (10.0.0.106) 56(84) bytes of data. 64 bytes from 10.0.0.106: icmp_seq=1 ttl=252 time=14.7 ms 64 bytes from 10.0.0.106: icmp_seq=2 ttl=252 time=12.1 ms 64 bytes from 10.0.0.106: icmp_seq=3 ttl=252 time=12.2 ms 64 bytes from 10.0.0.106: icmp_seq=4 ttl=252 time=11.9 ms 64 bytes from 10.0.0.106: icmp_seq=5 ttl=252 time=12.1 ms ^C --- 10.0.0.106 ping statistics --- 5 packets transmitted, 5 received, 0% packet loss, time 4007ms rtt min/avg/max/mdev = 11.893/12.598/14.707/1.060 ms student@VM-AZ-01:~$ Contract configuration makes communication possible. Cross Cloud traffic infrastructure check Let's now check what was confiugure on the infrastructure that traffic can now flow. Azure Cloud veryfication Go back to Services -> Virutal machines and locate your Virutal Machine. Click on the name to open it. Under the \"Settings\" , go to \"Networking\" and then take a look in Inbound port rules and Outband port rules . Our contract configuration added one more rule to the port rules which allows for communication with CIDR of AWS Cloud. Under the \"Settings\" , go to \"Networking\" locate the \"Network Interface\" and click on it's ID. Under the Network Interface locate the Help Section in left navigation menu and select \"Effective Routes\" tab It may take couple of second to load, once loaded scroll down to the end of the list and notice one User Route added. You can see that subnet representing AWS CIDR space - 10.0.0.0/23 is routed to Virtual Appliance which is an Internal Load Balancer balancing the traffic between 2 Cloud Routers. With that configuration, traffic is send to Azure Cloud Routers from which via IPSec tunnels is send to AWS Cloud Routers and towards destination AWS VPC. AWS Cloud veryfication Login to AWS user tenant via https://console.aws.amazon.com and make sure that you have Frankfurt/eu-central-1 region selected In seach bar type \"Subnet\" and select Subnet (VPC feature) from Features list. Locate you subnet subnet-[10.0.0.0/25] and click on Subnet ID assocaited to open that subnet. On the subnet details page, scroll down and locate \"Route Table\" tab, click on it Notice that there was route added for Azure VNET space - 10.100.0.0/23 towards \"tgw-*\" which is our Transit Gateway used to communicated with AWS Cloud Routers , so traffic can be send to Azure. Cloud Routers Veryfication Open Putty client from desktop and put IP address of Cloud router from Azure - you can login to any of 4 routers. Note Cloud Routers IP address are avaibale in POD details IP address schema, or you can find them in Azure/AWS console directly. Login with username \"csradmin\" and password \"CiscoLive2023!\" Execute command \"show vrf | inc VRF-01 show vrf | inc VRF-01 Expected output: ct-routerp-francecentral-0-0#show vrf | inc VRF-01 Tenant-01:VRF-01 10.20.0.52:7 ipv4 BD7 ct-routerp-francecentral-0-0# Check the routing table for that VRF Execute command \"show ip route vrf Tenant-01:VRF-01 show ip route vrf Tenant-01:VRF-01 Expected output: ct-routerp-francecentral-0-0#show ip route vrf Tenant-01:VRF-01 Routing Table: Tenant-01:VRF-01 [output truncated] Gateway of last resort is not set 10.0.0.0/23 is subnetted, 2 subnets B 10.0.0.0 [20/0] via binding label: 0x3000001, 1d21h [20/0] via binding label: 0x3000002, 1d21h S 10.100.0.0 [1/0] via 10.20.0.1, GigabitEthernet2 192.168.100.0/24 is variably subnetted, 2 subnets, 2 masks C 192.168.100.0/24 is directly connected, BDI7 L 192.168.100.100/32 is directly connected, BDI7 ct-routerp-francecentral-0-0# Output shows routing table from perspective of Azure Cloud Router, hence: Azure subnet 10.100.0.0/23 is available via Static route towards interfaces GigabitEthernet2 , which is interfaces conencted to inside of Azure. AWS subnet 10.0.0.0/23 is available from BGP protocol, from two (2) Cloud Router located in AWS Cloud In that way cloud routers know how to route traffic towards respective clouds.","title":"Use-case 01 - Stretched VRF"},{"location":"use-case1/#use-case-01-stretched-vrf","text":"Previous tasks were focusing on infrastructure configuration, from now on infrastructure is ready and will be working on logical topology. First use-case task will focus on: Configuration of stretch VRF inside Tenant-01 VPC CIDR, VNET configuration in respective clouds EPG configuration in both clouds EC2/Virutal Machine creation for communication testing Contract configuration to allow traffic flow Communication testing","title":"Use-case 01 - Stretched VRF"},{"location":"use-case1/#schema-template-configuration","text":"All logical polices and configuration is done in Nexus Dashboard via Schemas and Templates. Each Schema can have multiple Templates, but Template can belong to only one Schema. Each Template is also assocaited to one and only one Tenant. Template to site assoction will also define to which fabric (on prem or cloud) configration will be pushed, therefore is the smallest logical unit we can decide where changes are deployed.","title":"Schema, Template configuration"},{"location":"use-case1/#1-schema-creation","text":"Navigate to Dashboard -> Application Management -> Schemas , then hit \"Add Schema\" Add Schema name and Description and hit \"Add\" button Name: Schema-T01 Description: Schema for Tenant-01","title":"1. Schema creation"},{"location":"use-case1/#2-template-creation","text":"Under the Schema-T01, let's create first Template with \"Add New Template\" button For a Template type select \"ACI Multi-Cloud\" and hit \"Add\" On the right side of the template screen, we can customize the Tempalte Display Name, and also select Tenant. Add Display Name and Select a Tenant. Display Name: temp-stretch-01 Tenant setting: Tenant-01","title":"2. Template creation"},{"location":"use-case1/#3-template-to-site-association","text":"For configuration in template to be deployed, appropriate sites need to be added. Sites added will decide to which fabric configuration will be pushed. For temp-stretch-01 , we want to add both Azure and AWS sites. To do it under the Template Properties locate the Actions button and hit Sites Associations Select both site CNC-AWS-01 and CNC-Azure-01 and hit Ok","title":"3. Template to site association"},{"location":"use-case1/#vrf-epg-selectors-deployment","text":"","title":"VRF, EPG, selectors deployment"},{"location":"use-case1/#1-vrf-configuration","text":"Inside Schema-T01 , inside temp-stretch-01 add the VRF with \"Add VRF\" button under VRFs box Add VRF Common Properties Display Name: VRF-01 Description: CL2023 Stretch VRF Leave other as default. As Tempalte was assocaited to both sites, we need to update details for each cloud, to do so under temp-stretch-01 expand \"Template Properties\" and go to \"CNC-AWS-01\" Click on the \"VRF-01\" which opens Site specific properties for this CNC-AWS-01 Site Under the \"Template Properties\" hit \"Add Region\" button and select Region from the list and then hit \"Add CIDRs\" button Region: eu-central-1 Now we need to specify what subnet we would like to use in AWS cloud for that VPC. This subnet will be configure as VPC CIDR on AWS cloud and will be used for VM and endpoint addressing. CIDR: 10.0.0.0/23 Our CIDR also needs to be divided into subnets for respective Availability Zones (AZ) - hit \"Add Subnet\" to configure it. We need to add subnet for a,b,c (number of AZ depends on the region), each subet needs to be confirmed with click on checkbox button. Create subnet for all 3 AZ in our Region, use non-virutal zone. Subnet: 10.0.0.0/25 Name: az-1a-subnet AZ: eu-central-1a Subnet: 10.0.0.128/25 Name: az-1b-subnet AZ: eu-central-1b Subnet: 10.0.1.0/25 Name: az-1c-subnet AZ: eu-central-1c When done, hit \"Save\" button to save subnet configuration. We also want to connect our subnets to Catalys8000V routers, this is done via Hub Network (Transit Gateway in AWS) - check the checkbox for \"Hub Network\" , select Hub Network and also all Subnets added. Hub Network: TGW-HUB Subnets: 10.0.0.0/2, 10.0.0.128/25, 10.0.1.0/25 Hit ok to finish configuration for CNC-AWS-01 fabric. Similar configuration has to be done for CNC-Azure-01 . Under temp-stretch-01 expand \"Tempalate Properties\" and go to \"CNC-Azure-01\" Click on the \"VRF-01\" which opens Site specific properties for this CNC-Azure-01 Site Under the \"Tempalate Properties\" hit \"Add Region\" button and select Region from the list and then hit \"Add CIDRs\" button Region: francecentral Now we need to specify what subnet we would like to use in AWS cloud for that VPC. CIDR: 10.0.0.0/23 Similar like for AWS we also needs to add subnet, but in case of Azure we don't need to select Availability Zones. We can add whole CIDR as subnet. When done, hit \"Save\" button to save subnet configuration. In case of Azure cloud, connection to CNC VNET is done via \"VNET Peering\" feature - enable it and select \"Default\" Hub Network Hit ok to finish configuration for CNC-Azure-01 fabric. Under temp-stretch-01 expand \"Template Properties\" and go to \"Template Properties\" main settings. When done \"Deploy to sites\" button will become active (blue) - click to deploy VRF to respective sites. Nexus Dashboard will also show you what changes are to be made. Review those and hit Deploy button to push configuration. If all goes well, confirmation pop-up will get displayed on the screen. At this point in both Azure and AWS cloud, there will be VNET/VPC (respectively) created with defined addressing.","title":"1. VRF Configuration"},{"location":"use-case1/#2-vrf-verification","text":"In the browser, open AWS console page. From the Region list, make sure that you have Frankfurt (eu-central-1) region selected. If not, switch to it. Search for \"VPC\" resource in Search tool and open it. You should have 2 VPCs created. context-[VRF-01]-addr-[10.0.0.0/23] - coresponding to NDO created VRF Default, without name, created automaticaly by AWS Let's check the same in Azure Cloud. Open the Azure Portal and login if needed. Search for Virtual Networks\" in search bar As we are using shared subscribtion in this lab, there will be 2 Virtual Networks VRF-01 - corresponding to our VRF overlay-1 - infrastructure VRF in which CNC and Cloud Routers are connected","title":"2. VRF verification"},{"location":"use-case1/#3-additional-template-creation","text":"As per our topology diagram, EPGs in each Cloud are separate. As current temp-stretch-01 is associated with both sites, we need to create 2 new templates in the Schema-T01 schema, to Azure and AWS cloud sites respectively. Use the \"Add new Template\" and create 2 new templates with following names. Similar to first template form Actions mennu associate to respective sites. AWS Template: Template name: temp-AWS-01 Template type: ACI Multi-Cloud Template Tenant: Tenant-01 Site associated: CNC-AWS-01 Hit Save button to save the template. Azure Template: Template name: temp-Azure-01 Template type: ACI Multi-Cloud Template Tenant: Tenant-01 Site associated: CNC-Azure-01 Hit Save button to save the template.","title":"3. Additional Template creation"},{"location":"use-case1/#4-application-profile-and-epg-configuration-for-aws","text":"ACI in Cloud doesn't use the concept of Bridge Domain , they don't have any representation in Cloud, that's why IP ranges were defined as part of VRF configuration. EPGs can be now created and attached to VRF directly. Navigate to temp-AWS-01 template using View dropdown menu. Add Application Profile using Add Application Profile button. Display Name: AppProf-AWS-01 Description: Application Profile CL 2023 AWS Hit Save Under created Application Profile add EPG Display Name: EPG-AWS-01 Description: EPG AWS CL 2023 EPG Type: Application Skip the \"On-Premises Properties\" and click on \"Cloud Properties\" and select \"Virtual Routing & Forwarding\" as \"VRF-01\" . We also need to add Selector . Selectors are used in Public Cloud to assign Virtual Machines and Endpoints to correct EPG represented by a Security Group. Selector should be added Under Site Specific configuration for each EPG. Thare can be different type of selectores: IP based TAG based Region Based Custom In our case we will use IP based selectors. Under Template Properties swtich to CNC-AWS-01 Site, click on EPG-AWS-01 and hit \"Add Selector\" button under the EPG Cloud Properties. Selector details: Endpoint Selector Name: AWS-sel Expression: Type: IP address Operator: Equals Value: 10.0.0.0/23 Hit checkbox sign to save expression and then Ok to finish. Under Template Properties swtich back to Template Properties and hit \"Deploy to sites\" and \"Deploy\" . Once done confirmation will pop up on the screen.","title":"4. Application Profile and EPG Configuration for AWS"},{"location":"use-case1/#5-application-profile-and-epg-configuration-for-azure","text":"Navigate to temp-Azure-01 template using View dropdown menu. Add Application Profile using Add Application Profile button. Display Name: AppProf-Azure-01 Description: Application Profile CL 2023 AWS Hit Save Under created Application Profile add EPG Display Name: EPG-AWS-01 Description: EPG AWS CL 2023 EPG Type: Application Skip the \"On-Premises Properties\" and click on \"Cloud Properties\" and select \"Virtual Routing & Forwarding\" as \"VRF-01\" . We also need to add Selector . Selectors are used in Public Cloud to assign Virtual Machines and Endpoints to correct EPG represented by a Security Group. Selector should be added Under Site Specific configuration for each EPG. Under Template Properties swtich to CNC-Azure-01 Site, click on EPG-Azure-01 and hit \"Add Selector\" button under the EPG Cloud Properties. Selector details: Endpoint Selector Name: Azure-sel Expression: Type: IP address Operator: Equals Value: 10.100.0.0/23 Hit checkbox sign to save expression and then Ok to finish. Under Template Properties swtich back to Template Properties and hit \"Deploy to sites\" and \"Deploy\" . Once done confirmation will pop up on the screen.","title":"5. Application Profile and EPG Configuration for Azure"},{"location":"use-case1/#ec2vm-creation-and-verification","text":"Next step is creation of Vritual Machine/EC2 Instance in respective clouds for traffic veryfication.","title":"EC2/VM creation and verification"},{"location":"use-case1/#1-aws-ec2-creation","text":"Login to AWS user tenant via https://console.aws.amazon.com and make sure that you have Frankfurt/eu-central-1 region selected To be able to launch and login to VM we need to have SSH key-pair created. In seach bar type \"key pairs\" and select Key Pairs from Features list. Hit \"Create key pair\" in top right corner and provide following settings: Name: AWS-key-pair Key pair type: RSA Private key file format: .pem Hit \"Create key pair\" to finish. Once you do it key will be downloaded to your desktop, so you can use for connection to VMs. Make sure to store it in know place on your desktop. In seach bar type \"EC2\" and select EC2 from Service list. On the EC2 Dashboard locate \"Launch instance\" and hit Launch instance option. Instance details: Name: VM-AWS-01 Application and OS Images: Amazon Linux Architecture: 64-bit Instance type: t2.micro Key pair(login): AWS-key-pair Network settings (hit Edit to change): VPC: context-[VRF-01]-addr[10.0.0.0/23] subnet: subnet-[10.0.0.0/25] Auto-assign public IP: Enable Firewall (security groups): leave default Other setting leave default Review summary and hit \"Launch instance\" to create Virtual Machine It may take 3-5 minutes for instance to be ready - you may jump to next step and deploy instance in Azure, then come back here for veryfication.","title":"1. AWS EC2 creation"},{"location":"use-case1/#2-aws-ec2-veryfication","text":"Go back to EC2 -> Instances and locate your EC2 machine. Click on Instance-ID of you machine to open it. Verify that VPC ID is correct Verify that subnet is correct Verify that instance has correct Private IPv4 addressess\" Note down IP address of EC2 instance, we will need it for veryfication. Go to Security settings of VM and check that Security groups for you instance is the one related to EPG-AWS-01 This security group was pushed when EPG was deployed towards AWS site. Instance interfaces was assigned to this EPG/Security Group based on the IP based Selector configured for this EPG. Cloud Network Controller is monitoring resources created in managed Tenants and it's automatically assigning Security groupes based on selectors.","title":"2. AWS EC2 veryfication"},{"location":"use-case1/#3-azure-virutal-machine-creation","text":"Login to Azure portal via https://portal.azure.com with your account Search for Virtual machine in search bar and open from Services list Under Create button select \"Azure virtual machine\" Virutal Machine details: !!!Note If setting is not listed, leave default. Subscription: leave selected Resource Group: create new \"RG-CL23\" Virtual Machine name: VM-AZ-01 Region: (Europe) France Central Authentication type: Password username: student password: CiscoLive2023! Hit \"Next: Disks >\" and accept all default values, hit \"Next: Networking >\" and configure: Virtual Network: VRF-01 Subnet: az-subnet (10.100.0.0/23) Public IP: leave default Hit directly \"Review + Create\" and all other setting will stat default. Hit directly \"Create\" to deploy Virutal Machine. It may take 3-5 minutes for instance to be ready. Portal will notify you when done.","title":"3. Azure Virutal Machine creation"},{"location":"use-case1/#4-azure-virutal-machine-veryfication","text":"Go back to Services -> Virutal machines and locate your Virutal Machine. Click on the name to open it. Under the \"Settings\" , go to \"Networking\" and then \"Application Security groups (ASG)\" and notice that our Virutal Machine is assigned to \"EPG-Azure-01_cloudapp-AppProf-Azure-01\" securty group. Similarly like for AWS, this ASG was assigned due to Selector configuration for EPG.","title":"4. Azure Virutal Machine veryfication"},{"location":"use-case1/#cross-cloud-traffic-verification","text":"Let's check if our Virutal Machines are able to communicate. For now we have configured that part of our infrastructure. On the Azure Virtual Machine, scroll down to \"Help\" Section and select \"Serial Console\" Hit enter and provide VM login credentials: username: student password: CiscoLive2023! Inside the console try to reach via ping to AWS EC2 IP address we noted earlier. student@VM-AZ-01:~$ ping 10.0.0.106 PING 10.0.0.106 (10.0.0.106) 56(84) bytes of data. --- 10.0.0.106 ping statistics --- 7 packets transmitted, 0 received, 100% packet loss, time 6134ms student@VM-AZ-01:~$ It doesnt work, what is expected as we didn't connect our two EPGs via contract. So even they are part of the same VRF, they are not able to communicate, as exactly same principles as on ACI applies.","title":"Cross Cloud traffic verification"},{"location":"use-case1/#contract-configuration","text":"","title":"Contract configuration"},{"location":"use-case1/#1-filter-creation","text":"In order to create Contract, we need to have a filter which defines what type of traffis is allowed or denied under specyfic contract. Open Nexus Dashboard Orchestrator GUI then go to Application Management -> Schemas -> \"Schema-T01\" -> open Under View select \"temp-stretch-01\" and \"Add filter\" under Filter section. Display Name: permit-any Then click \"Add entry\" to define protocols and ports. Name: permit-any Leave rest setting as default - this will allow for all protocols and ports. Hit Ok to save it.","title":"1. Filter creation"},{"location":"use-case1/#2-contract-configuration","text":"As contracts will be used to connect AWS EPG to Azure EPG, contract should be created in stretched templates so it's deployed in both sites. Under View select \"temp-stretch-01\" and \"Add contract\" under Contract section. Create two(2) contracts with following details: Contract con-AWS-01-to-Azure-01 Display Name: con-AWS-01-to-Azure-01 Scope: VRF Apply both direction: yes Add Filter: permit-any Hit Save to finish contract configuration. Contract con-Azure-01-to-AWS-01 Display Name: con-Azure-01-to-AWS-01 Scope: VRF Apply both direction: yes Add Filter: permit-any Hit Save to finish contract configuration. Hit Deploy to sites for contracts to be pushed.","title":"2. Contract configuration"},{"location":"use-case1/#2-contract-assigment-to-epgs","text":"Just creation of contract doesn't have any impact on the traffic. Contract needs to have at least on Provider EPG and one Consumer EPG to allow communication between them. Under View select \"temp-AWS-01\" click on the EPG-AWS-01 and under EPG specific setting locate Contract section Hit \"Add Conctract\" button and add the following Contract 1: EPG: EPG-AWS-01 Contract: 152 Type: provider Contract 2: EPG: EPG-AWS-01 Contract: con-Azure-01-to-AWS-01 Type: consumer Hit Deploy to sites for contracts to be applied to EPGs. Under View select \"temp-Azure-01\" click on the EPG-Azure-01 and under EPG specific setting locate Contract section Hit \"Add Conctract\" button and add the following Contract 1: EPG: EPG-Azure-01 Contract: con-Azure-01-to-AWS-01 Type: provider Contract 2: EPG: EPG-Azure-01 Contract: con-AWS-01-to-Azure-01 Type: consumer Hit Deploy to sites for contracts to be applied to EPGs.","title":"2. Contract assigment to EPGs"},{"location":"use-case1/#cross-cloud-traffic-verification-attempt-2","text":"Let's check now if our Virutal Machine are able to communicate. For now we have configured that part of our infrastructure. On the Azure Virtual Machine, scroll down to \"Help\" Section and select \"Serial Console\" Hit enter and provide VM login credentials: username: student password: CiscoLive2023! Once in the console try to reach via ping to AWS EC2 IP address we noted earlier. student@VM-AZ-01:~$ ping 10.0.0.106 PING 10.0.0.106 (10.0.0.106) 56(84) bytes of data. 64 bytes from 10.0.0.106: icmp_seq=1 ttl=252 time=14.7 ms 64 bytes from 10.0.0.106: icmp_seq=2 ttl=252 time=12.1 ms 64 bytes from 10.0.0.106: icmp_seq=3 ttl=252 time=12.2 ms 64 bytes from 10.0.0.106: icmp_seq=4 ttl=252 time=11.9 ms 64 bytes from 10.0.0.106: icmp_seq=5 ttl=252 time=12.1 ms ^C --- 10.0.0.106 ping statistics --- 5 packets transmitted, 5 received, 0% packet loss, time 4007ms rtt min/avg/max/mdev = 11.893/12.598/14.707/1.060 ms student@VM-AZ-01:~$ Contract configuration makes communication possible.","title":"Cross Cloud traffic verification attempt 2"},{"location":"use-case1/#cross-cloud-traffic-infrastructure-check","text":"Let's now check what was confiugure on the infrastructure that traffic can now flow.","title":"Cross Cloud traffic infrastructure check"},{"location":"use-case1/#azure-cloud-veryfication","text":"Go back to Services -> Virutal machines and locate your Virutal Machine. Click on the name to open it. Under the \"Settings\" , go to \"Networking\" and then take a look in Inbound port rules and Outband port rules . Our contract configuration added one more rule to the port rules which allows for communication with CIDR of AWS Cloud. Under the \"Settings\" , go to \"Networking\" locate the \"Network Interface\" and click on it's ID. Under the Network Interface locate the Help Section in left navigation menu and select \"Effective Routes\" tab It may take couple of second to load, once loaded scroll down to the end of the list and notice one User Route added. You can see that subnet representing AWS CIDR space - 10.0.0.0/23 is routed to Virtual Appliance which is an Internal Load Balancer balancing the traffic between 2 Cloud Routers. With that configuration, traffic is send to Azure Cloud Routers from which via IPSec tunnels is send to AWS Cloud Routers and towards destination AWS VPC.","title":"Azure Cloud veryfication"},{"location":"use-case1/#aws-cloud-veryfication","text":"Login to AWS user tenant via https://console.aws.amazon.com and make sure that you have Frankfurt/eu-central-1 region selected In seach bar type \"Subnet\" and select Subnet (VPC feature) from Features list. Locate you subnet subnet-[10.0.0.0/25] and click on Subnet ID assocaited to open that subnet. On the subnet details page, scroll down and locate \"Route Table\" tab, click on it Notice that there was route added for Azure VNET space - 10.100.0.0/23 towards \"tgw-*\" which is our Transit Gateway used to communicated with AWS Cloud Routers , so traffic can be send to Azure.","title":"AWS Cloud veryfication"},{"location":"use-case1/#cloud-routers-veryfication","text":"Open Putty client from desktop and put IP address of Cloud router from Azure - you can login to any of 4 routers. Note Cloud Routers IP address are avaibale in POD details IP address schema, or you can find them in Azure/AWS console directly. Login with username \"csradmin\" and password \"CiscoLive2023!\" Execute command \"show vrf | inc VRF-01 show vrf | inc VRF-01 Expected output: ct-routerp-francecentral-0-0#show vrf | inc VRF-01 Tenant-01:VRF-01 10.20.0.52:7 ipv4 BD7 ct-routerp-francecentral-0-0# Check the routing table for that VRF Execute command \"show ip route vrf Tenant-01:VRF-01 show ip route vrf Tenant-01:VRF-01 Expected output: ct-routerp-francecentral-0-0#show ip route vrf Tenant-01:VRF-01 Routing Table: Tenant-01:VRF-01 [output truncated] Gateway of last resort is not set 10.0.0.0/23 is subnetted, 2 subnets B 10.0.0.0 [20/0] via binding label: 0x3000001, 1d21h [20/0] via binding label: 0x3000002, 1d21h S 10.100.0.0 [1/0] via 10.20.0.1, GigabitEthernet2 192.168.100.0/24 is variably subnetted, 2 subnets, 2 masks C 192.168.100.0/24 is directly connected, BDI7 L 192.168.100.100/32 is directly connected, BDI7 ct-routerp-francecentral-0-0# Output shows routing table from perspective of Azure Cloud Router, hence: Azure subnet 10.100.0.0/23 is available via Static route towards interfaces GigabitEthernet2 , which is interfaces conencted to inside of Azure. AWS subnet 10.0.0.0/23 is available from BGP protocol, from two (2) Cloud Router located in AWS Cloud In that way cloud routers know how to route traffic towards respective clouds.","title":"Cloud Routers Veryfication"},{"location":"use-case2/","text":"Use-case 02 - Internet Gateway In this task we will configure Internet Gateway in AWS cloud, so Virtual Machines and Endpoints are able to reach the Internet Instances in AWS can assess Internet in AWS cloud with help of Internet Gateway. An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between VPC and the internet. An internet gateway provides a target VPC route tables for internet-routable traffic. For communication using IPv4, the internet gateway also performs network address translation (NAT). Internet Gateway as External EPG As all VPC aspects and routing are controller by Cloud Network Controller, also Internet Gateway(IGW) it's configured with it's help. Internet Gateway is represented by well know ACI object - External EPG. External EPG configuration Open Nexus Dashboard Orchestrator GUI then go to Application Management -> Schemas -> \"Schema-T01\" -> open Under View select \"temp-AWS-01\" and hit \"Add External EPGs\" under External EPGs section. Display Name: ExtEPG-IGW Description: Ext EPG for Internet Gateway AWS Virutal Routing & Forwarding: VRF-01 Select Site Type: CLOUD Cloud Properties: Application Profile: AppProf-AWS-01 Add Selector: Endpoint Selector Name: Internet Expression Type: IP Address Expression Operator: Equal Expression Value: 0.0.0.0/0 Hit Save to finish. Contract configuration configuration As External EPG is functioning as any other EPG, contract configuration from EPG to External EPG is mandatory for traffic to be allowed. 1. Filter creation Open Nexus Dashboard Orchestrator GUI then go to Application Management -> Schemas -> \"Schema-T01\" -> open Under View select \"temp-AWS-01\"\" and \"Add filter\" under Filter section. Display Name: permit-any-aws Then click \"Add entry\" to define protocols and ports. Name: permit-any Leave rest setting as default - this will allow for all protocols and ports. Hit Ok to save it. 2. Contract configuration As contracts will be used to connect AWS EPG to Ext EPG of AWS , contract should be created in AWS template Under View select \"temp-AWS-01\" and \"Add contract\" under Contract section. Create two(2) contracts with following details: Contract con-AWS-01-to-IGW Display Name: con-AWS-01-to-IGW Scope: VRF Apply both direction: yes Add Filter: permit-any-AWS Hit Save to finish contract configuration. Contract con-IGW-to-AWS-01 Display Name: con-IGW-to-AWS-01 Scope: VRF Apply both direction: yes Add Filter: permit-any-AWS Hit Save to finish contract configuration. Hit Deploy to sites for contracts to be pushed. 3. Contract assigment to EPGs Under View select \"temp-AWS-01\" click on the EPG-AWS-01 and under EPG specific setting locate Contract section Hit \"Add Contract\" button and add the following Contract 1: EPG: EPG-AWS01 Contract: con-AWS-01-to-IGW * Type: provider Contract 2: EPG: EPG-AWS-01 Contract: con-IGW-to-AWS-01 Type: consumer Under View select \"temp-AWS-01\" click on the ExtEPG-IGW and under Common Properties locate Contract section Hit \"Add Contract\" button and add the following Contract 1: EPG: ExtEPG-IGW Contract: con-IGW-to-AWS-01 Type: provider Contract 2: EPG: ExtEPG-IGW Contract: con-AWS-01-to-IGW Type: consumer Hit Deploy to sites to create IGW and deploy contracts associations. Veryfication Let's check if AWS EC2 instance is now reachable from internet. 1. Open AWS console via browser 2. Select IAM user, provide Account ID for User tenant and hit \"Next\" 3. Provide Username and password and hit \"Sign In\" 4. In AWS Search Bar type \"EC2\" and Select \"EC2\" Services Tab 5. From \"Resources\" select \"Instances (Running)\" Locate Public IPv4 Address of EC2 instance, either on the EC2 instance list or under Details section From workstation open command line interface by hitting \"Start\" and typing \"cmd\" Execute command ping <Public IPv4 address of EC2> Communication is successfull now, EC2 instance has access to Internet via AWS Internet Gateway. At this point we completed that part of our topology configuration.","title":"Use-case 02 - Internet Gateway"},{"location":"use-case2/#use-case-02-internet-gateway","text":"In this task we will configure Internet Gateway in AWS cloud, so Virtual Machines and Endpoints are able to reach the Internet Instances in AWS can assess Internet in AWS cloud with help of Internet Gateway. An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between VPC and the internet. An internet gateway provides a target VPC route tables for internet-routable traffic. For communication using IPv4, the internet gateway also performs network address translation (NAT).","title":"Use-case 02 - Internet Gateway"},{"location":"use-case2/#internet-gateway-as-external-epg","text":"As all VPC aspects and routing are controller by Cloud Network Controller, also Internet Gateway(IGW) it's configured with it's help. Internet Gateway is represented by well know ACI object - External EPG.","title":"Internet Gateway as External EPG"},{"location":"use-case2/#external-epg-configuration","text":"Open Nexus Dashboard Orchestrator GUI then go to Application Management -> Schemas -> \"Schema-T01\" -> open Under View select \"temp-AWS-01\" and hit \"Add External EPGs\" under External EPGs section. Display Name: ExtEPG-IGW Description: Ext EPG for Internet Gateway AWS Virutal Routing & Forwarding: VRF-01 Select Site Type: CLOUD Cloud Properties: Application Profile: AppProf-AWS-01 Add Selector: Endpoint Selector Name: Internet Expression Type: IP Address Expression Operator: Equal Expression Value: 0.0.0.0/0 Hit Save to finish.","title":"External EPG configuration"},{"location":"use-case2/#contract-configuration-configuration","text":"As External EPG is functioning as any other EPG, contract configuration from EPG to External EPG is mandatory for traffic to be allowed.","title":"Contract configuration configuration"},{"location":"use-case2/#1-filter-creation","text":"Open Nexus Dashboard Orchestrator GUI then go to Application Management -> Schemas -> \"Schema-T01\" -> open Under View select \"temp-AWS-01\"\" and \"Add filter\" under Filter section. Display Name: permit-any-aws Then click \"Add entry\" to define protocols and ports. Name: permit-any Leave rest setting as default - this will allow for all protocols and ports. Hit Ok to save it.","title":"1. Filter creation"},{"location":"use-case2/#2-contract-configuration","text":"As contracts will be used to connect AWS EPG to Ext EPG of AWS , contract should be created in AWS template Under View select \"temp-AWS-01\" and \"Add contract\" under Contract section. Create two(2) contracts with following details: Contract con-AWS-01-to-IGW Display Name: con-AWS-01-to-IGW Scope: VRF Apply both direction: yes Add Filter: permit-any-AWS Hit Save to finish contract configuration. Contract con-IGW-to-AWS-01 Display Name: con-IGW-to-AWS-01 Scope: VRF Apply both direction: yes Add Filter: permit-any-AWS Hit Save to finish contract configuration. Hit Deploy to sites for contracts to be pushed.","title":"2. Contract configuration"},{"location":"use-case2/#3-contract-assigment-to-epgs","text":"Under View select \"temp-AWS-01\" click on the EPG-AWS-01 and under EPG specific setting locate Contract section Hit \"Add Contract\" button and add the following Contract 1: EPG: EPG-AWS01 Contract: con-AWS-01-to-IGW * Type: provider Contract 2: EPG: EPG-AWS-01 Contract: con-IGW-to-AWS-01 Type: consumer Under View select \"temp-AWS-01\" click on the ExtEPG-IGW and under Common Properties locate Contract section Hit \"Add Contract\" button and add the following Contract 1: EPG: ExtEPG-IGW Contract: con-IGW-to-AWS-01 Type: provider Contract 2: EPG: ExtEPG-IGW Contract: con-AWS-01-to-IGW Type: consumer Hit Deploy to sites to create IGW and deploy contracts associations.","title":"3. Contract assigment to EPGs"},{"location":"use-case2/#veryfication","text":"Let's check if AWS EC2 instance is now reachable from internet.","title":"Veryfication"},{"location":"use-case2/#1-open-aws-console-via-browser","text":"","title":"1. Open AWS console via browser"},{"location":"use-case2/#2-select-iam-user-provide-account-id-for-user-tenant-and-hit-next","text":"","title":"2. Select IAM user,  provide Account ID for User tenant and hit \"Next\""},{"location":"use-case2/#3-provide-username-and-password-and-hit-sign-in","text":"","title":"3. Provide Username and password and hit \"Sign In\""},{"location":"use-case2/#4-in-aws-search-bar-type-ec2-and-select-ec2-services-tab","text":"","title":"4. In AWS Search Bar type \"EC2\" and Select \"EC2\" Services Tab"},{"location":"use-case2/#5-from-resources-select-instances-running","text":"Locate Public IPv4 Address of EC2 instance, either on the EC2 instance list or under Details section From workstation open command line interface by hitting \"Start\" and typing \"cmd\" Execute command ping <Public IPv4 address of EC2> Communication is successfull now, EC2 instance has access to Internet via AWS Internet Gateway. At this point we completed that part of our topology configuration.","title":"5. From \"Resources\" select \"Instances (Running)\""},{"location":"use-case3/","text":"TBD","title":"Use-case 03 - Inter-Tenant routing"}]}